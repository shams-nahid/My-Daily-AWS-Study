{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home My Daily AWS Study Notes Select the notes from the top Navigation Features CSAA & CDA Suitable for both associate level certification AWS Certified Solution Architect Associate AWS Certified Developer Associate Notes on Exam Dumps Notes of practice exam from, Wizlab (CSAA, CDA) Tutorial Dojo (CSAA, CDA) Stephane Maarek (CSAA) ACloud Guru (CSAA)","title":"Home"},{"location":"#home","text":"My Daily AWS Study Notes","title":"Home"},{"location":"Notes/Next%20Study/","text":"More Study Needed KMS Envelope Encryption Cloudformation Masterclass X-Ray trust policy and iam role envelope encryption code commit Cloudwatch period, data point and evaluation period Dynamodb rcu wcu API and Lambda method/integration request/response Cognito properties like cognito streams 15th November S3 Athena (From Neal Devis, specially the partitioning for Athena) KMS ECS (TASK) STS (Less important)","title":"Next Study"},{"location":"Notes/Next%20Study/#more-study-needed","text":"KMS Envelope Encryption Cloudformation Masterclass X-Ray trust policy and iam role envelope encryption code commit Cloudwatch period, data point and evaluation period Dynamodb rcu wcu API and Lambda method/integration request/response Cognito properties like cognito streams 15th November S3 Athena (From Neal Devis, specially the partitioning for Athena) KMS ECS (TASK) STS (Less important)","title":"More Study Needed"},{"location":"Notes/AWS%20Fundamentals/01%20Region%20and%20AZ/","text":"Region Combination of data centers Data centers are called AZ , i.e. Availability Zone AZ Availability Zone In region, couple of AZ exists AZ are isolated each other, so Disaster in one AZ does not impact on other AZ AZ and Region Some service are AZ scoped Some services are Global scope AZ name can be us-east-1a , us-east-2b Region name be us-east , us-west AZ names are randomized according to the Account us-east-1a in one account could be us-east-1b to another account","title":"01 Region and AZ"},{"location":"Notes/AWS%20Fundamentals/01%20Region%20and%20AZ/#region","text":"Combination of data centers Data centers are called AZ , i.e. Availability Zone","title":"Region"},{"location":"Notes/AWS%20Fundamentals/01%20Region%20and%20AZ/#az","text":"Availability Zone In region, couple of AZ exists AZ are isolated each other, so Disaster in one AZ does not impact on other AZ","title":"AZ"},{"location":"Notes/AWS%20Fundamentals/01%20Region%20and%20AZ/#az-and-region","text":"Some service are AZ scoped Some services are Global scope AZ name can be us-east-1a , us-east-2b Region name be us-east , us-west AZ names are randomized according to the Account us-east-1a in one account could be us-east-1b to another account","title":"AZ and Region"},{"location":"Notes/AWS%20Fundamentals/02%20IAM/","text":"IAM Identity and Access Management AWS Security spans Users (For Physical Person) Groups (Can be admins, dev-ops, developers) Roles (For AWS Resources ) Policy Written in JSON format Determine what Users , Groups and Role has access Verify IAM Policy by dry-run policy can be used to verify if there is available permission IAM Policy Simulator By default, the AWS Billing and Cost Management does not allow access IAM user. Need to grant access for each user. Permission specified in cli with access key and secret overrides the IAM role permissions For any unauthorized encrypt message of the unauthorized access, can be decrypt by decode-authorization-message of STS API Account Alias By default, sign in url is like, account-id.signin.aws.amazon.com/console By creating the account alias, url become, account-alias.signin.aws.amazon.com/console IAM Certificate Store Can be used to import 3rd party SSL/TLS certificate. Both ACM and IAM Certificate Store can be used to import 3rd party SSL/TLS Certificate. Trust Policy When it comes to resource based policy, IAM only supports Trust Policy To access a service using cli/api from ec2 instance First need to create policy for the targeted resources Add the ec2 service as the trust policy (So ec2 can use the policy created in the first step) PassRole With passRole we can ensure, user does not have more permission than it required In passRole of a EC2 instance, if there is a definition of S3 Read access , we can not allow the EC2 any roles, other than the S3 Read Access This way, we do not need to store any credentials in the ec2 service Policy Evaluation Any explicit deny => ends up deny Any explicit allow => ends up allow Unless there is any allow => ends up deny Combination of IAM and Bucket Policy From security standpoint, the final policy is the union of both IAM and Bucket policy Example 1: EC2 instance IAM role allows reading a bucket and the bucket's buckets policy is empty => EC2 instance can read from the bucket Example 2: EC2 instance IAM role is empty and the bucket's buckets policy is allows reading from the EC2 instance => EC2 instance can read from the bucket Example 3: EC2 instance IAM role allows reading a bucket and the bucket's buckets policy is deny to read it from EC2 => EC2 instance can not read from the bucket Example 4: EC2 instance IAM role denys reading a bucket and the bucket's buckets policy allows to read it from EC2 => EC2 instance can not read from the bucket Dynamic Policy [IAM Policy Variables] In policy, variables can be used to make one policy for different user. For example, if we have different folder for each user in S3 and we want to allow user access to only their named folder, in this case, instead of creating policy for each of the user, we can use it like ${aws:username} and attach it to all the users. Types of Policy AWS Managed Policy Managed by AWS Good for users and administrators AWS update for new API or services Customer Managed Policy Best practice, re-usable, allows rollback Central management Inline Policy Attached to the user or resource IAM Access Advisor Allow identify, analyze and remove unused roles Good for enhance the security of team members access by implementing minimal permission Best Practices Delete the root user access keys Create roles and IAM policies with least permissions Use groups for users and assign roles If the new policy does not work, we can revert back to the old policy by selecting the previous version","title":"02 IAM"},{"location":"Notes/AWS%20Fundamentals/02%20IAM/#iam","text":"Identity and Access Management AWS Security spans Users (For Physical Person) Groups (Can be admins, dev-ops, developers) Roles (For AWS Resources ) Policy Written in JSON format Determine what Users , Groups and Role has access Verify IAM Policy by dry-run policy can be used to verify if there is available permission IAM Policy Simulator By default, the AWS Billing and Cost Management does not allow access IAM user. Need to grant access for each user. Permission specified in cli with access key and secret overrides the IAM role permissions For any unauthorized encrypt message of the unauthorized access, can be decrypt by decode-authorization-message of STS API","title":"IAM"},{"location":"Notes/AWS%20Fundamentals/02%20IAM/#account-alias","text":"By default, sign in url is like, account-id.signin.aws.amazon.com/console By creating the account alias, url become, account-alias.signin.aws.amazon.com/console","title":"Account Alias"},{"location":"Notes/AWS%20Fundamentals/02%20IAM/#iam-certificate-store","text":"Can be used to import 3rd party SSL/TLS certificate. Both ACM and IAM Certificate Store can be used to import 3rd party SSL/TLS Certificate.","title":"IAM Certificate Store"},{"location":"Notes/AWS%20Fundamentals/02%20IAM/#trust-policy","text":"When it comes to resource based policy, IAM only supports Trust Policy To access a service using cli/api from ec2 instance First need to create policy for the targeted resources Add the ec2 service as the trust policy (So ec2 can use the policy created in the first step)","title":"Trust Policy"},{"location":"Notes/AWS%20Fundamentals/02%20IAM/#passrole","text":"With passRole we can ensure, user does not have more permission than it required In passRole of a EC2 instance, if there is a definition of S3 Read access , we can not allow the EC2 any roles, other than the S3 Read Access This way, we do not need to store any credentials in the ec2 service","title":"PassRole"},{"location":"Notes/AWS%20Fundamentals/02%20IAM/#policy-evaluation","text":"Any explicit deny => ends up deny Any explicit allow => ends up allow Unless there is any allow => ends up deny Combination of IAM and Bucket Policy From security standpoint, the final policy is the union of both IAM and Bucket policy Example 1: EC2 instance IAM role allows reading a bucket and the bucket's buckets policy is empty => EC2 instance can read from the bucket Example 2: EC2 instance IAM role is empty and the bucket's buckets policy is allows reading from the EC2 instance => EC2 instance can read from the bucket Example 3: EC2 instance IAM role allows reading a bucket and the bucket's buckets policy is deny to read it from EC2 => EC2 instance can not read from the bucket Example 4: EC2 instance IAM role denys reading a bucket and the bucket's buckets policy allows to read it from EC2 => EC2 instance can not read from the bucket","title":"Policy Evaluation"},{"location":"Notes/AWS%20Fundamentals/02%20IAM/#dynamic-policy-iam-policy-variables","text":"In policy, variables can be used to make one policy for different user. For example, if we have different folder for each user in S3 and we want to allow user access to only their named folder, in this case, instead of creating policy for each of the user, we can use it like ${aws:username} and attach it to all the users.","title":"Dynamic Policy [IAM Policy Variables]"},{"location":"Notes/AWS%20Fundamentals/02%20IAM/#types-of-policy","text":"AWS Managed Policy Managed by AWS Good for users and administrators AWS update for new API or services Customer Managed Policy Best practice, re-usable, allows rollback Central management Inline Policy Attached to the user or resource","title":"Types of Policy"},{"location":"Notes/AWS%20Fundamentals/02%20IAM/#iam-access-advisor","text":"Allow identify, analyze and remove unused roles Good for enhance the security of team members access by implementing minimal permission","title":"IAM Access Advisor"},{"location":"Notes/AWS%20Fundamentals/02%20IAM/#best-practices","text":"Delete the root user access keys Create roles and IAM policies with least permissions Use groups for users and assign roles If the new policy does not work, we can revert back to the old policy by selecting the previous version","title":"Best Practices"},{"location":"Notes/AWS%20Fundamentals/03%20EC2%20Introduction/","text":"EC2 Elastic Cloud Computer EC2 User Data Scripts that runs when the instance is booted up Use for Installing updates while boot up Installing software while boot up Download common files while boot up These scripts are run as root user like sudo command Scripts are executed with root user previleges EC2 Meta Data Information about the instance Launch Type On Demand Pay as you go Reserved 3 types in terms of duration Regular Reserved Instance Min 1 to max 3 Years Convertible Reserved Instance Can be convert the types like more cpu optimized , more memory optimized Scheduled Reserved Instance Will be up and running for certain times in regular basis Recommended when the time frame is at least 1 year usage 2 types in terms of capacity reservation Zonal - Allow capacity reservation, applicable for availability zones reserve instances Regional - Allow capacity reservation, applicable for regional reserve instances Spot Instance can loose instance very low price Define max spot price if the current spot price goes high of the defined spot price, we loose the instance in 2 minutes Using Spot Block we can extend the termination delay till -6 hours Two type of request One time request Once max price < current price, all instances are removed Persistent Request If spot instances stop and then things are good, these instances launched automatically To stop persistent request First delete the spot request Then removed the spot instances Spot Fleets Set of Spot Instances and On-Demand Instances We can define Possible launch pools Multiple AZ Various type of instance Various OS Automatically stops when meed the capacity It offers lowestPrice diversified (distribute across az and workloads) capacityOptimized Dedicated Instance Does not share hardware Dedicated Host Does not share server, entire placement is booked Dedicated Instance Vs Dedicated Host In Dedicated Instance The billing is is done per instance The other instance of same account, may share hardware No control over placement group In Dedicated Host , billing is on whole Dedicated Host Control over placement group Instance Type R Instance with lots of memory/ RAM Used when in memory caching is required C Instance with good Computation Power Used for DB Server M Middle between RAM and Computation Used for Application Server / General Application Has 10-25 GB/s networking Has ENA enabled Low latency network with NVME ebs I For heave I/O Application Used for DB When good Instance Storage is required G GPU optimized instance Used for video rendering or machine learning T2/T3 Burstble instance Provide good performance according to the capacity There is a criteria, where unlimited burst is provided Terminating a EC2 Instance For production EC2 Instance Need a tag In resource level there should be explicit deny for production tag for not to terminates","title":"03 EC2 Introduction"},{"location":"Notes/AWS%20Fundamentals/03%20EC2%20Introduction/#ec2","text":"Elastic Cloud Computer","title":"EC2"},{"location":"Notes/AWS%20Fundamentals/03%20EC2%20Introduction/#ec2-user-data","text":"Scripts that runs when the instance is booted up Use for Installing updates while boot up Installing software while boot up Download common files while boot up These scripts are run as root user like sudo command Scripts are executed with root user previleges","title":"EC2 User Data"},{"location":"Notes/AWS%20Fundamentals/03%20EC2%20Introduction/#ec2-meta-data","text":"Information about the instance","title":"EC2 Meta Data"},{"location":"Notes/AWS%20Fundamentals/03%20EC2%20Introduction/#launch-type","text":"On Demand Pay as you go Reserved 3 types in terms of duration Regular Reserved Instance Min 1 to max 3 Years Convertible Reserved Instance Can be convert the types like more cpu optimized , more memory optimized Scheduled Reserved Instance Will be up and running for certain times in regular basis Recommended when the time frame is at least 1 year usage 2 types in terms of capacity reservation Zonal - Allow capacity reservation, applicable for availability zones reserve instances Regional - Allow capacity reservation, applicable for regional reserve instances Spot Instance can loose instance very low price Define max spot price if the current spot price goes high of the defined spot price, we loose the instance in 2 minutes Using Spot Block we can extend the termination delay till -6 hours Two type of request One time request Once max price < current price, all instances are removed Persistent Request If spot instances stop and then things are good, these instances launched automatically To stop persistent request First delete the spot request Then removed the spot instances Spot Fleets Set of Spot Instances and On-Demand Instances We can define Possible launch pools Multiple AZ Various type of instance Various OS Automatically stops when meed the capacity It offers lowestPrice diversified (distribute across az and workloads) capacityOptimized Dedicated Instance Does not share hardware Dedicated Host Does not share server, entire placement is booked","title":"Launch Type"},{"location":"Notes/AWS%20Fundamentals/03%20EC2%20Introduction/#dedicated-instance-vs-dedicated-host","text":"In Dedicated Instance The billing is is done per instance The other instance of same account, may share hardware No control over placement group In Dedicated Host , billing is on whole Dedicated Host Control over placement group","title":"Dedicated Instance Vs Dedicated Host"},{"location":"Notes/AWS%20Fundamentals/03%20EC2%20Introduction/#instance-type","text":"R Instance with lots of memory/ RAM Used when in memory caching is required C Instance with good Computation Power Used for DB Server M Middle between RAM and Computation Used for Application Server / General Application Has 10-25 GB/s networking Has ENA enabled Low latency network with NVME ebs I For heave I/O Application Used for DB When good Instance Storage is required G GPU optimized instance Used for video rendering or machine learning T2/T3 Burstble instance Provide good performance according to the capacity There is a criteria, where unlimited burst is provided","title":"Instance Type"},{"location":"Notes/AWS%20Fundamentals/03%20EC2%20Introduction/#terminating-a-ec2-instance","text":"For production EC2 Instance Need a tag In resource level there should be explicit deny for production tag for not to terminates","title":"Terminating a EC2 Instance"},{"location":"Notes/AWS%20Fundamentals/04%20Security%20Group/","text":"Security Group Firewall on EC2 Instance Stateful Effective immediately","title":"04 Security Group"},{"location":"Notes/AWS%20Fundamentals/04%20Security%20Group/#security-group","text":"Firewall on EC2 Instance Stateful Effective immediately","title":"Security Group"},{"location":"Notes/AWS%20Fundamentals/06%20Public%20Vs%20Private%20vs%20Elastic%20IP/","text":"Public IP Any IP other than Private IP Private IP Private IP does not change if EC2 Instance is restarted (stop and then start) Big Network Range 10.0.0.0 - 10.255.255.255 and CIDR ( 10.0.0.0/8 ) AWS Default Range 172.16.0.0 - 172.31.255.255 and CIDR ( 172.16.0.0/12 ) Home Network Range 192.168.0.0 - 192.168.255.255 and CIDR ( 192.168.0.0/16 ) Elastic IP Does not change if EC2 Instance is restarted (stop and then start) or terminated Max 5 Elastic IP can be used in one region We can bring our own IP as Elastic IP Elastic IP is charge, even if it is not being used","title":"06 Public Vs Private vs Elastic IP"},{"location":"Notes/AWS%20Fundamentals/06%20Public%20Vs%20Private%20vs%20Elastic%20IP/#public-ip","text":"Any IP other than Private IP","title":"Public IP"},{"location":"Notes/AWS%20Fundamentals/06%20Public%20Vs%20Private%20vs%20Elastic%20IP/#private-ip","text":"Private IP does not change if EC2 Instance is restarted (stop and then start) Big Network Range 10.0.0.0 - 10.255.255.255 and CIDR ( 10.0.0.0/8 ) AWS Default Range 172.16.0.0 - 172.31.255.255 and CIDR ( 172.16.0.0/12 ) Home Network Range 192.168.0.0 - 192.168.255.255 and CIDR ( 192.168.0.0/16 )","title":"Private IP"},{"location":"Notes/AWS%20Fundamentals/06%20Public%20Vs%20Private%20vs%20Elastic%20IP/#elastic-ip","text":"Does not change if EC2 Instance is restarted (stop and then start) or terminated Max 5 Elastic IP can be used in one region We can bring our own IP as Elastic IP Elastic IP is charge, even if it is not being used","title":"Elastic IP"},{"location":"Notes/AWS%20Monitoring/01%20Cloudwatch/","text":"Cloudwatch Cloudwatch Metrics Cloudwatch provide metrics for every AWS Service Metrics is a variable to monitor, such as CPU Utilization Networking data Metric belong to Namespace Namespace are similar to Group Metric dimensions are Attribute , like Instance ID Environment Name Each Metric can have up to 10 Dimensions Metrics have Timestamps Using metric , the Cloudwatch Dashboard is generated Cloudwatch Detailed Monitoring By default, EC2 have metrics each 5 minutes With Detailed Monitoring Metric generate every 1 Minute Good for ASG Free Tier allows 10 Detail Monitoring For EC2 Memory Usage , there is no default metric. Need to use Custom Metric Cloudwatch Custom Metric Can send custom metrics to Cloudwatch Ability to send Dimension instance.id environment.name Metric Resolution Standard 1 Minute High Resolution, up to 1 Sec To send custom metric use PutMetricData Cloudwatch Dashboard Dashboard are Global (cross region, cross account) Dashboard Graph includes different Region Dashboard Graph includes different Account Can setup Auto Refresh Pricing 3 Dashboard (Up to 50 Metrics) free After free tier, 3 dollar/dashboard/per month Cloudwatch Logs Logs can be send to Cloudwatch through SDK Cloudwatch collect log from Elastic Beanstalk ECS AWS Lambda VPC Flow Logs API Gateway Cloudtrail Cloudwatch Log Agent (From EC2 Instance ) Route 53 (DNS Query) Logs go to S3 to store or archive Stream to Elastic Search for analytics Log Storage Architecture Groups: Log is grouped under name Each group has streams of logs Can define expiration period (After the expiration period, the logs will be deleted) Encryption KMS can be used to encrypt the logs Encryption is done in log group level Using encryption key, both new (create-log-group) and existing (associate) log group can be encrypted Encryption operation can be done only by CLI or SDK To send logs, make sure the Permission to write logs are set To follow/tail logs, we can use AWS CLI Possible to filter by expression Helpful to find logs or specific IP Can use to trigger alarm Cloudwatch Logs Insights Can be used to query logs To use, need to install the Unified Cloudwatch Logs Event Cloudwatch Logs Agent Vs Unified Agent Logs Agent Old Version Unified Agent Newer version Get Additional system level metrics Can use SSM Parameter Store to centralized configuration Cloudwatch Alarms Alarms are used to trigger notification for any metric Alarms can go to ASG EC2 Actions SNS Notifications Alarm can raise Sampling value Percentage value Max or Min value Alarm States OK (When everything is alright) INSUFFICIENT_DATA (When not enough data to measure it its OK or ALARM state) ALARM (When metrics reached the Threshold ) Period Time length to evaluate the metric In case of High Resolution Metric , period can be 10 sec Creating Cloudwatch Event While creating a cloudwatch event, we can set Period : Define evaluation time in seconds. Evaluation Period / Number of Data Point : Known as Data Point . Number of recent Period to consider to generate a alarm state DataPoints To Alarm : Determine to go to ALARM state. We can define how many period can be reached within a evaluation period to go to ALARM Cloudwatch Event Can schedule CRON Jobs Event Pattern Rules on react a service doing something Example: Code Pipeline state change Can trigger Lambda Function SQS SNS Kinesis Cloudwatch Event create a sample document to give information about the change Use case in S3 and Code Pipeline Code can be uploaded to S3 Cloudwatch Event trigger the Code Pipeline Code will be deployed to the Elastic Beanstalk Can be used to change the number of Fargate Cluster should run according to the events Allow monitor jobs in the batch jobs Cloudwatch Agent Collect system info and log files Can track memory, swap and disk space","title":"01 Cloudwatch"},{"location":"Notes/AWS%20Monitoring/01%20Cloudwatch/#cloudwatch","text":"","title":"Cloudwatch"},{"location":"Notes/AWS%20Monitoring/01%20Cloudwatch/#cloudwatch-metrics","text":"Cloudwatch provide metrics for every AWS Service Metrics is a variable to monitor, such as CPU Utilization Networking data Metric belong to Namespace Namespace are similar to Group Metric dimensions are Attribute , like Instance ID Environment Name Each Metric can have up to 10 Dimensions Metrics have Timestamps Using metric , the Cloudwatch Dashboard is generated","title":"Cloudwatch Metrics"},{"location":"Notes/AWS%20Monitoring/01%20Cloudwatch/#cloudwatch-detailed-monitoring","text":"By default, EC2 have metrics each 5 minutes With Detailed Monitoring Metric generate every 1 Minute Good for ASG Free Tier allows 10 Detail Monitoring For EC2 Memory Usage , there is no default metric. Need to use Custom Metric","title":"Cloudwatch Detailed Monitoring"},{"location":"Notes/AWS%20Monitoring/01%20Cloudwatch/#cloudwatch-custom-metric","text":"Can send custom metrics to Cloudwatch Ability to send Dimension instance.id environment.name Metric Resolution Standard 1 Minute High Resolution, up to 1 Sec To send custom metric use PutMetricData","title":"Cloudwatch Custom Metric"},{"location":"Notes/AWS%20Monitoring/01%20Cloudwatch/#cloudwatch-dashboard","text":"Dashboard are Global (cross region, cross account) Dashboard Graph includes different Region Dashboard Graph includes different Account Can setup Auto Refresh Pricing 3 Dashboard (Up to 50 Metrics) free After free tier, 3 dollar/dashboard/per month","title":"Cloudwatch Dashboard"},{"location":"Notes/AWS%20Monitoring/01%20Cloudwatch/#cloudwatch-logs","text":"Logs can be send to Cloudwatch through SDK Cloudwatch collect log from Elastic Beanstalk ECS AWS Lambda VPC Flow Logs API Gateway Cloudtrail Cloudwatch Log Agent (From EC2 Instance ) Route 53 (DNS Query) Logs go to S3 to store or archive Stream to Elastic Search for analytics Log Storage Architecture Groups: Log is grouped under name Each group has streams of logs Can define expiration period (After the expiration period, the logs will be deleted) Encryption KMS can be used to encrypt the logs Encryption is done in log group level Using encryption key, both new (create-log-group) and existing (associate) log group can be encrypted Encryption operation can be done only by CLI or SDK To send logs, make sure the Permission to write logs are set To follow/tail logs, we can use AWS CLI Possible to filter by expression Helpful to find logs or specific IP Can use to trigger alarm Cloudwatch Logs Insights Can be used to query logs To use, need to install the Unified Cloudwatch Logs Event","title":"Cloudwatch Logs"},{"location":"Notes/AWS%20Monitoring/01%20Cloudwatch/#cloudwatch-logs-agent-vs-unified-agent","text":"Logs Agent Old Version Unified Agent Newer version Get Additional system level metrics Can use SSM Parameter Store to centralized configuration","title":"Cloudwatch Logs Agent Vs Unified Agent"},{"location":"Notes/AWS%20Monitoring/01%20Cloudwatch/#cloudwatch-alarms","text":"Alarms are used to trigger notification for any metric Alarms can go to ASG EC2 Actions SNS Notifications Alarm can raise Sampling value Percentage value Max or Min value Alarm States OK (When everything is alright) INSUFFICIENT_DATA (When not enough data to measure it its OK or ALARM state) ALARM (When metrics reached the Threshold ) Period Time length to evaluate the metric In case of High Resolution Metric , period can be 10 sec","title":"Cloudwatch Alarms"},{"location":"Notes/AWS%20Monitoring/01%20Cloudwatch/#creating-cloudwatch-event","text":"While creating a cloudwatch event, we can set Period : Define evaluation time in seconds. Evaluation Period / Number of Data Point : Known as Data Point . Number of recent Period to consider to generate a alarm state DataPoints To Alarm : Determine to go to ALARM state. We can define how many period can be reached within a evaluation period to go to ALARM","title":"Creating Cloudwatch Event"},{"location":"Notes/AWS%20Monitoring/01%20Cloudwatch/#cloudwatch-event","text":"Can schedule CRON Jobs Event Pattern Rules on react a service doing something Example: Code Pipeline state change Can trigger Lambda Function SQS SNS Kinesis Cloudwatch Event create a sample document to give information about the change Use case in S3 and Code Pipeline Code can be uploaded to S3 Cloudwatch Event trigger the Code Pipeline Code will be deployed to the Elastic Beanstalk Can be used to change the number of Fargate Cluster should run according to the events Allow monitor jobs in the batch jobs","title":"Cloudwatch Event"},{"location":"Notes/AWS%20Monitoring/01%20Cloudwatch/#cloudwatch-agent","text":"Collect system info and log files Can track memory, swap and disk space","title":"Cloudwatch Agent"},{"location":"Notes/AWS%20Monitoring/02%20Cloudtrail/","text":"Cloudtrail All the AWS history and events are stored here, including Console SDK CLI AWS Service Cloudtrail is enabled by default Can put the logs to CloudWatch Logs Example, if need to trace, who change the resource, need to go to Cloudtrail The logs are encrypted by default By default Cloudtrail Logs are encrypted by S3 Server Side Encryption Also we can use KMS for encryption Enabling Cloudtrail Log File Integrity ensure Non compliance log Generate public and private key of the logs Put the digest in separate folder Cloudtrail Global Event Logs can only done by AWS CLI , not Console To monitor API calls in the Redshift Cluster , need to use Cloudtrail","title":"02 Cloudtrail"},{"location":"Notes/AWS%20Monitoring/02%20Cloudtrail/#cloudtrail","text":"All the AWS history and events are stored here, including Console SDK CLI AWS Service Cloudtrail is enabled by default Can put the logs to CloudWatch Logs Example, if need to trace, who change the resource, need to go to Cloudtrail The logs are encrypted by default By default Cloudtrail Logs are encrypted by S3 Server Side Encryption Also we can use KMS for encryption Enabling Cloudtrail Log File Integrity ensure Non compliance log Generate public and private key of the logs Put the digest in separate folder Cloudtrail Global Event Logs can only done by AWS CLI , not Console To monitor API calls in the Redshift Cluster , need to use Cloudtrail","title":"Cloudtrail"},{"location":"Notes/AWS%20Monitoring/03%20AWS%20Config/","text":"AWS Config Helps record configurations and changes over time Can store config data to S3 for further analysis Analysis can be Is there un-restricted SSH access in any SG Do bucket has public access Is ALB Config changes over time Alert for the config can be received by SNS AWS-Config is per-region service, although can be aggregate across regions and accounts Config Rules Can use AWS Managed Rules Can make custom rules using AWS Lambda , like If each EBS disk is type GP2 If each EC2 instance is type t2.micro Rules be triggered or evaluated On config change Regular time intervals Using Cloudwatch Event AWS Config does not prevent actions from happening, it just check the changes Pricing is 2 Dollar/per active rule/per region / per month","title":"03 AWS Config"},{"location":"Notes/AWS%20Monitoring/03%20AWS%20Config/#aws-config","text":"Helps record configurations and changes over time Can store config data to S3 for further analysis Analysis can be Is there un-restricted SSH access in any SG Do bucket has public access Is ALB Config changes over time Alert for the config can be received by SNS AWS-Config is per-region service, although can be aggregate across regions and accounts","title":"AWS Config"},{"location":"Notes/AWS%20Monitoring/03%20AWS%20Config/#config-rules","text":"Can use AWS Managed Rules Can make custom rules using AWS Lambda , like If each EBS disk is type GP2 If each EC2 instance is type t2.micro Rules be triggered or evaluated On config change Regular time intervals Using Cloudwatch Event AWS Config does not prevent actions from happening, it just check the changes Pricing is 2 Dollar/per active rule/per region / per month","title":"Config Rules"},{"location":"Notes/AWS%20Storage/01%20Snowball/","text":"Snowball Moving data from on-premise to S3 Features Secure Temper Resistant KMS 256 Encryption Charge per data transfer job Snowball tracking can be done using SNS (Text Message) and AWS Console Process Request a snowball Install snowball clients to server Connect snowball with server Copy files from server to snowball Ship back to the device Snowball Edge Snowball Edge has the computational capability Two types Storage optimized Compute Optimized (Can be with GPU) Supports EC2 AMI so it can perform processing on go Supports Lambda Function Use cases Data Migration Image Collection IoT Capture Machine Learning Snowmobile A truck Use to transfer exabytes of data Limitation Snowball can only transfer data back and forth with S3 To transfer data from/to Glacier , first need to use S3 . Then Life Cycle Policy can be used to transfer to/from Glacier .","title":"01 Snowball"},{"location":"Notes/AWS%20Storage/01%20Snowball/#snowball","text":"Moving data from on-premise to S3 Features Secure Temper Resistant KMS 256 Encryption Charge per data transfer job Snowball tracking can be done using SNS (Text Message) and AWS Console","title":"Snowball"},{"location":"Notes/AWS%20Storage/01%20Snowball/#process","text":"Request a snowball Install snowball clients to server Connect snowball with server Copy files from server to snowball Ship back to the device","title":"Process"},{"location":"Notes/AWS%20Storage/01%20Snowball/#snowball-edge","text":"Snowball Edge has the computational capability Two types Storage optimized Compute Optimized (Can be with GPU) Supports EC2 AMI so it can perform processing on go Supports Lambda Function Use cases Data Migration Image Collection IoT Capture Machine Learning","title":"Snowball Edge"},{"location":"Notes/AWS%20Storage/01%20Snowball/#snowmobile","text":"A truck Use to transfer exabytes of data","title":"Snowmobile"},{"location":"Notes/AWS%20Storage/01%20Snowball/#limitation","text":"Snowball can only transfer data back and forth with S3 To transfer data from/to Glacier , first need to use S3 . Then Life Cycle Policy can be used to transfer to/from Glacier .","title":"Limitation"},{"location":"Notes/AWS%20Storage/02%20Storage%20Gateway/","text":"Storage Gateway Bridge between AWS S3 and On-Premise data center 3 types of storage gateway File Gateway Volume Gateway Tape Gateway File Gateway Use for File Access Use for NFS and SMB protocol Can be mounted on many servers Bucket access is done by IAM role in the File Gateway Most recent data is being cached Hardware Appliance File gateway needs virtualization Alternative can be Hardware Appliance Use when small data center does not have virtualization capability Volume Gateway Use for Volumes and Block Storage Use for iSCSI protocol Backed by EBS Snapshot Two types of Volume Gateway Cached Volume Low latency for the recent data Stored Volumes All data are in the on-premise Schedule backup from on premise to S3 Tape Gateway Backup process use Physical Tape Backup with iSCSI Protocol Use VTL (Virtual Tape Library) Allow archive data to Glacier directly from the on-premise Misc Notes When we need low latency data access, use Storage Gateway Stored Volumes , it keeps all data in on-premise data center and make backup to cloud time basis In Storage Gateway Cached Volumes , only recent data are cached in the on-premise center","title":"02 Storage Gateway"},{"location":"Notes/AWS%20Storage/02%20Storage%20Gateway/#storage-gateway","text":"Bridge between AWS S3 and On-Premise data center 3 types of storage gateway File Gateway Volume Gateway Tape Gateway","title":"Storage Gateway"},{"location":"Notes/AWS%20Storage/02%20Storage%20Gateway/#file-gateway","text":"Use for File Access Use for NFS and SMB protocol Can be mounted on many servers Bucket access is done by IAM role in the File Gateway Most recent data is being cached Hardware Appliance File gateway needs virtualization Alternative can be Hardware Appliance Use when small data center does not have virtualization capability","title":"File Gateway"},{"location":"Notes/AWS%20Storage/02%20Storage%20Gateway/#volume-gateway","text":"Use for Volumes and Block Storage Use for iSCSI protocol Backed by EBS Snapshot Two types of Volume Gateway Cached Volume Low latency for the recent data Stored Volumes All data are in the on-premise Schedule backup from on premise to S3","title":"Volume Gateway"},{"location":"Notes/AWS%20Storage/02%20Storage%20Gateway/#tape-gateway","text":"Backup process use Physical Tape Backup with iSCSI Protocol Use VTL (Virtual Tape Library) Allow archive data to Glacier directly from the on-premise","title":"Tape Gateway"},{"location":"Notes/AWS%20Storage/02%20Storage%20Gateway/#misc-notes","text":"When we need low latency data access, use Storage Gateway Stored Volumes , it keeps all data in on-premise data center and make backup to cloud time basis In Storage Gateway Cached Volumes , only recent data are cached in the on-premise center","title":"Misc Notes"},{"location":"Notes/AWS%20Storage/03%20FSx/","text":"FSx Two types of FSx FSx for Windows FSx for Lustre FSx For Windows Fully managed widows file system Use SMB and NTFS protocol Built on SSD Can be accessed from On-Premise server Can be configured Multi-AZ Data is backed by S3 There is Microsoft Active Directory integration This type of storage can be managed by AWS Managed AD and accessed by other instances FSx for Lustre Parallel distributed file system for linux Stands for FSx for Linux Cluster Use for HPC (High Performance Computing) High performance storage Seamless integration with S3","title":"03 FSx"},{"location":"Notes/AWS%20Storage/03%20FSx/#fsx","text":"Two types of FSx FSx for Windows FSx for Lustre","title":"FSx"},{"location":"Notes/AWS%20Storage/03%20FSx/#fsx-for-windows","text":"Fully managed widows file system Use SMB and NTFS protocol Built on SSD Can be accessed from On-Premise server Can be configured Multi-AZ Data is backed by S3 There is Microsoft Active Directory integration This type of storage can be managed by AWS Managed AD and accessed by other instances","title":"FSx For Windows"},{"location":"Notes/AWS%20Storage/03%20FSx/#fsx-for-lustre","text":"Parallel distributed file system for linux Stands for FSx for Linux Cluster Use for HPC (High Performance Computing) High performance storage Seamless integration with S3","title":"FSx for Lustre"},{"location":"Notes/AWS%20Storage/04%20EFS/","text":"EFS Known as Elastic File System POSIX-compliant file system Network file system Only compatible with linux EFS Mount Helper can be used to encrypt data in transit File System Policy can be used to set default policy Service Linked Role used by AWS to invoke on our behalf KMS can be use to encrypt EFS Pay as go Increase as we use Can be used with only one VPC Used when Multiple linux server need to connect to storage simultaneously Linux server can be from multi-az Two types of performance mode General purpose Max I/O Cons are higher latency Can be used with higher level of throughput Can connect thousands of instances To share logs in single source, Use one task definition for multiple containers Mount a EFS volume Storage Tier Two types of EFS Standard Infrequent Access Lifecycle policy Can be used to move files to different tier after N days","title":"04 EFS"},{"location":"Notes/AWS%20Storage/04%20EFS/#efs","text":"Known as Elastic File System POSIX-compliant file system Network file system Only compatible with linux EFS Mount Helper can be used to encrypt data in transit File System Policy can be used to set default policy Service Linked Role used by AWS to invoke on our behalf KMS can be use to encrypt EFS Pay as go Increase as we use Can be used with only one VPC Used when Multiple linux server need to connect to storage simultaneously Linux server can be from multi-az Two types of performance mode General purpose Max I/O Cons are higher latency Can be used with higher level of throughput Can connect thousands of instances To share logs in single source, Use one task definition for multiple containers Mount a EFS volume","title":"EFS"},{"location":"Notes/AWS%20Storage/04%20EFS/#storage-tier","text":"Two types of EFS Standard Infrequent Access Lifecycle policy Can be used to move files to different tier after N days","title":"Storage Tier"},{"location":"Notes/AWS%20Storage/05%20EBS/","text":"EBS Elastic Block Store 4 Types GP2 General Purpose Handle up to 16000 IO/PS Used for Recommended for most workloads System Boot Low latency interactive apps Dev and Test environment With 5.3 TB size, the gp2 reached the max IO/PS IO1 IO optimized Used for When required more than 16000 IO/PS is required Large Database Critical business operation, require high sustained IO/PS When huge load and performance for the NoSQL database IO can be maximum 50x in comparision of storage capacity. For instance, a 200GB storage can have maximum 200 * 50 = 10000 IO/PS ST1 Throughput optimized Used for Streaming workloads, require consistent and fast throughput at low price Big Data, Data warehouses Apache kafka SC1 Infrequently Used Throughput optimized Used for large amount of data which are infrequently used For sequential I/O operations EBS transfer EBS volumes are AZ Locked First need to take a Snapshot Create the volume from the Snapshot to another AZ EBS Backup Backups are incremental, only backup the changed blocks Can take snapshot of the EBS Volume Theses snapshot can be made available to other Regions Snapshot can be automate by using DLM aka Data Lifecycle Manager While taking backups There's hamper on heavy traffic Recommended to detach the volume To use snapshot, require pre-warm Snapshots are taking place in S3 Using lifecycle policy, can automate the Snapshot Using the retention policy, can be delete the old Snapshot Reference For EBS Types Encryption Encryption is handled by AWS Supports both in-flight and at rest encryption by KMS Use KMS (AES-256) When a EBS is encrypted All data inside the volume is encrypted All moving data between instance and volume is encrypted All snapshots created from them is encrypted All volumes created from these snapshots are encrypted EBS volumes can be used while making a Snapshots , no problem. To encrypt an un encrypted EBS Take a snapshot Encrypt the snapshot using copy Create volume from the encrypted volume Delete volume and un encrypted snapshot for security leakage EBS vs Instance Store Instance Store Physically attached to the instance Good I/O When instance is terminated, the instance store along with the data lost Although it is block storage, size can not be increased over time EBS Network drive Data is persisted even the instance is terminated EBS RAID Two types of RAID RAID 0 Improve performance Example Lets say we have 2 EBS with 10000 IOps We logically merge and use 20000 IOps RAID 1 Use for fault tolerance Mirror the EBS volume Example If we have 2 EBS volume We write on each of volume So even if one EBS volume fails, data is still exist in another one Attaching a New EBS Volume When we attach a new EBS to EC2 instance, it is considered as block device. To make it usable, need to Format to a file system (AWS does not pre configure any file system to EBS) Mount it to the instance Detaching Existing EBS Volume For root volume Stop the instance Detach the volume For non-root volume For running instance, un-mount and detach For stopped instance, detach, no need to un-mount","title":"05 EBS"},{"location":"Notes/AWS%20Storage/05%20EBS/#ebs","text":"Elastic Block Store 4 Types GP2 General Purpose Handle up to 16000 IO/PS Used for Recommended for most workloads System Boot Low latency interactive apps Dev and Test environment With 5.3 TB size, the gp2 reached the max IO/PS IO1 IO optimized Used for When required more than 16000 IO/PS is required Large Database Critical business operation, require high sustained IO/PS When huge load and performance for the NoSQL database IO can be maximum 50x in comparision of storage capacity. For instance, a 200GB storage can have maximum 200 * 50 = 10000 IO/PS ST1 Throughput optimized Used for Streaming workloads, require consistent and fast throughput at low price Big Data, Data warehouses Apache kafka SC1 Infrequently Used Throughput optimized Used for large amount of data which are infrequently used For sequential I/O operations EBS transfer EBS volumes are AZ Locked First need to take a Snapshot Create the volume from the Snapshot to another AZ EBS Backup Backups are incremental, only backup the changed blocks Can take snapshot of the EBS Volume Theses snapshot can be made available to other Regions Snapshot can be automate by using DLM aka Data Lifecycle Manager While taking backups There's hamper on heavy traffic Recommended to detach the volume To use snapshot, require pre-warm Snapshots are taking place in S3 Using lifecycle policy, can automate the Snapshot Using the retention policy, can be delete the old Snapshot Reference For EBS Types","title":"EBS"},{"location":"Notes/AWS%20Storage/05%20EBS/#encryption","text":"Encryption is handled by AWS Supports both in-flight and at rest encryption by KMS Use KMS (AES-256) When a EBS is encrypted All data inside the volume is encrypted All moving data between instance and volume is encrypted All snapshots created from them is encrypted All volumes created from these snapshots are encrypted EBS volumes can be used while making a Snapshots , no problem. To encrypt an un encrypted EBS Take a snapshot Encrypt the snapshot using copy Create volume from the encrypted volume Delete volume and un encrypted snapshot for security leakage","title":"Encryption"},{"location":"Notes/AWS%20Storage/05%20EBS/#ebs-vs-instance-store","text":"Instance Store Physically attached to the instance Good I/O When instance is terminated, the instance store along with the data lost Although it is block storage, size can not be increased over time EBS Network drive Data is persisted even the instance is terminated","title":"EBS vs Instance Store"},{"location":"Notes/AWS%20Storage/05%20EBS/#ebs-raid","text":"Two types of RAID RAID 0 Improve performance Example Lets say we have 2 EBS with 10000 IOps We logically merge and use 20000 IOps RAID 1 Use for fault tolerance Mirror the EBS volume Example If we have 2 EBS volume We write on each of volume So even if one EBS volume fails, data is still exist in another one","title":"EBS RAID"},{"location":"Notes/AWS%20Storage/05%20EBS/#attaching-a-new-ebs-volume","text":"When we attach a new EBS to EC2 instance, it is considered as block device. To make it usable, need to Format to a file system (AWS does not pre configure any file system to EBS) Mount it to the instance","title":"Attaching a New EBS Volume"},{"location":"Notes/AWS%20Storage/05%20EBS/#detaching-existing-ebs-volume","text":"For root volume Stop the instance Detach the volume For non-root volume For running instance, un-mount and detach For stopped instance, detach, no need to un-mount","title":"Detaching Existing EBS Volume"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/01%20Cloudfront/","text":"Cloudfront Overview CDN (Content Delivery Network) Content is cached in the Edge Location There are more than 200 Edge Location all over the world This is a global service Only support web distribution Improve Read Performance Can expose any Internal HTTP endpoints External HTTP endpoints (Even on-premise server) Security DDoS Protection AWS Firewall Shield Integration If the Cache Control header has max-age is 0 , all the request will go to the Origin With cloudfront-viewer-country , we can determine, from which country the request is being made and redirect the traffic to specific url. To do so, have to make sure, viewer-request-events trigger the function Only root users can create cloudfront key-pairs Cloudfront Origin As S3 Bucket origin Use as Ingress i.e. upload file Distribute files Caching files at the Edge location OAI Origin Access Identity S3 Bucket only be accessed through Cloudfront As Custom Origin HTTP Application Load Balancer EC2 instance can be private SG of ALB must allow the Cloudfront Public IP EC2 Instance EC2 Instance must be public SG of the EC2 Instance must allow the Cloudfront Public IP S3 Static Website Any HTTP Backend (AWS Internal / on-premise) On Premise server can be used as a origin of Cloudfront Cloudfront Geo Restriction Two types of filtering Whitelist IP from Certain countries can access the content Blacklist IP from Certain countries can not access the content Determining the IP origin country is determined by using a 3rd party database Cloudfront vs S3 Cross Replication Cloudfront Use for static content When content must be available on almost all region Caching is for certain times (TTL) Use for both READ (Caching) WRITE (ingress) S3 Cross Replication Use for dynamic content When content must be available in certain region in very low latency Content is always available Use for READ Only Cloudfront Query String The delimiter character should be $ Parameter's name and values are case sensitive Cloudfront Origin Group Origin failover can be handled with using two origin Has two origin One is primary origin Other can be treated as secondary origin Cloudfront switch to secondary origin from primary origin if Primary origin fails Primary origin sends HTTP Fail Status Code Policies Cloudfront can ensure, From client object and request will be encrypted and use https When it comes to send response, if the object is not available in cache, cloudfront will fetch it from origin also in https format To enable both client -> cloudfront -> origin in https, we will need Viewer Protocol Policy (When HTTPS, there is ssl certificate installed in cloudfront) Origin Protocol Policy Cache Invalidation When new object is uploaded, to invalidate previous cache We can invalidate instantly We can wait for the existing cache to be invalidated Use versioned name TTL To make a object for a certain time, Configure origin to add a Cache-Control or Expires-Header Specify minimum TTL to the Cloudfront Cache Behevior Default value of TTL is 24 hours Cloudfront Function Cloudfront functions can be used Cache key normalization Header manipulation Status code modification and body generation Request Authorization Cloudfront Signed URL / Cookies Only root user can create key-pairs A root user can make max 2 key pairs After creating the signed url/cookies, public keys stored in cloudfront and privte keys goes to signer. Since, for this account, a root account is required, best practice is to avoid the operation","title":"01 Cloudfront"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/01%20Cloudfront/#cloudfront-overview","text":"CDN (Content Delivery Network) Content is cached in the Edge Location There are more than 200 Edge Location all over the world This is a global service Only support web distribution Improve Read Performance Can expose any Internal HTTP endpoints External HTTP endpoints (Even on-premise server) Security DDoS Protection AWS Firewall Shield Integration If the Cache Control header has max-age is 0 , all the request will go to the Origin With cloudfront-viewer-country , we can determine, from which country the request is being made and redirect the traffic to specific url. To do so, have to make sure, viewer-request-events trigger the function Only root users can create cloudfront key-pairs","title":"Cloudfront Overview"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/01%20Cloudfront/#cloudfront-origin","text":"As S3 Bucket origin Use as Ingress i.e. upload file Distribute files Caching files at the Edge location OAI Origin Access Identity S3 Bucket only be accessed through Cloudfront As Custom Origin HTTP Application Load Balancer EC2 instance can be private SG of ALB must allow the Cloudfront Public IP EC2 Instance EC2 Instance must be public SG of the EC2 Instance must allow the Cloudfront Public IP S3 Static Website Any HTTP Backend (AWS Internal / on-premise) On Premise server can be used as a origin of Cloudfront","title":"Cloudfront Origin"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/01%20Cloudfront/#cloudfront-geo-restriction","text":"Two types of filtering Whitelist IP from Certain countries can access the content Blacklist IP from Certain countries can not access the content Determining the IP origin country is determined by using a 3rd party database","title":"Cloudfront Geo Restriction"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/01%20Cloudfront/#cloudfront-vs-s3-cross-replication","text":"Cloudfront Use for static content When content must be available on almost all region Caching is for certain times (TTL) Use for both READ (Caching) WRITE (ingress) S3 Cross Replication Use for dynamic content When content must be available in certain region in very low latency Content is always available Use for READ Only","title":"Cloudfront vs S3 Cross Replication"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/01%20Cloudfront/#cloudfront-query-string","text":"The delimiter character should be $ Parameter's name and values are case sensitive","title":"Cloudfront Query String"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/01%20Cloudfront/#cloudfront-origin-group","text":"Origin failover can be handled with using two origin Has two origin One is primary origin Other can be treated as secondary origin Cloudfront switch to secondary origin from primary origin if Primary origin fails Primary origin sends HTTP Fail Status Code","title":"Cloudfront Origin Group"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/01%20Cloudfront/#policies","text":"Cloudfront can ensure, From client object and request will be encrypted and use https When it comes to send response, if the object is not available in cache, cloudfront will fetch it from origin also in https format To enable both client -> cloudfront -> origin in https, we will need Viewer Protocol Policy (When HTTPS, there is ssl certificate installed in cloudfront) Origin Protocol Policy","title":"Policies"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/01%20Cloudfront/#cache-invalidation","text":"When new object is uploaded, to invalidate previous cache We can invalidate instantly We can wait for the existing cache to be invalidated Use versioned name","title":"Cache Invalidation"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/01%20Cloudfront/#ttl","text":"To make a object for a certain time, Configure origin to add a Cache-Control or Expires-Header Specify minimum TTL to the Cloudfront Cache Behevior Default value of TTL is 24 hours","title":"TTL"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/01%20Cloudfront/#cloudfront-function","text":"Cloudfront functions can be used Cache key normalization Header manipulation Status code modification and body generation Request Authorization","title":"Cloudfront Function"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/01%20Cloudfront/#cloudfront-signed-url-cookies","text":"Only root user can create key-pairs A root user can make max 2 key pairs After creating the signed url/cookies, public keys stored in cloudfront and privte keys goes to signer. Since, for this account, a root account is required, best practice is to avoid the operation","title":"Cloudfront Signed URL / Cookies"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/02%20Cloudfront%20Signed%20URL%20or%20Cloudfront%20Signed%20Cookies/","text":"Cloudfront Signed URL or Cloudfront Signed Cookies Signed URL To distribute premium contents Should have a policy With URL expiration Define IP Ranges that can access the content Trusted Signers (Who can create signed URLS) Signed URL vs Signed Cookies Signed URL for single content Signed Cookies for multiple content Cloudfront Signed URL vs S3-presigned URL Cloudfront Signed URL Allow a path, no matter the origin Account wide Key-pair, only root can manage it Filter by IP Date Expiration Leverage caching features S3 Pre-Signed URL Only allow S3 as origin Use the IAM key of the signer, (Pre-sign URL has the same principle as the signer) Filter by Expiration No caching available","title":"02 Cloudfront Signed URL or Cloudfront Signed Cookies"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/02%20Cloudfront%20Signed%20URL%20or%20Cloudfront%20Signed%20Cookies/#cloudfront-signed-url-or-cloudfront-signed-cookies","text":"","title":"Cloudfront Signed URL or Cloudfront Signed Cookies"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/02%20Cloudfront%20Signed%20URL%20or%20Cloudfront%20Signed%20Cookies/#signed-url","text":"To distribute premium contents Should have a policy With URL expiration Define IP Ranges that can access the content Trusted Signers (Who can create signed URLS)","title":"Signed URL"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/02%20Cloudfront%20Signed%20URL%20or%20Cloudfront%20Signed%20Cookies/#signed-url-vs-signed-cookies","text":"Signed URL for single content Signed Cookies for multiple content","title":"Signed URL vs Signed Cookies"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/02%20Cloudfront%20Signed%20URL%20or%20Cloudfront%20Signed%20Cookies/#cloudfront-signed-url-vs-s3-presigned-url","text":"","title":"Cloudfront Signed URL vs S3-presigned URL"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/02%20Cloudfront%20Signed%20URL%20or%20Cloudfront%20Signed%20Cookies/#cloudfront-signed-url","text":"Allow a path, no matter the origin Account wide Key-pair, only root can manage it Filter by IP Date Expiration Leverage caching features","title":"Cloudfront Signed URL"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/02%20Cloudfront%20Signed%20URL%20or%20Cloudfront%20Signed%20Cookies/#s3-pre-signed-url","text":"Only allow S3 as origin Use the IAM key of the signer, (Pre-sign URL has the same principle as the signer) Filter by Expiration No caching available","title":"S3 Pre-Signed URL"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/03%20Global%20Accelerator/","text":"Global Accelerator Discussed in the VPC section Problem When we have an Application in another region, we have to reach that application through lots of ISP Provider When EC2 has a Public IP and some regions having difficulties with this Public IP access Since AWS Accelerator is using Any Cast , this problem can be resolved UniCast and Any Cast IP Uni Cast IP means each server has one IP Any Cast Ip means Multiple server has same IP Traffic routed to nearest server AWS Global Accelerator uses the Any Cast IP concept Overview Use 2 Any Cast IP Leverage AWS Internal Network Any Cast IP send traffic to the nearest AWS Edge Location From the Edge Location , traffic goes to the server using AWS Internal Network Works with Elastic IP Public/Private EC2 Instance Public/Private ALB Public/Private NLB Performance User intelligent routing to ensure lowest latency Use Internal AWS Network Has health check If issue with an application, has automatic failover feature So good for disaster recovery Security Only 2 IP needs to be white listed DDoS Protection by AWS Shield Global Accelerator Vs Cloudfront Both use AWS Global Network i.e. Edge Locations Both use Shield for DDoS protection Cloudfront Cache Content Global Accelerator Improve performance for TCL and UDP Has Failover","title":"03 Global Accelerator"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/03%20Global%20Accelerator/#global-accelerator","text":"Discussed in the VPC section","title":"Global Accelerator"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/03%20Global%20Accelerator/#problem","text":"When we have an Application in another region, we have to reach that application through lots of ISP Provider When EC2 has a Public IP and some regions having difficulties with this Public IP access Since AWS Accelerator is using Any Cast , this problem can be resolved","title":"Problem"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/03%20Global%20Accelerator/#unicast-and-any-cast-ip","text":"Uni Cast IP means each server has one IP Any Cast Ip means Multiple server has same IP Traffic routed to nearest server AWS Global Accelerator uses the Any Cast IP concept","title":"UniCast and Any Cast IP"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/03%20Global%20Accelerator/#overview","text":"Use 2 Any Cast IP Leverage AWS Internal Network Any Cast IP send traffic to the nearest AWS Edge Location From the Edge Location , traffic goes to the server using AWS Internal Network Works with Elastic IP Public/Private EC2 Instance Public/Private ALB Public/Private NLB Performance User intelligent routing to ensure lowest latency Use Internal AWS Network Has health check If issue with an application, has automatic failover feature So good for disaster recovery Security Only 2 IP needs to be white listed DDoS Protection by AWS Shield","title":"Overview"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/03%20Global%20Accelerator/#global-accelerator-vs-cloudfront","text":"Both use AWS Global Network i.e. Edge Locations Both use Shield for DDoS protection","title":"Global Accelerator Vs Cloudfront"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/03%20Global%20Accelerator/#cloudfront","text":"Cache Content","title":"Cloudfront"},{"location":"Notes/Cloudfront%20And%20Global%20Accerator/03%20Global%20Accelerator/#global-accelerator_1","text":"Improve performance for TCL and UDP Has Failover","title":"Global Accelerator"},{"location":"Notes/DB/01%20Redshift/","text":"Redshift Its an OLAP i.e. Online Analytics Processing based on PostgreSQL Columnar Storage MPP i.e. Massively Parallel Query Pay as instance provisioned For long term consider using Reserved Instance Has SQL interface to perform query BI tools are integrated with it AWS Quicksight Tableau Data can be loaded from S3 DynamoDB DMS It can have nodes number from 1 to 128 Each node can contain 160GB data Two types of node Leader Node , do the planning and results aggregation Compute Node , perform queries and send results to the leader Using VPC Enhanced Routing , Redshift Clusters can be access through the AWS Private Network Snapshot and DR Snapshots are Point in time backup of a cluster Snapshots are stored in S3 Snapshots are incremental, on changes are saved Snapshots can be restored to a new cluster Snapshots can be copied to another AWS Region Manual snapshot does not delete automatically Automatic snapshot has an automatic retention period (35 days) Monitor performance of Redshift Cluster by Cloudwatch and AWS Trusted Advisor Can be enabled cross-region snapshots for Redshift Cluster Redshift Spectrum Direct query to S3 without loading Must have a Cluster available to start the query Query is submitted to thousands of Redshift Spectrum Nodes Best Practices To load data from S3 use the COPY command","title":"01 Redshift"},{"location":"Notes/DB/01%20Redshift/#redshift","text":"Its an OLAP i.e. Online Analytics Processing based on PostgreSQL Columnar Storage MPP i.e. Massively Parallel Query Pay as instance provisioned For long term consider using Reserved Instance Has SQL interface to perform query BI tools are integrated with it AWS Quicksight Tableau Data can be loaded from S3 DynamoDB DMS It can have nodes number from 1 to 128 Each node can contain 160GB data Two types of node Leader Node , do the planning and results aggregation Compute Node , perform queries and send results to the leader Using VPC Enhanced Routing , Redshift Clusters can be access through the AWS Private Network","title":"Redshift"},{"location":"Notes/DB/01%20Redshift/#snapshot-and-dr","text":"Snapshots are Point in time backup of a cluster Snapshots are stored in S3 Snapshots are incremental, on changes are saved Snapshots can be restored to a new cluster Snapshots can be copied to another AWS Region Manual snapshot does not delete automatically Automatic snapshot has an automatic retention period (35 days) Monitor performance of Redshift Cluster by Cloudwatch and AWS Trusted Advisor Can be enabled cross-region snapshots for Redshift Cluster","title":"Snapshot and DR"},{"location":"Notes/DB/01%20Redshift/#redshift-spectrum","text":"Direct query to S3 without loading Must have a Cluster available to start the query Query is submitted to thousands of Redshift Spectrum Nodes","title":"Redshift Spectrum"},{"location":"Notes/DB/01%20Redshift/#best-practices","text":"To load data from S3 use the COPY command","title":"Best Practices"},{"location":"Notes/DB/02%20RDS/","text":"RDS OLTP i.e. Online Transaction Processing Relational Database -Managed PostgreSQL MySQL Oracle MSSQL Must provision EC2 Instance EBS Volume Support Read Replica for performance Multi AZ for digester recovery and availability Have Backup Snapshot Point in time restore Managed and scheduled maintenance Monitoring through Cloudwatch IAM authentication can be used as a feature in PostgreSQL MySQL To increase the number of db connection increase max_connection (resource heavy and not optimize) Create a parameter group Attach parameter group to DB Instance Change the parameter group settings RDS_Proxy Manage a connection pool Reuse the connections from the connction pools Instead of directly connected with the database, use the proxy When the db instance CPU is 100% and stopped working, we can Use read replica or cross-region read replica Use elastic cache in the application layer Shard data among multiple RDS DB instance For async database copy, use READ Replica While using READ Replica , in these READ Replica , there should be some replication lag When primary instance failed the CNAME of the DB Instance switch to Standby Instance Non supported oracle feature RMAN RAC In multi-AZ deployment, Standby Instance can not be used for read and write operations Automated Backup Take snapshot every 24 hours RDS take Snapshot of the whole database instance It captures the transaction logs of every 5 minutes A new DB Instance can be created from the from the DB Snapshot Encryption Un-encrypted database can not be encrypted on the fly (This is a limitation) Un-encrypted database read replica can not be encrypted To encrypted the un-encrypted database Create a DB snapshot Copy the snapshot Encrypt the copied snapshot Restore database from the Encrypted Snapshot To encrypt data while write and decrypt during read, there is Transparent Data Encryption or TDE TDE is for data encryption whereas the regular RDS Encryption is for encrypt the ec2 instance and ebs volume TDE is only applicable for the microsoft sql server Security IAM DB Authentication can be used for MySQL and PostgreSQL IAM DB Authentication has following feature SSL Encryption of Network Traffic Application runs in EC2 Instance can connect with database without password To enable security between Web Server and DB server Force ssl by rds.force_ssl Download RDS Root CA Certificate Monitoring Default Monitoring CPU Utilization Database Connection Free able Memory Enhanced Monitoring RDS Process OS Process Get logs of Audit Log Error Log General Log Slow query log","title":"02 RDS"},{"location":"Notes/DB/02%20RDS/#rds","text":"OLTP i.e. Online Transaction Processing Relational Database -Managed PostgreSQL MySQL Oracle MSSQL Must provision EC2 Instance EBS Volume Support Read Replica for performance Multi AZ for digester recovery and availability Have Backup Snapshot Point in time restore Managed and scheduled maintenance Monitoring through Cloudwatch IAM authentication can be used as a feature in PostgreSQL MySQL To increase the number of db connection increase max_connection (resource heavy and not optimize) Create a parameter group Attach parameter group to DB Instance Change the parameter group settings RDS_Proxy Manage a connection pool Reuse the connections from the connction pools Instead of directly connected with the database, use the proxy When the db instance CPU is 100% and stopped working, we can Use read replica or cross-region read replica Use elastic cache in the application layer Shard data among multiple RDS DB instance For async database copy, use READ Replica While using READ Replica , in these READ Replica , there should be some replication lag When primary instance failed the CNAME of the DB Instance switch to Standby Instance Non supported oracle feature RMAN RAC In multi-AZ deployment, Standby Instance can not be used for read and write operations","title":"RDS"},{"location":"Notes/DB/02%20RDS/#automated-backup","text":"Take snapshot every 24 hours RDS take Snapshot of the whole database instance It captures the transaction logs of every 5 minutes A new DB Instance can be created from the from the DB Snapshot","title":"Automated Backup"},{"location":"Notes/DB/02%20RDS/#encryption","text":"Un-encrypted database can not be encrypted on the fly (This is a limitation) Un-encrypted database read replica can not be encrypted To encrypted the un-encrypted database Create a DB snapshot Copy the snapshot Encrypt the copied snapshot Restore database from the Encrypted Snapshot To encrypt data while write and decrypt during read, there is Transparent Data Encryption or TDE TDE is for data encryption whereas the regular RDS Encryption is for encrypt the ec2 instance and ebs volume TDE is only applicable for the microsoft sql server","title":"Encryption"},{"location":"Notes/DB/02%20RDS/#security","text":"IAM DB Authentication can be used for MySQL and PostgreSQL IAM DB Authentication has following feature SSL Encryption of Network Traffic Application runs in EC2 Instance can connect with database without password To enable security between Web Server and DB server Force ssl by rds.force_ssl Download RDS Root CA Certificate","title":"Security"},{"location":"Notes/DB/02%20RDS/#monitoring","text":"Default Monitoring CPU Utilization Database Connection Free able Memory Enhanced Monitoring RDS Process OS Process Get logs of Audit Log Error Log General Log Slow query log","title":"Monitoring"},{"location":"Notes/DB/03%20Aurora/","text":"Aurora Proprietary database by AWS Data is replicated to 6 Replicas in 3 AZ Auto healing capability Multi AZ Auto scaling read-replica Storage from 10GB to 64TB Aurora Global database for DR and Latency Uses Need to provision EC2 Instance and EBS Volume Using Aurora Serverless does not require instance and volume 5x faster than regular RDMS Support MAX 15 Read Replicas We can create custom endpoint for Production Database Reporting queries Built in reader-endpoint can be used for distribute the traffic between read-replicas If DB Primary Instance fails, it create a new DB Instance in the same AZ as the Original Instance and done by Best Effort basis Read Replicas can be used to avoid un necessary downtime With binlog replication, another aurora cluster can be used as read replica. Aurora cluster can be same or different region Aurora auto scaling policy can be used to adjust performance Based of number of connections (when the lambda function is using the database)","title":"03 Aurora"},{"location":"Notes/DB/03%20Aurora/#aurora","text":"Proprietary database by AWS Data is replicated to 6 Replicas in 3 AZ Auto healing capability Multi AZ Auto scaling read-replica Storage from 10GB to 64TB Aurora Global database for DR and Latency Uses Need to provision EC2 Instance and EBS Volume Using Aurora Serverless does not require instance and volume 5x faster than regular RDMS Support MAX 15 Read Replicas We can create custom endpoint for Production Database Reporting queries Built in reader-endpoint can be used for distribute the traffic between read-replicas If DB Primary Instance fails, it create a new DB Instance in the same AZ as the Original Instance and done by Best Effort basis Read Replicas can be used to avoid un necessary downtime With binlog replication, another aurora cluster can be used as read replica. Aurora cluster can be same or different region Aurora auto scaling policy can be used to adjust performance Based of number of connections (when the lambda function is using the database)","title":"Aurora"},{"location":"Notes/DB/04%20Elastic%20Cache/","text":"Elastic Cache Key-value store In memory database Sub millisecond performance 2 types Memcached Redis Support Clustering (Redis) Multi AZ Read Replicas (sharding) Have Backup Snapshot Point in time recovery Scheduled Maintenance Monitoring through Cloudwatch Redis has authentication feature Redis Auth To secure the redis cluster access Use redis auth in transit encryption, enabled for clusters When Cluster Mode is enabled All nodes should reside in the same region Can not promote any more read replica In most cases, it's better to use Redis. But should choice Memcached when, Required the simplest model possible Multi-thread or multi core supports Redis is not primarily designed for using the multi-cpu. Caching Strategies Lazy Loading : Loads data only when it is required Russian doll : Have nested records. Top level keys are the cahce keys for child resources. Write Througth : Adds item to cache when a item is added or updated. may cause cachec churn ie. lots of cache is not being used or read.","title":"04 Elastic Cache"},{"location":"Notes/DB/04%20Elastic%20Cache/#elastic-cache","text":"Key-value store In memory database Sub millisecond performance 2 types Memcached Redis Support Clustering (Redis) Multi AZ Read Replicas (sharding) Have Backup Snapshot Point in time recovery Scheduled Maintenance Monitoring through Cloudwatch Redis has authentication feature Redis Auth To secure the redis cluster access Use redis auth in transit encryption, enabled for clusters When Cluster Mode is enabled All nodes should reside in the same region Can not promote any more read replica In most cases, it's better to use Redis. But should choice Memcached when, Required the simplest model possible Multi-thread or multi core supports Redis is not primarily designed for using the multi-cpu.","title":"Elastic Cache"},{"location":"Notes/DB/04%20Elastic%20Cache/#caching-strategies","text":"Lazy Loading : Loads data only when it is required Russian doll : Have nested records. Top level keys are the cahce keys for child resources. Write Througth : Adds item to cache when a item is added or updated. may cause cachec churn ie. lots of cache is not being used or read.","title":"Caching Strategies"},{"location":"Notes/DB/05%20DynamoDB/","text":"Check the serverless section","title":"05 DynamoDB"},{"location":"Notes/DB/06%20S3/","text":"Check the S3 section","title":"06 S3"},{"location":"Notes/DB/07%20Athena/","text":"Check the S3 section","title":"07 Athena"},{"location":"Notes/DB/08%20Neptune/","text":"Neptune Fully managed Graph Database Use for Highly Relational Data Available across 3AZ Can have 15 Read Replicas Has Point in time recovery Continuous Backup","title":"08 Neptune"},{"location":"Notes/DB/08%20Neptune/#neptune","text":"Fully managed Graph Database Use for Highly Relational Data Available across 3AZ Can have 15 Read Replicas Has Point in time recovery Continuous Backup","title":"Neptune"},{"location":"Notes/DB/09%20Elastic%20Search/","text":"Elastic Search Use for Searching and Indexing Open source Use to search any field or partially search It's common to use Elastic Search as a compliment of other databases Used in big data application Need to provision a cluster of instances Built in integrations of Amazon Kinesis Data Firehose AWS IoT Cloudwatch Cloudwatch logs ELK stack stands for Elastic Search Logstash (Log Ingestion) Kibana (Visualization)","title":"09 Elastic Search"},{"location":"Notes/DB/09%20Elastic%20Search/#elastic-search","text":"Use for Searching and Indexing Open source Use to search any field or partially search It's common to use Elastic Search as a compliment of other databases Used in big data application Need to provision a cluster of instances Built in integrations of Amazon Kinesis Data Firehose AWS IoT Cloudwatch Cloudwatch logs ELK stack stands for Elastic Search Logstash (Log Ingestion) Kibana (Visualization)","title":"Elastic Search"},{"location":"Notes/Decoupling%20Application/01%20SQS/","text":"SQS Overview Fully manage message broker system Unlimited throughput Unlimited number of messages Retention period Default 4 days Max 14 days Max message size 256 KB Can have duplicate message At least once delivery Can have out of order Use best effort ordering Applicable for SQS Standard FIFO SQS maintain order but limited throughput Message can be produced by the SQS SDK Message can be consumed by AWS Lambda , EC2 Instance Consumer can pull max 10 messages at a time After process the message, the consumer has to delete the message, otherwise the message will be appeared in the queue again Can be use to handle extended number of db write operations After a queue is being created, we can not change the type (FIFO vs regular), instead we need to recreate it Properties DeleteQueue : Use to delete the queue along with all messages PurgeQueue : Use to delete the messages of the queue We can ignore default message size from 256 KB to up to 2GB by making use of S3 and SQS Extended library for Java Security IAM Policies can be used to regulate the SQS API SQS Access Policy Use for Cross Account Access Allowing other service like S3 , SNS Encryption In flight encryption using the HTTPS At rest encryption using KMS , can be enabled without changing the code Client side encryption Encryption be done by Message Producers Decryption can be by the Message Consumers Message Visibility Timeout After message is polled by a consumer, the message become invisible for other consumer Default visibility timeout is 30 Seconds Is message is not proceed by the consumer within the visibility timeout, it will appear again in the queue and might possibility to proceed again by another consumer If message require more time than Visibility Timeout , the consumer can call the api ChangeMessageVisibility to increase Message Visibility Timeout Best practices If the Message Visibility Time too high and consumer crashes, it took long time to re appear the message and proceed If Message Visibility Time is too low, it is possible to duplicate processing Better to set minimum Message Visibility Time and increase by calling ChangeMessageVisibility api if necessary Dead Letter Queue If consumer fails to process a message couple of times it can be passed to the DLQ We can set the threshold, how many fails attempts to allow Best practice to set retention period of 14 days for dead letter queue While creating a Dead Letter Queue , For standard queue create Standard Dead Letter Queue in the same AWS account For fifo queue create Fifo Dead Letter Queue in the same AWS account To define the Dead Letter Queue in the lambda use Amazon Resource Name in the DeadLetterConfig parameter There should be defined failed attempts for processing the message The lambda function invocation should be asynchronus Delay Queue Using delay queue the message will be appeared in the queue after certain times Time Default is 0 Sec Could be max 15 Min FIFO Queue Maintain Orders (First In First Out) Limited throughput 300 msg/s without batching 3000 msg/s with batching De duplication ensure multiple message with same ID does not appear When message has identical bodies, use unique de duplication id When message has unique message bodies, use content-based de duplication ID Only FIFO queue has this feature Message group Id ensure the order of message is being proceed De duplication id prevent duplication, group id ensure message is being ordered A regular queue can not be converted to a FIFO queue. I should be created from scratch SQS With ASG ASG can be implemented to scale the Consumers Using Cloudwatch ApproximateNumberOfMessages can be determined number of messages in the queue Using Cloudwatch ApproximateNumberOfMessages can be scaled up/down the EC2 Instances Need two alarm to scale up/down the instance Step Scaling is being used here Migration, Queue to Fifo Queue Need Message de-duplication Id (As token while sending the message, ensure no message duplication happen) Message group ID (As tag, to make message group, ensure order of message been proceed) Performance Use long poling Use batch processing With AWS Lambda For standard queue use the long poling For FIFO queue use the group id and en-sure, all the message of the group is being proceed","title":"01 SQS"},{"location":"Notes/Decoupling%20Application/01%20SQS/#sqs","text":"","title":"SQS"},{"location":"Notes/Decoupling%20Application/01%20SQS/#overview","text":"Fully manage message broker system Unlimited throughput Unlimited number of messages Retention period Default 4 days Max 14 days Max message size 256 KB Can have duplicate message At least once delivery Can have out of order Use best effort ordering Applicable for SQS Standard FIFO SQS maintain order but limited throughput Message can be produced by the SQS SDK Message can be consumed by AWS Lambda , EC2 Instance Consumer can pull max 10 messages at a time After process the message, the consumer has to delete the message, otherwise the message will be appeared in the queue again Can be use to handle extended number of db write operations After a queue is being created, we can not change the type (FIFO vs regular), instead we need to recreate it Properties DeleteQueue : Use to delete the queue along with all messages PurgeQueue : Use to delete the messages of the queue We can ignore default message size from 256 KB to up to 2GB by making use of S3 and SQS Extended library for Java","title":"Overview"},{"location":"Notes/Decoupling%20Application/01%20SQS/#security","text":"IAM Policies can be used to regulate the SQS API SQS Access Policy Use for Cross Account Access Allowing other service like S3 , SNS Encryption In flight encryption using the HTTPS At rest encryption using KMS , can be enabled without changing the code Client side encryption Encryption be done by Message Producers Decryption can be by the Message Consumers","title":"Security"},{"location":"Notes/Decoupling%20Application/01%20SQS/#message-visibility-timeout","text":"After message is polled by a consumer, the message become invisible for other consumer Default visibility timeout is 30 Seconds Is message is not proceed by the consumer within the visibility timeout, it will appear again in the queue and might possibility to proceed again by another consumer If message require more time than Visibility Timeout , the consumer can call the api ChangeMessageVisibility to increase Message Visibility Timeout Best practices If the Message Visibility Time too high and consumer crashes, it took long time to re appear the message and proceed If Message Visibility Time is too low, it is possible to duplicate processing Better to set minimum Message Visibility Time and increase by calling ChangeMessageVisibility api if necessary","title":"Message Visibility Timeout"},{"location":"Notes/Decoupling%20Application/01%20SQS/#dead-letter-queue","text":"If consumer fails to process a message couple of times it can be passed to the DLQ We can set the threshold, how many fails attempts to allow Best practice to set retention period of 14 days for dead letter queue While creating a Dead Letter Queue , For standard queue create Standard Dead Letter Queue in the same AWS account For fifo queue create Fifo Dead Letter Queue in the same AWS account To define the Dead Letter Queue in the lambda use Amazon Resource Name in the DeadLetterConfig parameter There should be defined failed attempts for processing the message The lambda function invocation should be asynchronus","title":"Dead Letter Queue"},{"location":"Notes/Decoupling%20Application/01%20SQS/#delay-queue","text":"Using delay queue the message will be appeared in the queue after certain times Time Default is 0 Sec Could be max 15 Min","title":"Delay Queue"},{"location":"Notes/Decoupling%20Application/01%20SQS/#fifo-queue","text":"Maintain Orders (First In First Out) Limited throughput 300 msg/s without batching 3000 msg/s with batching De duplication ensure multiple message with same ID does not appear When message has identical bodies, use unique de duplication id When message has unique message bodies, use content-based de duplication ID Only FIFO queue has this feature Message group Id ensure the order of message is being proceed De duplication id prevent duplication, group id ensure message is being ordered A regular queue can not be converted to a FIFO queue. I should be created from scratch","title":"FIFO Queue"},{"location":"Notes/Decoupling%20Application/01%20SQS/#sqs-with-asg","text":"ASG can be implemented to scale the Consumers Using Cloudwatch ApproximateNumberOfMessages can be determined number of messages in the queue Using Cloudwatch ApproximateNumberOfMessages can be scaled up/down the EC2 Instances Need two alarm to scale up/down the instance Step Scaling is being used here","title":"SQS With ASG"},{"location":"Notes/Decoupling%20Application/01%20SQS/#migration-queue-to-fifo-queue","text":"Need Message de-duplication Id (As token while sending the message, ensure no message duplication happen) Message group ID (As tag, to make message group, ensure order of message been proceed)","title":"Migration, Queue to Fifo Queue"},{"location":"Notes/Decoupling%20Application/01%20SQS/#performance","text":"Use long poling Use batch processing With AWS Lambda For standard queue use the long poling For FIFO queue use the group id and en-sure, all the message of the group is being proceed","title":"Performance"},{"location":"Notes/Decoupling%20Application/02%20SNS/","text":"SNS Simple Notification Service Pub/Sub Model Event Producer send the message to the SNS Event Receiver receive notification from the SNS Subscriber can be SQS HTTP/HTTPS Endpoint Lambda Function Email SMS Mobile Notification Filtering: Using filter in policy to filter message before publish Use Case Cloudwatch alarm ASG Notification S3 bucket Events Cloudformation State Change Publish Topic Publish Create a topic Create subscription Publish Topic Direct Publish (In Platform) Create Platform Create Platform Endpoint Publish to Platform Endpoint Works with Google GCM Apple APNS Amazon ADM Security Encryption In flight encryption by HTTPS endpoint At rest encryption by KMS Client side encryption and decryption Publisher is responsible to encrypt the message Subscriber is responsible to decrypt the message Access Controls by IAM Policy SNS Access Policy Sharing cross-account SNS Allow other service to allow Publish Topic","title":"02 SNS"},{"location":"Notes/Decoupling%20Application/02%20SNS/#sns","text":"Simple Notification Service Pub/Sub Model Event Producer send the message to the SNS Event Receiver receive notification from the SNS Subscriber can be SQS HTTP/HTTPS Endpoint Lambda Function Email SMS Mobile Notification Filtering: Using filter in policy to filter message before publish Use Case Cloudwatch alarm ASG Notification S3 bucket Events Cloudformation State Change Publish Topic Publish Create a topic Create subscription Publish Topic Direct Publish (In Platform) Create Platform Create Platform Endpoint Publish to Platform Endpoint Works with Google GCM Apple APNS Amazon ADM Security Encryption In flight encryption by HTTPS endpoint At rest encryption by KMS Client side encryption and decryption Publisher is responsible to encrypt the message Subscriber is responsible to decrypt the message Access Controls by IAM Policy SNS Access Policy Sharing cross-account SNS Allow other service to allow Publish Topic","title":"SNS"},{"location":"Notes/Decoupling%20Application/03%20Fan%20Out/","text":"Fan Out SNS + SQS Push event in one SNS and received by many SQS SQS access policy should allow SNS to publish message FIFO SQS can not be a subscriber to a SNS Used when one topic should be published to multiple SQS","title":"03 Fan Out"},{"location":"Notes/Decoupling%20Application/03%20Fan%20Out/#fan-out","text":"SNS + SQS Push event in one SNS and received by many SQS SQS access policy should allow SNS to publish message FIFO SQS can not be a subscriber to a SNS Used when one topic should be published to multiple SQS","title":"Fan Out"},{"location":"Notes/Decoupling%20Application/04%20Kinesis%20Data%20Stream/","text":"Kinesis Data Stream Managed Apache Kafka Used for Real Time Big Data Cover streaming processing framework like Spark NiFi Data is replication across 3 AZ 3 tools included Kinesis Stream (Low latency streaming at scale) Kinesis Analytics (Real time analytics using SQL) Kinesis Firehose (Load data to S3 , RedShift , Elastic Search ) Using a Kinesis Data Stream Consumer ensure a dedicated connection for each shard. Helps to reduce the latency. To ensure preventing duplication, sequence and only once process Use timestamp as sequenceNumberForOrdering parameter Use putRecord instead of putRecords to prevent duplication Use Unique id in external service (like DynamoDB), that will be checked before process a data Security Control Access by IAM Policy Encryption In flight by HTTPS Install SSL certificate kinesis Send data through SSL At rest by KMS Encryption is enabled at rest Ensure streams are transferring data from producers Client side encryption VPC Endpoints are available to access Kinesis through AWS Private Network Stream Shards One stream is combination of multiple shard Each shard throughput Read 2MB/s Write 1MB/s or 1000 message/s Batching is available to reduce the cost and increase throughput Number of shards can be merged or re-shards over time Records are ordered per shard Multiple shard can not ensure the ordering Merging shards to process less data Splitting shards to process more data Kinesis API (Put Records) Same key always go to same partition Partition keys should be highly distributed, otherwise it cause Hot Partition Problem If partition key is userID, it is highly distributed If partition key is country code and 90% users are from the same country then 90% data will go to the same shard ProvisionThroughputExceed happens when we go over limit Hot Sharding Happen Solution of ProvisionThroughputExceed be Retry Increase shard Ensure partition key is a good one As consumer we can use CLI SDK Kinesis Client Library i.e. KCL available for almost all major languages 01 Kinesis Stream Streams are divided into shards Data retention period Default 1 Day Max 7 Days Data can be proceed multiple time (In SQS we can process message only one time) Multiple consumer can consume the data Once data is inserted in the shards it can not be deleted 02 Kinesis Firehose Managed Service No Administration Auto Scaling Serverless Load data to S3 Elastic Search Redshift Splunk Near real time 60 Sec Or Minimum 32Mb Can get data from Kinesis Stream KCL (Kinesis Client Library) 03 Kinesis Data Analytics Perform real time analytics using SQL Features Auto Scaling Managed Real time Firehose Vs Streams Streams Custom code for producer and consumer Real time Must manage scaling (Re shards and merging) Data store for 1-7 days Multi consumers Replay capability Firehose Fully managed, serverless Near real time Automated scaling No data storage Re-Sharding Enables increase or decrease of the number of shards in the stream. We can increase instance size and shards number to handle more data. To get optimize performance, do not use more instances than the shards. Kinesis Client Library (KCL) In instances, used to process data from data stream. There should be same number of kcl as well as the ec2 instances as the number of open shards ( kcl shards = no of ec2 instances ). To process data, if we make use of the lambda functions, to get optimal performance, need to have same number of concurrent lambda function same as the shard number. Enhanced Fan Out Enabled shards get data 2MB/s per shard with 70s propagation delay to all consumers.","title":"04 Kinesis Data Stream"},{"location":"Notes/Decoupling%20Application/04%20Kinesis%20Data%20Stream/#kinesis-data-stream","text":"Managed Apache Kafka Used for Real Time Big Data Cover streaming processing framework like Spark NiFi Data is replication across 3 AZ 3 tools included Kinesis Stream (Low latency streaming at scale) Kinesis Analytics (Real time analytics using SQL) Kinesis Firehose (Load data to S3 , RedShift , Elastic Search ) Using a Kinesis Data Stream Consumer ensure a dedicated connection for each shard. Helps to reduce the latency. To ensure preventing duplication, sequence and only once process Use timestamp as sequenceNumberForOrdering parameter Use putRecord instead of putRecords to prevent duplication Use Unique id in external service (like DynamoDB), that will be checked before process a data","title":"Kinesis Data Stream"},{"location":"Notes/Decoupling%20Application/04%20Kinesis%20Data%20Stream/#security","text":"Control Access by IAM Policy Encryption In flight by HTTPS Install SSL certificate kinesis Send data through SSL At rest by KMS Encryption is enabled at rest Ensure streams are transferring data from producers Client side encryption VPC Endpoints are available to access Kinesis through AWS Private Network","title":"Security"},{"location":"Notes/Decoupling%20Application/04%20Kinesis%20Data%20Stream/#stream-shards","text":"One stream is combination of multiple shard Each shard throughput Read 2MB/s Write 1MB/s or 1000 message/s Batching is available to reduce the cost and increase throughput Number of shards can be merged or re-shards over time Records are ordered per shard Multiple shard can not ensure the ordering Merging shards to process less data Splitting shards to process more data","title":"Stream Shards"},{"location":"Notes/Decoupling%20Application/04%20Kinesis%20Data%20Stream/#kinesis-api-put-records","text":"Same key always go to same partition Partition keys should be highly distributed, otherwise it cause Hot Partition Problem If partition key is userID, it is highly distributed If partition key is country code and 90% users are from the same country then 90% data will go to the same shard ProvisionThroughputExceed happens when we go over limit Hot Sharding Happen Solution of ProvisionThroughputExceed be Retry Increase shard Ensure partition key is a good one As consumer we can use CLI SDK Kinesis Client Library i.e. KCL available for almost all major languages","title":"Kinesis API (Put Records)"},{"location":"Notes/Decoupling%20Application/04%20Kinesis%20Data%20Stream/#01-kinesis-stream","text":"Streams are divided into shards Data retention period Default 1 Day Max 7 Days Data can be proceed multiple time (In SQS we can process message only one time) Multiple consumer can consume the data Once data is inserted in the shards it can not be deleted","title":"01 Kinesis Stream"},{"location":"Notes/Decoupling%20Application/04%20Kinesis%20Data%20Stream/#02-kinesis-firehose","text":"Managed Service No Administration Auto Scaling Serverless Load data to S3 Elastic Search Redshift Splunk Near real time 60 Sec Or Minimum 32Mb Can get data from Kinesis Stream KCL (Kinesis Client Library)","title":"02 Kinesis Firehose"},{"location":"Notes/Decoupling%20Application/04%20Kinesis%20Data%20Stream/#03-kinesis-data-analytics","text":"Perform real time analytics using SQL Features Auto Scaling Managed Real time","title":"03 Kinesis Data Analytics"},{"location":"Notes/Decoupling%20Application/04%20Kinesis%20Data%20Stream/#firehose-vs-streams","text":"Streams Custom code for producer and consumer Real time Must manage scaling (Re shards and merging) Data store for 1-7 days Multi consumers Replay capability Firehose Fully managed, serverless Near real time Automated scaling No data storage","title":"Firehose Vs Streams"},{"location":"Notes/Decoupling%20Application/04%20Kinesis%20Data%20Stream/#re-sharding","text":"Enables increase or decrease of the number of shards in the stream. We can increase instance size and shards number to handle more data. To get optimize performance, do not use more instances than the shards.","title":"Re-Sharding"},{"location":"Notes/Decoupling%20Application/04%20Kinesis%20Data%20Stream/#kinesis-client-library-kcl","text":"In instances, used to process data from data stream. There should be same number of kcl as well as the ec2 instances as the number of open shards ( kcl shards = no of ec2 instances ). To process data, if we make use of the lambda functions, to get optimal performance, need to have same number of concurrent lambda function same as the shard number.","title":"Kinesis Client Library (KCL)"},{"location":"Notes/Decoupling%20Application/04%20Kinesis%20Data%20Stream/#enhanced-fan-out","text":"Enabled shards get data 2MB/s per shard with 70s propagation delay to all consumers.","title":"Enhanced Fan Out"},{"location":"Notes/Decoupling%20Application/05%20Amazon%20MQ/","text":"Amazon MQ Managed Apache ActiveMQ Amazon MQ has both Queue feature Topic feature Compatible with MQTT AMQP STOMP OPEN-WIRE WSS SQS and SNS are proprietary tech If application use existing open-source message broker system, then need to use Amazon MQ Amazon MQ does not scale as SQS and SNS Amazon MQ run on dedicated machine, can be used as HA Failover","title":"05 Amazon MQ"},{"location":"Notes/Decoupling%20Application/05%20Amazon%20MQ/#amazon-mq","text":"Managed Apache ActiveMQ Amazon MQ has both Queue feature Topic feature Compatible with MQTT AMQP STOMP OPEN-WIRE WSS SQS and SNS are proprietary tech If application use existing open-source message broker system, then need to use Amazon MQ Amazon MQ does not scale as SQS and SNS Amazon MQ run on dedicated machine, can be used as HA Failover","title":"Amazon MQ"},{"location":"Notes/Disaster%20Recovery%20%26%20Migrations/01%20Disaster%20Recovery/","text":"Disaster Recovery DR is all about preparing and recovering from disaster DR can be On-premise to On-premise is an expensive one On-premise to AWS Cloud is hybrid one AWS Cloud to AWS Cloud Could be multi-AZ Could be cross-region AWS Service for DR Backup EBS Snapshots (copy to destination region) AMI Snapshots (Copy to destination region) RDS auto backup/snapshot S3 replication, glacier, lifecycle policy, cross-region-replication snowball, snowmobile, storage gateway HA cross-region multi-az site-to-site vpn, direct connect Replication RDS, Aurora Global Automation Cloudformation Elastic Beanstalk AWS lambda Chaos simian army","title":"01 Disaster Recovery"},{"location":"Notes/Disaster%20Recovery%20%26%20Migrations/01%20Disaster%20Recovery/#disaster-recovery","text":"DR is all about preparing and recovering from disaster DR can be On-premise to On-premise is an expensive one On-premise to AWS Cloud is hybrid one AWS Cloud to AWS Cloud Could be multi-AZ Could be cross-region AWS Service for DR Backup EBS Snapshots (copy to destination region) AMI Snapshots (Copy to destination region) RDS auto backup/snapshot S3 replication, glacier, lifecycle policy, cross-region-replication snowball, snowmobile, storage gateway HA cross-region multi-az site-to-site vpn, direct connect Replication RDS, Aurora Global Automation Cloudformation Elastic Beanstalk AWS lambda Chaos simian army","title":"Disaster Recovery"},{"location":"Notes/Disaster%20Recovery%20%26%20Migrations/02%20RPO%20%26%20RTO/","text":"RPO & RTO RPO stands for Recovery Point Objective How often backup being taken When disaster happen, time between RPO and Disaster is a Data Loss If we take backups more frequently Data Loss will be minimized In this case, it become expensive also RTO stands for Recovery Time Objective RTO is the time to recover from the Disaster When Disaster happen, time between Disaster and Time to Make Application Online is the Downtime The Lower Time the RPO and RTO , the better the application, also become expensive","title":"02 RPO & RTO"},{"location":"Notes/Disaster%20Recovery%20%26%20Migrations/02%20RPO%20%26%20RTO/#rpo-rto","text":"RPO stands for Recovery Point Objective How often backup being taken When disaster happen, time between RPO and Disaster is a Data Loss If we take backups more frequently Data Loss will be minimized In this case, it become expensive also RTO stands for Recovery Time Objective RTO is the time to recover from the Disaster When Disaster happen, time between Disaster and Time to Make Application Online is the Downtime The Lower Time the RPO and RTO , the better the application, also become expensive","title":"RPO &amp; RTO"},{"location":"Notes/Disaster%20Recovery%20%26%20Migrations/03%20Disaster%20Recovery%20Strategies/","text":"Disaster Recovery Strategies 4 types of Disaster Recovery Strategies Backup and Restore Pilot Light Warm Standby Hot Site / Multi Site Approach Backup And Restore Data and applications backup is taken time to time When Disaster happen, restore these data and spin the application Has very high RPO and RTO and comparatively cheaper Pilot Light Application most critical part is always standby in other places In case of Disaster , add the non-critical part Not whole system is standby, only critical part is standby Warm Standby Minimum version of the whole system is standby Hot Site / Multi Site Approach Whole system is standby in another az or region or from op-premise to aws-cloud Most expensive one","title":"03 Disaster Recovery Strategies"},{"location":"Notes/Disaster%20Recovery%20%26%20Migrations/03%20Disaster%20Recovery%20Strategies/#disaster-recovery-strategies","text":"4 types of Disaster Recovery Strategies Backup and Restore Pilot Light Warm Standby Hot Site / Multi Site Approach","title":"Disaster Recovery Strategies"},{"location":"Notes/Disaster%20Recovery%20%26%20Migrations/03%20Disaster%20Recovery%20Strategies/#backup-and-restore","text":"Data and applications backup is taken time to time When Disaster happen, restore these data and spin the application Has very high RPO and RTO and comparatively cheaper","title":"Backup And Restore"},{"location":"Notes/Disaster%20Recovery%20%26%20Migrations/03%20Disaster%20Recovery%20Strategies/#pilot-light","text":"Application most critical part is always standby in other places In case of Disaster , add the non-critical part Not whole system is standby, only critical part is standby","title":"Pilot Light"},{"location":"Notes/Disaster%20Recovery%20%26%20Migrations/03%20Disaster%20Recovery%20Strategies/#warm-standby","text":"Minimum version of the whole system is standby","title":"Warm Standby"},{"location":"Notes/Disaster%20Recovery%20%26%20Migrations/03%20Disaster%20Recovery%20Strategies/#hot-site-multi-site-approach","text":"Whole system is standby in another az or region or from op-premise to aws-cloud Most expensive one","title":"Hot Site / Multi Site Approach"},{"location":"Notes/Disaster%20Recovery%20%26%20Migrations/04%20DMS/","text":"DMS DMS stands for Database Migration Service Use to migrate database from on-premise to RDS Supports Homogeneous Migration Source and destination DB has same data base engine Heterogeneous Migration Source and destination DB has different engine Require SCT i.e. Schema Conversion Tool While use SCT , first put the converted data to S3 Then put all the data to the Database It is CDC i.e. Continuous Data Migration If there is a change in source database, it is replicated to the destination database as well DMS needs to run in EC2 instance In order to allow DMS , the instance should have suitable AWS Identity and IAM Policy ( important ) This enable migrating production database without any interruption","title":"04 DMS"},{"location":"Notes/Disaster%20Recovery%20%26%20Migrations/04%20DMS/#dms","text":"DMS stands for Database Migration Service Use to migrate database from on-premise to RDS Supports Homogeneous Migration Source and destination DB has same data base engine Heterogeneous Migration Source and destination DB has different engine Require SCT i.e. Schema Conversion Tool While use SCT , first put the converted data to S3 Then put all the data to the Database It is CDC i.e. Continuous Data Migration If there is a change in source database, it is replicated to the destination database as well DMS needs to run in EC2 instance In order to allow DMS , the instance should have suitable AWS Identity and IAM Policy ( important ) This enable migrating production database without any interruption","title":"DMS"},{"location":"Notes/Disaster%20Recovery%20%26%20Migrations/05%20On%20Premise%20Strategy/","text":"On Premise Strategy Amazon Linux 2 can be downloaded to on-premise server. VM Import / Export Standby an AMI in VM and put it in AWS Cloud in case of Disaster Application Discovery Service Gather info about on-premise servers and plan migration AWS Server Migration Service , i.e. SMS Incremental replication of live server to AWS Automate, schedule, track incremental of live volumes Can be used for Hyper V machines","title":"05 On Premise Strategy"},{"location":"Notes/Disaster%20Recovery%20%26%20Migrations/05%20On%20Premise%20Strategy/#on-premise-strategy","text":"Amazon Linux 2 can be downloaded to on-premise server. VM Import / Export Standby an AMI in VM and put it in AWS Cloud in case of Disaster Application Discovery Service Gather info about on-premise servers and plan migration AWS Server Migration Service , i.e. SMS Incremental replication of live server to AWS Automate, schedule, track incremental of live volumes Can be used for Hyper V machines","title":"On Premise Strategy"},{"location":"Notes/Disaster%20Recovery%20%26%20Migrations/06%20Data%20Sync/","text":"Data Sync Use to transfer Large Data Sets Work with SMB and NFS protocol Data can be transferred from on-premise to S3 EFS FSx (for windows) Data can be transferred from EFS to EFS A Data Sync Client need to implement in the client It looks for the changes Transfer data time to time Hourly Daily Weekly While transfer data from on-premise to EFS , to ensure all data is copied, we can do the following for faster transfer Disable verification during initial file transfer Enable it post last data transfer","title":"06 Data Sync"},{"location":"Notes/Disaster%20Recovery%20%26%20Migrations/06%20Data%20Sync/#data-sync","text":"Use to transfer Large Data Sets Work with SMB and NFS protocol Data can be transferred from on-premise to S3 EFS FSx (for windows) Data can be transferred from EFS to EFS A Data Sync Client need to implement in the client It looks for the changes Transfer data time to time Hourly Daily Weekly While transfer data from on-premise to EFS , to ensure all data is copied, we can do the following for faster transfer Disable verification during initial file transfer Enable it post last data transfer","title":"Data Sync"},{"location":"Notes/Disaster%20Recovery%20%26%20Migrations/07%20Transferring%20Large%20Data%20Sets/","text":"Transferring Large Data Sets Sending 200TB data with 100Mbps through public Internet ( Site to Site VPN ) take 185 days Sending 200TB data with 1Gbps connection through Aws Direct Connect take 18 days Also consider 30 days to set up AWS Direct Connect Using snowball might take 7 days consider it will come to facility in 2-3 days","title":"07 Transferring Large Data Sets"},{"location":"Notes/Disaster%20Recovery%20%26%20Migrations/07%20Transferring%20Large%20Data%20Sets/#transferring-large-data-sets","text":"Sending 200TB data with 100Mbps through public Internet ( Site to Site VPN ) take 185 days Sending 200TB data with 1Gbps connection through Aws Direct Connect take 18 days Also consider 30 days to set up AWS Direct Connect Using snowball might take 7 days consider it will come to facility in 2-3 days","title":"Transferring Large Data Sets"},{"location":"Notes/EC2/01%20EC2%20Instance%20Recovery/","text":"EC2 Instance Recovery Using Cloudwatch Alarm we can recover EC2 Instance It recover previous Public IP Private IP Elastic IP (if attached) Metadata Placement Group It does not recover Instance Store Set up Check EC2 Instance Check EC2 Instance Underlying Hardware SNS Topic can be used to notify this incident","title":"01 EC2 Instance Recovery"},{"location":"Notes/EC2/01%20EC2%20Instance%20Recovery/#ec2-instance-recovery","text":"Using Cloudwatch Alarm we can recover EC2 Instance It recover previous Public IP Private IP Elastic IP (if attached) Metadata Placement Group It does not recover Instance Store Set up Check EC2 Instance Check EC2 Instance Underlying Hardware SNS Topic can be used to notify this incident","title":"EC2 Instance Recovery"},{"location":"Notes/EC2/02%20EC2%20Fundamentals/","text":"EC2 Fundamentals While restart an EC2 instance Public IP changes Both IP6 and private IP does not change It is possible to change the underlying hardware Pre warm for minimum launching time Launch EC2 Instance with EBS Volume Enable Hibernate To save cost Use reserved instance (Save up to 54%) Use EC2 Instance Saving Plan (For same instance family, save up to 72%) Nitro based instance Regular instance can handle only 32000 IO/ps Nitro based instance can handle more SSH protocol use TCP connection with PORT 22 EC2 instance billing When in running state When preparing to hibernate from the stopping state When reserved instance is in terminate state For High Performance EC2 Instance Use EFA Use Dedicated Instance When a reserved instance is no longer required Stop the instance, so it wont billed after expiration Sell the instance to the Reserved Instance Marketplace EC2 Classic instance can be launched outside of the VPC EC2 Enhanced Networking allows Higher Packet Per Second (PPS) Consistently lower inter instance latencies If Cluster Placement Group through insufficient Capacity Error , restart the instance, there's no such capacity limitation Warm Attach means attaching an ENI when it is Stopped Placement Group Cluster Tightly coupled used for HPC ( High Performance Computing ) Low latency network performance Partition Logical partition group Does not share underlying hardware Used for distributed and replicated workloads Hadoop Cassandra kafka Spread Distinct underlying hardware Logging aws ec3 describe-instances for get logs including the recently terminated instance Limitation There are limitation for launching vCPU in each region Default is 20 Submitting a request to increase the limit can work There is no cost for EIP , if there is only one EIP and it is associated with running EC2 Use common key-pairs in different region Create a public key form a private key Manually import the public keys to selected regions","title":"02 EC2 Fundamentals"},{"location":"Notes/EC2/02%20EC2%20Fundamentals/#ec2-fundamentals","text":"While restart an EC2 instance Public IP changes Both IP6 and private IP does not change It is possible to change the underlying hardware Pre warm for minimum launching time Launch EC2 Instance with EBS Volume Enable Hibernate To save cost Use reserved instance (Save up to 54%) Use EC2 Instance Saving Plan (For same instance family, save up to 72%) Nitro based instance Regular instance can handle only 32000 IO/ps Nitro based instance can handle more SSH protocol use TCP connection with PORT 22 EC2 instance billing When in running state When preparing to hibernate from the stopping state When reserved instance is in terminate state For High Performance EC2 Instance Use EFA Use Dedicated Instance When a reserved instance is no longer required Stop the instance, so it wont billed after expiration Sell the instance to the Reserved Instance Marketplace EC2 Classic instance can be launched outside of the VPC EC2 Enhanced Networking allows Higher Packet Per Second (PPS) Consistently lower inter instance latencies If Cluster Placement Group through insufficient Capacity Error , restart the instance, there's no such capacity limitation Warm Attach means attaching an ENI when it is Stopped Placement Group Cluster Tightly coupled used for HPC ( High Performance Computing ) Low latency network performance Partition Logical partition group Does not share underlying hardware Used for distributed and replicated workloads Hadoop Cassandra kafka Spread Distinct underlying hardware Logging aws ec3 describe-instances for get logs including the recently terminated instance Limitation There are limitation for launching vCPU in each region Default is 20 Submitting a request to increase the limit can work There is no cost for EIP , if there is only one EIP and it is associated with running EC2 Use common key-pairs in different region Create a public key form a private key Manually import the public keys to selected regions","title":"EC2 Fundamentals"},{"location":"Notes/ELB%20And%20ASG/01%20ASG%20Overview/","text":"ASG Overview Terms Desired capacity or actual size means, number of instance while first time running Minimum size means number of minimum instance while application load is minimum Maximum size means number of maximum instance while application load is maximum Scale Out mean adding instance Scale In means removing instance When new AMI is required, Keep the existing Target Group Create new Launch Configuration ASG terminate the instance in the following manner Select instance with oldest launch config Select instance with closest billing hour The more instance in a AZ got terminated first When we need to manually add or remove instances, update the launch configuration with desired capacity A launch configuration can not be updated after being created Cool Down Period Ensure the ASG does not add or terminate instance without previous activity being completed Default cool-down period is 300 sec Cool Down Period ensures it will not terminate or launch before the last scaling takes place To reduce the changing of EC2 Instance Use high cool-down period Use high threshold value in the Cloudwatch Alarm Metric Applicable for Simple Scaling Policy Type of Scaling Target Tracking Scaling Scaling is done based on specific metric value Example be, want all the CPU average speed 40%. If goes more usage, add instance Eligible metrics are CPU Network Requests Step Scaling oe Simple Scaling Scaling is done based on set of scaling adjustments Cloudwatch alarm is involved Example If CPU Usage is over 30% add 2 instance If CPU usage is over 70% add 5 instance Schedule Scaling Based on usage pattern Example be, use 10 instance on each friday 10AM - 5PM, other time 1 instance When a new AMI is required to launch to ASG , need to update the ASG Launch Config Termination Policy Find the AZ with most instances and terminate from there Find the instance with oldest config Find instance with closest billing period Lifecycle Policy While launch instance, there is pending state In pending state, we can go to Pending:wait Pending:proceed Then it goes to in service While terminate instance, there is terminating state In terminating state we can go to Terminating:wait terminating:proceed Then it goes to terminate We can use following state to do additional task Pending:wait Pending:proceed Terminating:wait Terminating:proceed Launch Configuration vs Launch Template Both Use AMI Instance Type Key pair SG Launch Configuration Must be created every time Launch Template Can have multiple version Use both on-demand and spot instances Can use t2 burst feature Recommended by AWS","title":"01 ASG Overview"},{"location":"Notes/ELB%20And%20ASG/01%20ASG%20Overview/#asg-overview","text":"Terms Desired capacity or actual size means, number of instance while first time running Minimum size means number of minimum instance while application load is minimum Maximum size means number of maximum instance while application load is maximum Scale Out mean adding instance Scale In means removing instance When new AMI is required, Keep the existing Target Group Create new Launch Configuration ASG terminate the instance in the following manner Select instance with oldest launch config Select instance with closest billing hour The more instance in a AZ got terminated first When we need to manually add or remove instances, update the launch configuration with desired capacity A launch configuration can not be updated after being created","title":"ASG Overview"},{"location":"Notes/ELB%20And%20ASG/01%20ASG%20Overview/#cool-down-period","text":"Ensure the ASG does not add or terminate instance without previous activity being completed Default cool-down period is 300 sec Cool Down Period ensures it will not terminate or launch before the last scaling takes place To reduce the changing of EC2 Instance Use high cool-down period Use high threshold value in the Cloudwatch Alarm Metric Applicable for Simple Scaling Policy","title":"Cool Down Period"},{"location":"Notes/ELB%20And%20ASG/01%20ASG%20Overview/#type-of-scaling","text":"Target Tracking Scaling Scaling is done based on specific metric value Example be, want all the CPU average speed 40%. If goes more usage, add instance Eligible metrics are CPU Network Requests Step Scaling oe Simple Scaling Scaling is done based on set of scaling adjustments Cloudwatch alarm is involved Example If CPU Usage is over 30% add 2 instance If CPU usage is over 70% add 5 instance Schedule Scaling Based on usage pattern Example be, use 10 instance on each friday 10AM - 5PM, other time 1 instance When a new AMI is required to launch to ASG , need to update the ASG Launch Config","title":"Type of Scaling"},{"location":"Notes/ELB%20And%20ASG/01%20ASG%20Overview/#termination-policy","text":"Find the AZ with most instances and terminate from there Find the instance with oldest config Find instance with closest billing period","title":"Termination Policy"},{"location":"Notes/ELB%20And%20ASG/01%20ASG%20Overview/#lifecycle-policy","text":"While launch instance, there is pending state In pending state, we can go to Pending:wait Pending:proceed Then it goes to in service While terminate instance, there is terminating state In terminating state we can go to Terminating:wait terminating:proceed Then it goes to terminate We can use following state to do additional task Pending:wait Pending:proceed Terminating:wait Terminating:proceed","title":"Lifecycle Policy"},{"location":"Notes/ELB%20And%20ASG/01%20ASG%20Overview/#launch-configuration-vs-launch-template","text":"Both Use AMI Instance Type Key pair SG Launch Configuration Must be created every time Launch Template Can have multiple version Use both on-demand and spot instances Can use t2 burst feature Recommended by AWS","title":"Launch Configuration vs Launch Template"},{"location":"Notes/ELB%20And%20ASG/02%20ELB%20Overview/","text":"ELB Overview Use to forward traffic to multiple servers Expose single point of access DNS to the application Do regular health checks Provide SSL for the site Can be used stickiness with cookies User with same cookie will go to the same server/instance Allow cross zone availability 3 types of load balancers Classic Load Balancer Handle HTTP , HTTPS and TCP traffic Application Load Balancer Handle HTTP , HTTPS and Websocket Great fit for micro-services and container based applications Has port mapping feature to redirect a dynamic port in ECS The application can get the IP from header x-forwarded-for The application can get the protocol from header x-forwarded-proto The application can get the port from header x-forwarded-port Network Load Balancer Handle TCP , TLS aka Secure TCP and UDP Supports one EIP for each AZ, that is helpful for whitelisting the IP Use for Extreme performance TCP or UDP protocol ALB and CLB expose DNS, on the other hand NLB expose static IP Load balancer can be Public and Private For huge scale out, need to use warm-up . Need to contact AWS for this purpose Troubleshooting 4xx for client induced error 5xx for application induced error 503 for at capacity or no registered target Monitoring ALB access logs can provide details of API Calls Cloudwatch for aggregate statistics ELB does the health check by HTTP HTTPS A security feature is Perfect forward secrecy offer SSl/TLS to Cloudfront and ELB In ALB using path condition we can forward request to different Target Groups based on api path, like abc.com/a abc.com/b using host condition we can forward request to different Target Groups based on host name in the header, like abc.site.com def.site.com ALB can have 3 types of target, Instances IP Lambda Stickiness Ensure the user goes to the same instance Supported by Application Load Balancer Need to update the target group Classic Load Balancer Can set the time of stickiness Cross Zone Load Balancing Load balancer can distribute traffic evenly among all the AZ For NLB there is charge for Inter AZ load balancing In CLB and NLB this Cross Zone Load Balancing is turned of by default SSL and TLS Certificate SNI stands for server name identification SNI can be use for multiple endpoint with multiple certificate Connection Draining It is the time of In Flight Request while the instance is de registering or unhealthy For CLB it is called Connection Draining For ALB and NLB it is called De registration Delay Happen in Target Group Connection draining time can be set from 0 (disabled) to 3600 sec Lambda Function Integration With ALB and target groups, We can send multi value headers Enable health check multi value query enables us sending query as name=['foo', 'bar'] instead of ?name='foo'&name='bar'","title":"02 ELB Overview"},{"location":"Notes/ELB%20And%20ASG/02%20ELB%20Overview/#elb-overview","text":"Use to forward traffic to multiple servers Expose single point of access DNS to the application Do regular health checks Provide SSL for the site Can be used stickiness with cookies User with same cookie will go to the same server/instance Allow cross zone availability 3 types of load balancers Classic Load Balancer Handle HTTP , HTTPS and TCP traffic Application Load Balancer Handle HTTP , HTTPS and Websocket Great fit for micro-services and container based applications Has port mapping feature to redirect a dynamic port in ECS The application can get the IP from header x-forwarded-for The application can get the protocol from header x-forwarded-proto The application can get the port from header x-forwarded-port Network Load Balancer Handle TCP , TLS aka Secure TCP and UDP Supports one EIP for each AZ, that is helpful for whitelisting the IP Use for Extreme performance TCP or UDP protocol ALB and CLB expose DNS, on the other hand NLB expose static IP Load balancer can be Public and Private For huge scale out, need to use warm-up . Need to contact AWS for this purpose Troubleshooting 4xx for client induced error 5xx for application induced error 503 for at capacity or no registered target Monitoring ALB access logs can provide details of API Calls Cloudwatch for aggregate statistics ELB does the health check by HTTP HTTPS A security feature is Perfect forward secrecy offer SSl/TLS to Cloudfront and ELB In ALB using path condition we can forward request to different Target Groups based on api path, like abc.com/a abc.com/b using host condition we can forward request to different Target Groups based on host name in the header, like abc.site.com def.site.com ALB can have 3 types of target, Instances IP Lambda","title":"ELB Overview"},{"location":"Notes/ELB%20And%20ASG/02%20ELB%20Overview/#stickiness","text":"Ensure the user goes to the same instance Supported by Application Load Balancer Need to update the target group Classic Load Balancer Can set the time of stickiness","title":"Stickiness"},{"location":"Notes/ELB%20And%20ASG/02%20ELB%20Overview/#cross-zone-load-balancing","text":"Load balancer can distribute traffic evenly among all the AZ For NLB there is charge for Inter AZ load balancing In CLB and NLB this Cross Zone Load Balancing is turned of by default","title":"Cross Zone Load Balancing"},{"location":"Notes/ELB%20And%20ASG/02%20ELB%20Overview/#ssl-and-tls-certificate","text":"SNI stands for server name identification SNI can be use for multiple endpoint with multiple certificate","title":"SSL and TLS Certificate"},{"location":"Notes/ELB%20And%20ASG/02%20ELB%20Overview/#connection-draining","text":"It is the time of In Flight Request while the instance is de registering or unhealthy For CLB it is called Connection Draining For ALB and NLB it is called De registration Delay Happen in Target Group Connection draining time can be set from 0 (disabled) to 3600 sec","title":"Connection Draining"},{"location":"Notes/ELB%20And%20ASG/02%20ELB%20Overview/#lambda-function-integration","text":"With ALB and target groups, We can send multi value headers Enable health check multi value query enables us sending query as name=['foo', 'bar'] instead of ?name='foo'&name='bar'","title":"Lambda Function Integration"},{"location":"Notes/ELB%20And%20ASG/03%20Scalability%20%26%20High%20Availability/","text":"Scalability & High Availability Scalability Two types of scalability Vertical Scalability Upgrade the Underlying Hardware There is hardware limitation for Vertical Scalability Horizontal Scalability Increase number of instances/system High Availability Run the server at least 2 AZ or data center Goal is survive the a data center loss or AZ down","title":"03 Scalability & High Availability"},{"location":"Notes/ELB%20And%20ASG/03%20Scalability%20%26%20High%20Availability/#scalability-high-availability","text":"","title":"Scalability &amp; High Availability"},{"location":"Notes/ELB%20And%20ASG/03%20Scalability%20%26%20High%20Availability/#scalability","text":"Two types of scalability Vertical Scalability Upgrade the Underlying Hardware There is hardware limitation for Vertical Scalability Horizontal Scalability Increase number of instances/system","title":"Scalability"},{"location":"Notes/ELB%20And%20ASG/03%20Scalability%20%26%20High%20Availability/#high-availability","text":"Run the server at least 2 AZ or data center Goal is survive the a data center loss or AZ down","title":"High Availability"},{"location":"Notes/IAM/01%20IAM%20STS/","text":"STS Security Token Service Limited validity time (15 mins to 1 hour) AssumeRole Use to create temporary role within the account and share resource 4 types of AssumeRole Plain AssumeRole AssumeRoleWithSAMl , this return the credentials to the users who is logged in with SAML Federation AssumeRoleWithWebIdentity , this return the credentials to the users who is logged in with LDB , like google, facebook etc. Recommended to use Cognito instead GetSessionToken , for MFA . This returns AccessKeyId : used for programmatic access SecretAccessKey : used for programmatic access SessionToken Expiration GetCallerIdentity , for get details of IAM user or role used in API call DecodeAuthorizationMessage , decode error message when AWS API is denied Using AssumeRole Define IAM role within your account or cross-account Define principal to access resource in the IAM Role Use STS to retrieve credentials and impersonate the IAM role using AssumeRole API This credentials valid 15 mins to 1 hour Sharing Resource To share a resource of account A to account B Account A creates an IAM role and attach it with a permission policy Account A attaches a trust policy that identifies account B as principle who can assume role Account B create a permission to assume the role of account B","title":"01 IAM STS"},{"location":"Notes/IAM/01%20IAM%20STS/#sts","text":"Security Token Service Limited validity time (15 mins to 1 hour) AssumeRole Use to create temporary role within the account and share resource 4 types of AssumeRole Plain AssumeRole AssumeRoleWithSAMl , this return the credentials to the users who is logged in with SAML Federation AssumeRoleWithWebIdentity , this return the credentials to the users who is logged in with LDB , like google, facebook etc. Recommended to use Cognito instead GetSessionToken , for MFA . This returns AccessKeyId : used for programmatic access SecretAccessKey : used for programmatic access SessionToken Expiration GetCallerIdentity , for get details of IAM user or role used in API call DecodeAuthorizationMessage , decode error message when AWS API is denied","title":"STS"},{"location":"Notes/IAM/01%20IAM%20STS/#using-assumerole","text":"Define IAM role within your account or cross-account Define principal to access resource in the IAM Role Use STS to retrieve credentials and impersonate the IAM role using AssumeRole API This credentials valid 15 mins to 1 hour","title":"Using AssumeRole"},{"location":"Notes/IAM/01%20IAM%20STS/#sharing-resource","text":"To share a resource of account A to account B Account A creates an IAM role and attach it with a permission policy Account A attaches a trust policy that identifies account B as principle who can assume role Account B create a permission to assume the role of account B","title":"Sharing Resource"},{"location":"Notes/IAM/02%20IAM%20Identity%20Fedaration/","text":"Identity Federation User management outside of AWS No need to create AWS Users Allows users, who are outside of AWS can access AWS Resource 6 Types of Identity Federation SAML 2.0 Custom Identity Broker Web Identity Federation without Cognito Web Identity Federation with Cognito Single Sign On Non SAML, like compatible like Microsoft AD Custom Identity Broker Use when the Identity Broker is not compatible with SAML Web Identity Federation allow to connect Single Sign In using OpenID-connect","title":"02 IAM Identity Fedaration"},{"location":"Notes/IAM/02%20IAM%20Identity%20Fedaration/#identity-federation","text":"User management outside of AWS No need to create AWS Users Allows users, who are outside of AWS can access AWS Resource 6 Types of Identity Federation SAML 2.0 Custom Identity Broker Web Identity Federation without Cognito Web Identity Federation with Cognito Single Sign On Non SAML, like compatible like Microsoft AD Custom Identity Broker Use when the Identity Broker is not compatible with SAML Web Identity Federation allow to connect Single Sign In using OpenID-connect","title":"Identity Federation"},{"location":"Notes/IAM/03%20IAM%20Directory%20Service/","text":"Directory Service 3 types of Directory service AWS Managed Microsoft AD Hybrid solution Users can be from AWS Directory and On premise Directory Both AWS Directory and On premise Directory are connected through a TRUST connection This TRUST connection can be set up using VPN or Direct Connect AD Connector Directory Gateway (Proxy) Users managed by only On premise AD Redirect to the On premise AD Allows MFA Simple AD AD compatible service by AWS Can not join with On premise AD Microsoft Active Directory Available in any windows server with AD Domain Service Combination of Objects, like User Accounts Computers Printers File Shares Security Groups Objects are organized in Trees A group of Trees are Forest AD has feature Centralized Security Management Create Account Assigning Permission","title":"03 IAM Directory Service"},{"location":"Notes/IAM/03%20IAM%20Directory%20Service/#directory-service","text":"3 types of Directory service AWS Managed Microsoft AD Hybrid solution Users can be from AWS Directory and On premise Directory Both AWS Directory and On premise Directory are connected through a TRUST connection This TRUST connection can be set up using VPN or Direct Connect AD Connector Directory Gateway (Proxy) Users managed by only On premise AD Redirect to the On premise AD Allows MFA Simple AD AD compatible service by AWS Can not join with On premise AD","title":"Directory Service"},{"location":"Notes/IAM/03%20IAM%20Directory%20Service/#microsoft-active-directory","text":"Available in any windows server with AD Domain Service Combination of Objects, like User Accounts Computers Printers File Shares Security Groups Objects are organized in Trees A group of Trees are Forest AD has feature Centralized Security Management Create Account Assigning Permission","title":"Microsoft Active Directory"},{"location":"Notes/IAM/04%20IAM%20Organizations/","text":"Organizations Global Service Allow to manage multiple AWS Account Main Account is Master Account Master Account can not be changed Other account is Member Account Each member can attach only one Organization Consolidated Billing For all Organization Account Pricing benefits are calculated by Consolidated Billing API is available to automate creating the AWS Account Resource sharing can be done by individual account even though the resource sharing is not enabled by SCP SCP Service Control Policies Use for White List and Black List the IAM Action Applied to Organization Unit or Account Level Does not apply to Master Account SCP By default deny everything Need explicit allow to for any action Effect of Service Linked Roles Service Linked Roles enable AWS Service to AWS Organization SCP can not affect Service Linked Roles Transfer An Account Between AWS Organization Member Account Transfer Leave the current organization Get invitation from the other organization Accept the invitation Master Account Transfer Remove all the member account Delete old organization","title":"04 IAM Organizations"},{"location":"Notes/IAM/04%20IAM%20Organizations/#organizations","text":"Global Service Allow to manage multiple AWS Account Main Account is Master Account Master Account can not be changed Other account is Member Account Each member can attach only one Organization Consolidated Billing For all Organization Account Pricing benefits are calculated by Consolidated Billing API is available to automate creating the AWS Account Resource sharing can be done by individual account even though the resource sharing is not enabled by SCP","title":"Organizations"},{"location":"Notes/IAM/04%20IAM%20Organizations/#scp","text":"Service Control Policies Use for White List and Black List the IAM Action Applied to Organization Unit or Account Level Does not apply to Master Account SCP By default deny everything Need explicit allow to for any action Effect of Service Linked Roles Service Linked Roles enable AWS Service to AWS Organization SCP can not affect Service Linked Roles","title":"SCP"},{"location":"Notes/IAM/04%20IAM%20Organizations/#transfer-an-account-between-aws-organization","text":"Member Account Transfer Leave the current organization Get invitation from the other organization Accept the invitation Master Account Transfer Remove all the member account Delete old organization","title":"Transfer An Account Between AWS Organization"},{"location":"Notes/IAM/05%20IAM%20Conditions/","text":"IAM Conditions Allow IP to API call to the AWS from certain IP Any request to the AWS should come from 192.0.2.0/24 Allow taking action to certain resources only from certain region, like EC2 Instance can be start or stop if the request is from eu-east-1 Restriction can be based on Tags , like We can start a instance if the instance has certain tags We can stop a instance if the instance has certain tags For certain actions we can force using MFA , like Stop EC2 Instance Terminate EC2 Instance","title":"05 IAM Conditions"},{"location":"Notes/IAM/05%20IAM%20Conditions/#iam-conditions","text":"Allow IP to API call to the AWS from certain IP Any request to the AWS should come from 192.0.2.0/24 Allow taking action to certain resources only from certain region, like EC2 Instance can be start or stop if the request is from eu-east-1 Restriction can be based on Tags , like We can start a instance if the instance has certain tags We can stop a instance if the instance has certain tags For certain actions we can force using MFA , like Stop EC2 Instance Terminate EC2 Instance","title":"IAM Conditions"},{"location":"Notes/IAM/06%20IAM%20for%20S3/","text":"IAM for S3 When arn is arn:aws:s3:::test , then the rules applied to Bucket Level when arn is arn:aws:s3:::test/* the the rules applied to Object Level","title":"06 IAM for S3"},{"location":"Notes/IAM/06%20IAM%20for%20S3/#iam-for-s3","text":"When arn is arn:aws:s3:::test , then the rules applied to Bucket Level when arn is arn:aws:s3:::test/* the the rules applied to Object Level","title":"IAM for S3"},{"location":"Notes/IAM/07%20IAM%20vs%20Resource%20Policies/","text":"IAM vs Resource Policies Using AssumeRole , original permissions have to give up Lets say, I have permission to access DynamoDB` Using AssumeRole I got access of S3 When I am using this AssumeRole , I can only access S3 , but can not access DynamoDB But using Resource Policies we can use both S3 and DynamoDB Resource Based Policies are provided by S3 SNS SQS","title":"07 IAM vs Resource Policies"},{"location":"Notes/IAM/07%20IAM%20vs%20Resource%20Policies/#iam-vs-resource-policies","text":"Using AssumeRole , original permissions have to give up Lets say, I have permission to access DynamoDB` Using AssumeRole I got access of S3 When I am using this AssumeRole , I can only access S3 , but can not access DynamoDB But using Resource Policies we can use both S3 and DynamoDB Resource Based Policies are provided by S3 SNS SQS","title":"IAM vs Resource Policies"},{"location":"Notes/IAM/08%20IAM%20Policy%20Evaluation/","text":"IAM Policy Evaluation IAM Permission Boundaries Supported for users and roles (not groups) Even a user has Administrator Access , these access can be restricted using the IAM Permission Boundaries This can be used as a combination of Organization SCP Evaluation Logic Couple of roles and policies we can define Deny Evaluation Organization SCP Resource Based Policy IAM Permission Boundaries Session Policy Identity Based Policy If there is any Explicit Deny the other Allow will be discarded IAM Policy Evaluation Order Command Line Options : Override any other config. Used like --region , --output , --profile etc Environment variable CLI Credentials File : Created by aws configure and store in ~/.aws/credentials CLI Config File : TODO: find diff between CLI Credentials File and CLI Config File Container Credentials : Temporary credentials in the ECS Task container Instance Profile Credentials : IAM role attached to the instance","title":"08 IAM Policy Evaluation"},{"location":"Notes/IAM/08%20IAM%20Policy%20Evaluation/#iam-policy-evaluation","text":"","title":"IAM Policy Evaluation"},{"location":"Notes/IAM/08%20IAM%20Policy%20Evaluation/#iam-permission-boundaries","text":"Supported for users and roles (not groups) Even a user has Administrator Access , these access can be restricted using the IAM Permission Boundaries This can be used as a combination of Organization SCP","title":"IAM Permission Boundaries"},{"location":"Notes/IAM/08%20IAM%20Policy%20Evaluation/#evaluation-logic","text":"Couple of roles and policies we can define Deny Evaluation Organization SCP Resource Based Policy IAM Permission Boundaries Session Policy Identity Based Policy If there is any Explicit Deny the other Allow will be discarded","title":"Evaluation Logic"},{"location":"Notes/IAM/08%20IAM%20Policy%20Evaluation/#iam-policy-evaluation-order","text":"Command Line Options : Override any other config. Used like --region , --output , --profile etc Environment variable CLI Credentials File : Created by aws configure and store in ~/.aws/credentials CLI Config File : TODO: find diff between CLI Credentials File and CLI Config File Container Credentials : Temporary credentials in the ECS Task container Instance Profile Credentials : IAM role attached to the instance","title":"IAM Policy Evaluation Order"},{"location":"Notes/IAM/09%20IAM%20Resource%20Access%20Manager/","text":"Resource Access Manager Known as RAM Share AWS Resource with other AWS Account Avoid resource duplication Example of RAM on VPC Subnet Allow to have all the resource launched in the same subnets For VPC Subnet , other account have to be same Organization Can not share SG or Default VPC Participants can manage there own resource there Participant can not view/modify other participants resource Also we can use RAM in AWS Transit Gateway Route53 Resolver License Manager Configurations","title":"09 IAM Resource Access Manager"},{"location":"Notes/IAM/09%20IAM%20Resource%20Access%20Manager/#resource-access-manager","text":"Known as RAM Share AWS Resource with other AWS Account Avoid resource duplication Example of RAM on VPC Subnet Allow to have all the resource launched in the same subnets For VPC Subnet , other account have to be same Organization Can not share SG or Default VPC Participants can manage there own resource there Participant can not view/modify other participants resource Also we can use RAM in AWS Transit Gateway Route53 Resolver License Manager Configurations","title":"Resource Access Manager"},{"location":"Notes/IAM/10%20IAM%20Single%20Sign%20On/","text":"Single Sign On Single sion on for Multiple accounts 3rd party Applications Integrated with AWS Organization Supports SAML 2.0 Markup On Premise AD Feature Centralized Permission Centralized Auditing Cloudtrail","title":"10 IAM Single Sign On"},{"location":"Notes/IAM/10%20IAM%20Single%20Sign%20On/#single-sign-on","text":"Single sion on for Multiple accounts 3rd party Applications Integrated with AWS Organization Supports SAML 2.0 Markup On Premise AD Feature Centralized Permission Centralized Auditing Cloudtrail","title":"Single Sign On"},{"location":"Notes/IAM/12%20IAM%20Access%20Analyzer/","text":"IAM Access Analyzer.md Allow monitoring the resources, that are shared with external entity Can determine if there is a security risk","title":"12 IAM Access Analyzer"},{"location":"Notes/IAM/12%20IAM%20Access%20Analyzer/#iam-access-analyzermd","text":"Allow monitoring the resources, that are shared with external entity Can determine if there is a security risk","title":"IAM Access Analyzer.md"},{"location":"Notes/IAM/11%20SSL/TLS%20Certificate/","text":"SSL/TLS Certificate Can use AWS Certificate Manager (ACM) IAM Certificate Store For 3rd party certificate, we can use one of the following Import it to ACM Upload it to IAM","title":"TLS Certificate"},{"location":"Notes/IAM/11%20SSL/TLS%20Certificate/#ssltls-certificate","text":"Can use AWS Certificate Manager (ACM) IAM Certificate Store For 3rd party certificate, we can use one of the following Import it to ACM Upload it to IAM","title":"SSL/TLS Certificate"},{"location":"Notes/Other%20Services/01%20CI-CD_deprecated/","text":"CI/CD Flow Code -> Build -> Test -> Deploy -> Provision AWS Codepipeline AWS Codecommit Like Github Responsible for Code portion AWS Codebuild Like Jenkins Responsible for Build and Test portion AWS Code Deploy Responsible for Deploy portion Ues AWS Beanstalk or AWS Cloudformation to provision the code Can be use for deploying code to EC2 Instance On premise sever Lambda Function Use for Rapid release of new feature Updating Lambda Function Avoid downtime during Application deployment Type of code deploy Linear (This deployment send traffic incrementally) All at once (All traffic shifts to new deployment) Canary (Can be define, how many traffic will flow new and updated deployments) While deploying a code base Code commit can be a source stage Beanstalk can be a deploy stage Blue/Green Deployment Isolation between blue and green Roll incoming traffic during deployments Minimum downtime","title":"01 CI CD deprecated"},{"location":"Notes/Other%20Services/01%20CI-CD_deprecated/#cicd","text":"Flow Code -> Build -> Test -> Deploy -> Provision AWS Codepipeline AWS Codecommit Like Github Responsible for Code portion AWS Codebuild Like Jenkins Responsible for Build and Test portion AWS Code Deploy Responsible for Deploy portion Ues AWS Beanstalk or AWS Cloudformation to provision the code Can be use for deploying code to EC2 Instance On premise sever Lambda Function Use for Rapid release of new feature Updating Lambda Function Avoid downtime during Application deployment Type of code deploy Linear (This deployment send traffic incrementally) All at once (All traffic shifts to new deployment) Canary (Can be define, how many traffic will flow new and updated deployments) While deploying a code base Code commit can be a source stage Beanstalk can be a deploy stage Blue/Green Deployment Isolation between blue and green Roll incoming traffic during deployments Minimum downtime","title":"CI/CD"},{"location":"Notes/Other%20Services/02%20Cloudformation/","text":"Cloudformation Infrastructure as Code Easy to CRUD the infrastructure as Code Savings Strategy Automate the creation and deletion of the infrastructure at for certain times StackSet : Extends the functionalities of the stack, administrator create the stack template and other accounts can extends the stacks functionality Allow Create , Update , Delete stacks across multiple accounts and regions Change Sets : Allow to show the changes preview. Stack Instances : Reference to a original stack in another account Artifacts : Used to conjunction with the code pipeline Drift Detection : Allows to check if created resourses changed over time or not. Helps to resolve out-of-bound fixes To share informations between stacks From one template, in the output section, put data under export In another template, in the input, use Fn::ImportValue to get that data Output values name must be unique within the region Commands cfn-init : Used to retrieve metadata, install packages (like nginx in ec2), run services cfn-signal : Send signal for create or wait, use to synchronize the resources cfn-get-metadata : Use to retrieve metadata for a service or resources cfn-hup : Upon checking the metadata, execute custom hooks when changes are detected Properties SecureString , in AWS Parameter Store : for license keys or external secret values. Cost effective while compare with AWS Secret Manager NoEcho : prevent displaying the value in plain text Allowed properties in Cloudformation AWSTemplateFormatVersion Description Metadata Parameters Mappings Conditions Transform Resources Outputs FindInMap method takes 3 parameters, MapName , TopLevelKey , SecondLevelKey AWS::NoValue : from return property, remove specified resource property AWS::Region : region name, where the stack was created AWS::StackName : name of the stack, given during stack initialization AWS::AccountID : creator id of the stack Supported parameters type String \u2013 A literal string Number \u2013 An integer or float List \u2013 An array of integers or floats CommaDelimitedList \u2013 An array of literal strings that are separated by commas AWS::EC2::KeyPair::KeyName \u2013 An Amazon EC2 key pair name AWS::EC2::SecurityGroup::Id \u2013 A security group ID AWS::EC2::Subnet::Id \u2013 A subnet ID AWS::EC2::VPC::Id \u2013 A VPC ID List \u2013 An array of VPC IDs List \u2013 An array of security group IDs List \u2013 An array of subnet IDs","title":"02 Cloudformation"},{"location":"Notes/Other%20Services/02%20Cloudformation/#cloudformation","text":"Infrastructure as Code Easy to CRUD the infrastructure as Code Savings Strategy Automate the creation and deletion of the infrastructure at for certain times StackSet : Extends the functionalities of the stack, administrator create the stack template and other accounts can extends the stacks functionality Allow Create , Update , Delete stacks across multiple accounts and regions Change Sets : Allow to show the changes preview. Stack Instances : Reference to a original stack in another account Artifacts : Used to conjunction with the code pipeline Drift Detection : Allows to check if created resourses changed over time or not. Helps to resolve out-of-bound fixes To share informations between stacks From one template, in the output section, put data under export In another template, in the input, use Fn::ImportValue to get that data Output values name must be unique within the region","title":"Cloudformation"},{"location":"Notes/Other%20Services/02%20Cloudformation/#commands","text":"cfn-init : Used to retrieve metadata, install packages (like nginx in ec2), run services cfn-signal : Send signal for create or wait, use to synchronize the resources cfn-get-metadata : Use to retrieve metadata for a service or resources cfn-hup : Upon checking the metadata, execute custom hooks when changes are detected","title":"Commands"},{"location":"Notes/Other%20Services/02%20Cloudformation/#properties","text":"SecureString , in AWS Parameter Store : for license keys or external secret values. Cost effective while compare with AWS Secret Manager NoEcho : prevent displaying the value in plain text Allowed properties in Cloudformation AWSTemplateFormatVersion Description Metadata Parameters Mappings Conditions Transform Resources Outputs FindInMap method takes 3 parameters, MapName , TopLevelKey , SecondLevelKey AWS::NoValue : from return property, remove specified resource property AWS::Region : region name, where the stack was created AWS::StackName : name of the stack, given during stack initialization AWS::AccountID : creator id of the stack Supported parameters type String \u2013 A literal string Number \u2013 An integer or float List \u2013 An array of integers or floats CommaDelimitedList \u2013 An array of literal strings that are separated by commas AWS::EC2::KeyPair::KeyName \u2013 An Amazon EC2 key pair name AWS::EC2::SecurityGroup::Id \u2013 A security group ID AWS::EC2::Subnet::Id \u2013 A subnet ID AWS::EC2::VPC::Id \u2013 A VPC ID List \u2013 An array of VPC IDs List \u2013 An array of security group IDs List \u2013 An array of subnet IDs","title":"Properties"},{"location":"Notes/Other%20Services/03%20ECS/","text":"ECS Elastic Container Service Run docker containers in EC2 machines Components ECS Core provision EC2 instance to run docker container Fargate run ECS task to AWS Provide Compute , more serverless than ECS Core EKS is Elastic Kubernetes Service by AWS ECR is Elastic Container Registry by AWS IAM Security and Roles are in ECS Task level ECS can enable dynamic port mapping with ALB NLB ECS setup For regular EC2 Instance install the ECS Agent and edit config file For ECS ready Linux AMI , no need to install ECS Agent , only edit the config file Editing the config file Location /etc/ecs/ecs.config There are 35 Configuration to edit ECS CLUSTER ECS_ENGINE_AUTH_DATA ECS_AVAILABLE_LOGGING_DRIVER ECS_ENABLE_TASK_IAM_ROLE If the associated ec2 instances are in stopped state and we terminate ecs, it will not be de-registered automatically When ecs client is stopped, the instance remain active with agent connection status false Cluster name is set up in /etc/ecs/ecs.config file ECS Components Task Definition : Allow port mapping. Port Mapping allows the container to send and receive traffic through the host machine Service Schedular : Allow to run tasks manually Container Instance : On which the container runs on, typically the ec2 instance or ecs ready ec2 instance Container Agent : Allows the containers to connect with the cluster Task Placement Strategy Determine how the tasks will be placed between instances. ECS supports 3 types of Task Placement Strategies binpack : Placed task by using least amount of CPU or memory. This minimize the number of instances are being used random : Place tasks randomly. Make sure tasks are scheduled in instances with enough resources. spread : Placed tasks based on specified value (key-value pairs, instanceId or host). For example, if the field is instanceId the task will be distributed evenly in different instances. Another example, if the field is az , the tasks will be evenly distributed among availability zones. When a container requires to listen to specic port, Specify the port for the container Put 0 as the host port In this case, the ECS will automatically assign the port. Cluster Query Language Allow more fine grained way to place tasks Using environment variables Use advanced task definition Define environment parameters under the task definition environment variables","title":"03 ECS"},{"location":"Notes/Other%20Services/03%20ECS/#ecs","text":"Elastic Container Service Run docker containers in EC2 machines Components ECS Core provision EC2 instance to run docker container Fargate run ECS task to AWS Provide Compute , more serverless than ECS Core EKS is Elastic Kubernetes Service by AWS ECR is Elastic Container Registry by AWS IAM Security and Roles are in ECS Task level ECS can enable dynamic port mapping with ALB NLB ECS setup For regular EC2 Instance install the ECS Agent and edit config file For ECS ready Linux AMI , no need to install ECS Agent , only edit the config file Editing the config file Location /etc/ecs/ecs.config There are 35 Configuration to edit ECS CLUSTER ECS_ENGINE_AUTH_DATA ECS_AVAILABLE_LOGGING_DRIVER ECS_ENABLE_TASK_IAM_ROLE If the associated ec2 instances are in stopped state and we terminate ecs, it will not be de-registered automatically When ecs client is stopped, the instance remain active with agent connection status false Cluster name is set up in /etc/ecs/ecs.config file","title":"ECS"},{"location":"Notes/Other%20Services/03%20ECS/#ecs-components","text":"Task Definition : Allow port mapping. Port Mapping allows the container to send and receive traffic through the host machine Service Schedular : Allow to run tasks manually Container Instance : On which the container runs on, typically the ec2 instance or ecs ready ec2 instance Container Agent : Allows the containers to connect with the cluster","title":"ECS Components"},{"location":"Notes/Other%20Services/03%20ECS/#task-placement-strategy","text":"Determine how the tasks will be placed between instances. ECS supports 3 types of Task Placement Strategies binpack : Placed task by using least amount of CPU or memory. This minimize the number of instances are being used random : Place tasks randomly. Make sure tasks are scheduled in instances with enough resources. spread : Placed tasks based on specified value (key-value pairs, instanceId or host). For example, if the field is instanceId the task will be distributed evenly in different instances. Another example, if the field is az , the tasks will be evenly distributed among availability zones. When a container requires to listen to specic port, Specify the port for the container Put 0 as the host port In this case, the ECS will automatically assign the port.","title":"Task Placement Strategy"},{"location":"Notes/Other%20Services/03%20ECS/#cluster-query-language","text":"Allow more fine grained way to place tasks","title":"Cluster Query Language"},{"location":"Notes/Other%20Services/03%20ECS/#using-environment-variables","text":"Use advanced task definition Define environment parameters under the task definition environment variables","title":"Using environment variables"},{"location":"Notes/Other%20Services/04%20EKS/","text":"EKS EKS stands for Elastic Kuberneties Service Managed kubernetes Cluster by AWS Similar to ECS but it's open source Easy when migrate kubernetes from other provider","title":"04 EKS"},{"location":"Notes/Other%20Services/04%20EKS/#eks","text":"EKS stands for Elastic Kuberneties Service Managed kubernetes Cluster by AWS Similar to ECS but it's open source Easy when migrate kubernetes from other provider","title":"EKS"},{"location":"Notes/Other%20Services/05%20Step%20Function%20vs%20SWF/","text":"SWF Simple Work Flow Run on EC2 , not Serverless Concept of Activity Step Decision Step Has built in Human Intervention Feature Used when External Signal is is required to intervene When child process return value to parent process Can be used to de couple an application Properties Tags : Allow filtering the list of executions Makers : Records history of executions Timers : Allow notify the decider after a certain amount of defined time Signals : Enable to inject information to the execution Step Function vs SWF Step Function Serverless No human intervention SWF No Serverless Build in human intervention","title":"05 Step Function vs SWF"},{"location":"Notes/Other%20Services/05%20Step%20Function%20vs%20SWF/#swf","text":"Simple Work Flow Run on EC2 , not Serverless Concept of Activity Step Decision Step Has built in Human Intervention Feature Used when External Signal is is required to intervene When child process return value to parent process Can be used to de couple an application Properties Tags : Allow filtering the list of executions Makers : Records history of executions Timers : Allow notify the decider after a certain amount of defined time Signals : Enable to inject information to the execution","title":"SWF"},{"location":"Notes/Other%20Services/05%20Step%20Function%20vs%20SWF/#step-function-vs-swf","text":"Step Function Serverless No human intervention SWF No Serverless Build in human intervention","title":"Step Function vs SWF"},{"location":"Notes/Other%20Services/06%20EMR/","text":"EMR Elastic Map Reduce Use for Big data processing Hadoop Clusters Apache Spark Collect and process EC2 log files Collect and process ALB log files Supports Apache Spark HBase Presto Flink To save cost, the EC2 instances are used to run in SPot Instances When it comes to process lot of log files, EMR could be a go to move","title":"06 EMR"},{"location":"Notes/Other%20Services/06%20EMR/#emr","text":"Elastic Map Reduce Use for Big data processing Hadoop Clusters Apache Spark Collect and process EC2 log files Collect and process ALB log files Supports Apache Spark HBase Presto Flink To save cost, the EC2 instances are used to run in SPot Instances When it comes to process lot of log files, EMR could be a go to move","title":"EMR"},{"location":"Notes/Other%20Services/07%20Glue/","text":"Glue Does ETL i.e. Extract Transform Load Move data from source to destination and process on the fly Keeps track of proceed data using Job Bookmark Using Job Bookmark we can only search the changes Prevent processing the whole data again","title":"07 Glue"},{"location":"Notes/Other%20Services/07%20Glue/#glue","text":"Does ETL i.e. Extract Transform Load Move data from source to destination and process on the fly Keeps track of proceed data using Job Bookmark Using Job Bookmark we can only search the changes Prevent processing the whole data again","title":"Glue"},{"location":"Notes/Other%20Services/08%20Opsworks/","text":"Opsworks Combination of Chef and Puppet Alternative of AWS SSM Opsworks Stacks are collection of instance, serving a common task Opsworks Stacks allows to Manage stack in different layers, like Load Balancing Database Application server Allow to manage app and server in AWS Cloud On premise","title":"08 Opsworks"},{"location":"Notes/Other%20Services/08%20Opsworks/#opsworks","text":"Combination of Chef and Puppet Alternative of AWS SSM Opsworks Stacks are collection of instance, serving a common task Opsworks Stacks allows to Manage stack in different layers, like Load Balancing Database Application server Allow to manage app and server in AWS Cloud On premise","title":"Opsworks"},{"location":"Notes/Other%20Services/09%20Elastic%20Transcoder/","text":"Elastic Transcoder Convert media files frm S3 To various size various formats So, compatible with Phone Tablet TV","title":"09 Elastic Transcoder"},{"location":"Notes/Other%20Services/09%20Elastic%20Transcoder/#elastic-transcoder","text":"Convert media files frm S3 To various size various formats So, compatible with Phone Tablet TV","title":"Elastic Transcoder"},{"location":"Notes/Other%20Services/10%20Workspace/","text":"AWS Workspace Managed, secure, cloud Desktop Alternative of VDI i.e. Virtual Desktop Infrastructure","title":"10 Workspace"},{"location":"Notes/Other%20Services/10%20Workspace/#aws-workspace","text":"Managed, secure, cloud Desktop Alternative of VDI i.e. Virtual Desktop Infrastructure","title":"AWS Workspace"},{"location":"Notes/Other%20Services/11%20AppSync/","text":"AppSync Managed GraphQL Can grab data from multiple sources Integrates with DynamoDB Aurora OpenSearch Extended functionalities by AWS Lambda Supports real time with webSocket or MQTT (Alternatives of ALB or API Gateway) For mobile app, allows local data access (offline sync)f and data synchronization Authorization can be done using API Key IAM CUP OpenID Connect","title":"11 AppSync"},{"location":"Notes/Other%20Services/11%20AppSync/#appsync","text":"Managed GraphQL Can grab data from multiple sources Integrates with DynamoDB Aurora OpenSearch Extended functionalities by AWS Lambda Supports real time with webSocket or MQTT (Alternatives of ALB or API Gateway) For mobile app, allows local data access (offline sync)f and data synchronization Authorization can be done using API Key IAM CUP OpenID Connect","title":"AppSync"},{"location":"Notes/Other%20Services/12%20AWS%20Polly/","text":"AWS Polly Use to read text Region specific SSML control generated speech SSML converts comma into period Strong tag control the speech speed Lexicons are region specific For a text, which appear multiple times Create alias using multiple lexicons","title":"12 AWS Polly"},{"location":"Notes/Other%20Services/12%20AWS%20Polly/#aws-polly","text":"Use to read text Region specific SSML control generated speech SSML converts comma into period Strong tag control the speech speed Lexicons are region specific For a text, which appear multiple times Create alias using multiple lexicons","title":"AWS Polly"},{"location":"Notes/Other%20Services/13%20Managed%20Blockchain/","text":"Managed Blockchain In managed blockchain, when a new member is added, an unique id is assigned to these members For any transaction, each member should use the following format ResourceID.MemberID.NetworkID.managedblockchain.amazonaws.com:PortNumber","title":"13 Managed Blockchain"},{"location":"Notes/Other%20Services/13%20Managed%20Blockchain/#managed-blockchain","text":"In managed blockchain, when a new member is added, an unique id is assigned to these members For any transaction, each member should use the following format ResourceID.MemberID.NetworkID.managedblockchain.amazonaws.com:PortNumber","title":"Managed Blockchain"},{"location":"Notes/Other%20Services/14%20AWS%20Quicksight/","text":"AWS Quicksight Cloud powered business intelligence tool Use for visualizations Ad hoc analysis ML insights","title":"14 AWS Quicksight"},{"location":"Notes/Other%20Services/14%20AWS%20Quicksight/#aws-quicksight","text":"Cloud powered business intelligence tool Use for visualizations Ad hoc analysis ML insights","title":"AWS Quicksight"},{"location":"Notes/Other%20Services/15%20AWS%20Cloud%20Search/","text":"AWS Cloud Search Managed Search Solution Has feature highlighting Autocomplete Geo spatial Search Scales seamlessly Use for Add rich search capabilities in website","title":"15 AWS Cloud Search"},{"location":"Notes/Other%20Services/15%20AWS%20Cloud%20Search/#aws-cloud-search","text":"Managed Search Solution Has feature highlighting Autocomplete Geo spatial Search Scales seamlessly Use for Add rich search capabilities in website","title":"AWS Cloud Search"},{"location":"Notes/Other%20Services/16%20Multiple%20Account/","text":"Multiple Account When to manage multiple AWS Account Business require administrative isolation on workloads Business require limited visibility and discoverability of workloads Business require isolation to minimize the blast radius Business require strong isolation of recovery and auditing data","title":"16 Multiple Account"},{"location":"Notes/Other%20Services/16%20Multiple%20Account/#multiple-account","text":"When to manage multiple AWS Account Business require administrative isolation on workloads Business require limited visibility and discoverability of workloads Business require isolation to minimize the blast radius Business require strong isolation of recovery and auditing data","title":"Multiple Account"},{"location":"Notes/Other%20Services/17%20Amazon%20X-Ray/","text":"Amazon X-Ray Allow developer debugging the production and distributed application and micro services For cost effective performance, Enable x-ray sampling The sampling rate should be low, so Get significant number of request statistically Optimum traces Properties Segments : Used to break down the data in sub segments. Sub Segments : Provides, timings, other aws service/resource calls in downstream forms. Inferred Segment : Allow display timings and calls to all services, even the services who does not support x-ray or tracing annotations and metadata do not trace the calls of other aws services and resources. Instead these annotations and metadata are included in the segments and sub-segments . sub-segments fields, namespace : Can be aws or remote aws for AWS SDK calls remote for other downstream calls http : Outgoing HTTP call annotations : Key-value for index search metadata : Additional data for debugging annotations can be used to filter the traced data from console. Also can trace by groups. metadata can include some additional key value data, that can be helpful during the debugging and tracing. Segment Document Add custom attributes as annotation to use as filter expression Add custom attributes as metadata to store custom data during trace Add custom attributes as segment field to use ::::TODO Sampling Rule to create a representative sampling of tracing With default sampling one request per sec and 5% additional requests per host Segment Properties Mandatory: Name id Trace_id Start_time End_time In_progress Optional: Service User Origin Parent_id http aws error, throttle, cause annotations metadata subsegments Sending data to x-ray Two ways, Use putSegmentTrace to uploda data directly to x-ray Put multiple request to x-ray-daemon and later using buffer, the x-ray-daemon will buffer these to the x-ray X Ray Daemon X-Ray sdk does not send data directly to X-Ray. Instead it sends data for multiple request in the daemon and then sends these to the X-Ray service. X-Ray Daemon listens to traffic to UDP 2000 port, Service Map Create dependency trees between services Detect latency between services Elastic Beanstalk Integration X-Ray is already installed in the elastic beanstalk. To enable the x-ray, update/create a config under the beanstalk extensions, .ebextensions/x-ray-daemon.config . This can also be done using by management console. max-age To get trace data, code must instrumented wit the x-ray sdk ECS Integration To use, Create a docker image, runs x-ray daemon Put the image in docker repository Launch in the ecs cluster ECS can integrate x-ray by x-ray daemon and side-car pattern As Daemon Pattern : x-ray daemin in ec2 instance. To use x-ray in ecs, we need to use the x-ray daemon container for each of the ecs instances. In this case, the x-ray daemon acts as the x-ray agents. As Side Car Pattern : x-ray daemin in container. In this case, each of the container in the ecs will have x-ray daemon inside it. When we need to use x-ray in the fargate, we have to use the side-car pattern. Because, we do not have control over the ec2 instances in fargate. To set up Run x-ray daemon according to the suited pattern Make port mapping to allows UDP 2000 for x-ray daemon Lambda Integration To implement the X-ray tracing, we need to implement the followings, Enable Active Tracing from the lambda function configuration. This will run the x-ray daemon for the function The attached policy should have write access to the x-ray daemon Following environment policies will require to communicate with x-ray _X_AMZN_TRACE_ID : tracing header AWS_XRAY_DAEMON_ADDRESS : IP_ADDRESS:PORT Others are, AWS_XRAY_CONTEXT_MISSING : default: LOG_ERROR AWS_XRAY_TRACING_NAME : set service name AUTO_INSTRUMENT : applicable for django framework EC2 Integration To use x-ray we need the x-ray daemon in the ec2 instance. We can install this manually or update the user script to install the daemon. Also we have to make sure the sg allow UDP of port 2000 for the x-ray daemon Inatall daemon in ec2 instance Enable UDP 2000 port On Premise Server Integration Install x-ray daemon For data, relay on x-ray service The UDP 2000 port should be open for x-ray daemon API After X-Ray collects the data, it use to combine abd summarize the trace data. GetTraceSummaries : Can get the trace summaries, ids and annotations. Use traceId or event time to get the summaries BatchGetTraces : Get the full traces GetGroup : A group resources GetServiceGraph : Return info of which service handle the incoming request To make a custom debug tool, First get all trace ids by GetTraceSummaries From each id, get full traces by BatchGetTraces","title":"17 Amazon X Ray"},{"location":"Notes/Other%20Services/17%20Amazon%20X-Ray/#amazon-x-ray","text":"Allow developer debugging the production and distributed application and micro services For cost effective performance, Enable x-ray sampling The sampling rate should be low, so Get significant number of request statistically Optimum traces","title":"Amazon X-Ray"},{"location":"Notes/Other%20Services/17%20Amazon%20X-Ray/#properties","text":"Segments : Used to break down the data in sub segments. Sub Segments : Provides, timings, other aws service/resource calls in downstream forms. Inferred Segment : Allow display timings and calls to all services, even the services who does not support x-ray or tracing annotations and metadata do not trace the calls of other aws services and resources. Instead these annotations and metadata are included in the segments and sub-segments . sub-segments fields, namespace : Can be aws or remote aws for AWS SDK calls remote for other downstream calls http : Outgoing HTTP call annotations : Key-value for index search metadata : Additional data for debugging annotations can be used to filter the traced data from console. Also can trace by groups. metadata can include some additional key value data, that can be helpful during the debugging and tracing. Segment Document Add custom attributes as annotation to use as filter expression Add custom attributes as metadata to store custom data during trace Add custom attributes as segment field to use ::::TODO Sampling Rule to create a representative sampling of tracing With default sampling one request per sec and 5% additional requests per host Segment Properties Mandatory: Name id Trace_id Start_time End_time In_progress Optional: Service User Origin Parent_id http aws error, throttle, cause annotations metadata subsegments","title":"Properties"},{"location":"Notes/Other%20Services/17%20Amazon%20X-Ray/#sending-data-to-x-ray","text":"Two ways, Use putSegmentTrace to uploda data directly to x-ray Put multiple request to x-ray-daemon and later using buffer, the x-ray-daemon will buffer these to the x-ray","title":"Sending data to x-ray"},{"location":"Notes/Other%20Services/17%20Amazon%20X-Ray/#x-ray-daemon","text":"X-Ray sdk does not send data directly to X-Ray. Instead it sends data for multiple request in the daemon and then sends these to the X-Ray service. X-Ray Daemon listens to traffic to UDP 2000 port,","title":"X Ray Daemon"},{"location":"Notes/Other%20Services/17%20Amazon%20X-Ray/#service-map","text":"Create dependency trees between services Detect latency between services","title":"Service Map"},{"location":"Notes/Other%20Services/17%20Amazon%20X-Ray/#elastic-beanstalk-integration","text":"X-Ray is already installed in the elastic beanstalk. To enable the x-ray, update/create a config under the beanstalk extensions, .ebextensions/x-ray-daemon.config . This can also be done using by management console. max-age To get trace data, code must instrumented wit the x-ray sdk","title":"Elastic Beanstalk Integration"},{"location":"Notes/Other%20Services/17%20Amazon%20X-Ray/#ecs-integration","text":"To use, Create a docker image, runs x-ray daemon Put the image in docker repository Launch in the ecs cluster ECS can integrate x-ray by x-ray daemon and side-car pattern As Daemon Pattern : x-ray daemin in ec2 instance. To use x-ray in ecs, we need to use the x-ray daemon container for each of the ecs instances. In this case, the x-ray daemon acts as the x-ray agents. As Side Car Pattern : x-ray daemin in container. In this case, each of the container in the ecs will have x-ray daemon inside it. When we need to use x-ray in the fargate, we have to use the side-car pattern. Because, we do not have control over the ec2 instances in fargate. To set up Run x-ray daemon according to the suited pattern Make port mapping to allows UDP 2000 for x-ray daemon","title":"ECS Integration"},{"location":"Notes/Other%20Services/17%20Amazon%20X-Ray/#lambda-integration","text":"To implement the X-ray tracing, we need to implement the followings, Enable Active Tracing from the lambda function configuration. This will run the x-ray daemon for the function The attached policy should have write access to the x-ray daemon Following environment policies will require to communicate with x-ray _X_AMZN_TRACE_ID : tracing header AWS_XRAY_DAEMON_ADDRESS : IP_ADDRESS:PORT Others are, AWS_XRAY_CONTEXT_MISSING : default: LOG_ERROR AWS_XRAY_TRACING_NAME : set service name AUTO_INSTRUMENT : applicable for django framework","title":"Lambda Integration"},{"location":"Notes/Other%20Services/17%20Amazon%20X-Ray/#ec2-integration","text":"To use x-ray we need the x-ray daemon in the ec2 instance. We can install this manually or update the user script to install the daemon. Also we have to make sure the sg allow UDP of port 2000 for the x-ray daemon Inatall daemon in ec2 instance Enable UDP 2000 port","title":"EC2 Integration"},{"location":"Notes/Other%20Services/17%20Amazon%20X-Ray/#on-premise-server-integration","text":"Install x-ray daemon For data, relay on x-ray service The UDP 2000 port should be open for x-ray daemon","title":"On Premise Server Integration"},{"location":"Notes/Other%20Services/17%20Amazon%20X-Ray/#api","text":"After X-Ray collects the data, it use to combine abd summarize the trace data. GetTraceSummaries : Can get the trace summaries, ids and annotations. Use traceId or event time to get the summaries BatchGetTraces : Get the full traces GetGroup : A group resources GetServiceGraph : Return info of which service handle the incoming request To make a custom debug tool, First get all trace ids by GetTraceSummaries From each id, get full traces by BatchGetTraces","title":"API"},{"location":"Notes/Other%20Services/18%20AWS%20Workdocs/","text":"AWS Workdocs Similar to dropbox or One drive For enhanced security Only power users can invite new external users Only power users can share public links","title":"18 AWS Workdocs"},{"location":"Notes/Other%20Services/18%20AWS%20Workdocs/#aws-workdocs","text":"Similar to dropbox or One drive For enhanced security Only power users can invite new external users Only power users can share public links","title":"AWS Workdocs"},{"location":"Notes/Other%20Services/19%20AWS%20Batch/","text":"AWS Batch Use to run thousands of batch computing jobs Dynamically provision Quantity Compute Resource (CPU, Memory) Multiple job Queue can be created high priority jobs are in on-demand instance Low priority jobs are in spot instance AWS Log Driver can be used to get logs in the Cloudwatch for further investigation","title":"19 AWS Batch"},{"location":"Notes/Other%20Services/19%20AWS%20Batch/#aws-batch","text":"Use to run thousands of batch computing jobs Dynamically provision Quantity Compute Resource (CPU, Memory) Multiple job Queue can be created high priority jobs are in on-demand instance Low priority jobs are in spot instance AWS Log Driver can be used to get logs in the Cloudwatch for further investigation","title":"AWS Batch"},{"location":"Notes/Other%20Services/20%20AWS%20Trusted%20Advisor/","text":"AWS Trusted Advisor Provide real time guidance to help AWS best practices It ensure Cost Optimization Security Fault Tolerance Performance Service Limits Can monitor S3 (Check versioning) Redshift Cluster (Check cluster configuration) Exposed Access Keys (Check if the access keys become public) EC2 Reserved Instance (Check if the instance will be expired in 30 days) Service limits (IAM service limit check)","title":"20 AWS Trusted Advisor"},{"location":"Notes/Other%20Services/20%20AWS%20Trusted%20Advisor/#aws-trusted-advisor","text":"Provide real time guidance to help AWS best practices It ensure Cost Optimization Security Fault Tolerance Performance Service Limits Can monitor S3 (Check versioning) Redshift Cluster (Check cluster configuration) Exposed Access Keys (Check if the access keys become public) EC2 Reserved Instance (Check if the instance will be expired in 30 days) Service limits (IAM service limit check)","title":"AWS Trusted Advisor"},{"location":"Notes/Other%20Services/21%20AWS%20GreenGrass/","text":"AWS GreenGrass Used to run lambda in connected devices Execute prediction based machine learning models Communicate between devices without internet Keep device date in sync","title":"21 AWS GreenGrass"},{"location":"Notes/Other%20Services/21%20AWS%20GreenGrass/#aws-greengrass","text":"Used to run lambda in connected devices Execute prediction based machine learning models Communicate between devices without internet Keep device date in sync","title":"AWS GreenGrass"},{"location":"Notes/Other%20Services/22%20AWS%20Migration%20Hub/","text":"AWS Migration Hub Use to Choose migration tool Track the progress of migration in AWS Single place to monitor migration Can be helpful while using DMS","title":"22 AWS Migration Hub"},{"location":"Notes/Other%20Services/22%20AWS%20Migration%20Hub/#aws-migration-hub","text":"Use to Choose migration tool Track the progress of migration in AWS Single place to monitor migration Can be helpful while using DMS","title":"AWS Migration Hub"},{"location":"Notes/Other%20Services/23%20AWS%20IoT%20Core/","text":"AWS IoT Core Lets connected device securely and easily interact with cloud applications Allow secure communication and data processing across different applications","title":"23 AWS IoT Core"},{"location":"Notes/Other%20Services/23%20AWS%20IoT%20Core/#aws-iot-core","text":"Lets connected device securely and easily interact with cloud applications Allow secure communication and data processing across different applications","title":"AWS IoT Core"},{"location":"Notes/Other%20Services/24%20AWS%20Secret%20Manager/","text":"AWS Secret Manager Manage secrets for applications Can encrypt the keys and credentials Can be enabled auto rotation Has strong integration with cloudformation and RDS Used for Database credentials Generate credentials using aws sdk API keys Other app secrets Parameter Store vs Secret Manager Parameter store has TTL, secret manager has auto rotation Parameter store can store config and secrets where the secret manager only store secrets Secret manager has tight integration with RDS","title":"24 AWS Secret Manager"},{"location":"Notes/Other%20Services/24%20AWS%20Secret%20Manager/#aws-secret-manager","text":"Manage secrets for applications Can encrypt the keys and credentials Can be enabled auto rotation Has strong integration with cloudformation and RDS Used for Database credentials Generate credentials using aws sdk API keys Other app secrets","title":"AWS Secret Manager"},{"location":"Notes/Other%20Services/24%20AWS%20Secret%20Manager/#parameter-store-vs-secret-manager","text":"Parameter store has TTL, secret manager has auto rotation Parameter store can store config and secrets where the secret manager only store secrets Secret manager has tight integration with RDS","title":"Parameter Store vs Secret Manager"},{"location":"Notes/Other%20Services/25%20AWS%20Cost%20Explorer/","text":"AWS Cost Explorer Enable to View usage Analyze cost Visualize over time Chart Graph Can be used to generate usage and cost periodically (Like everyday) over time and put it in s3 as CSV file","title":"25 AWS Cost Explorer"},{"location":"Notes/Other%20Services/25%20AWS%20Cost%20Explorer/#aws-cost-explorer","text":"Enable to View usage Analyze cost Visualize over time Chart Graph Can be used to generate usage and cost periodically (Like everyday) over time and put it in s3 as CSV file","title":"AWS Cost Explorer"},{"location":"Notes/Other%20Services/26%20AWS%20Budget/","text":"AWS Budget Use for setting custom budget alert AWS requires 5 weeks of data uage to generate the budget forecast","title":"26 AWS Budget"},{"location":"Notes/Other%20Services/26%20AWS%20Budget/#aws-budget","text":"Use for setting custom budget alert AWS requires 5 weeks of data uage to generate the budget forecast","title":"AWS Budget"},{"location":"Notes/Other%20Services/27%20AWS%20Inspector/","text":"AWS Inspector Automated security assessment service Improve security and application compliance","title":"27 AWS Inspector"},{"location":"Notes/Other%20Services/27%20AWS%20Inspector/#aws-inspector","text":"Automated security assessment service Improve security and application compliance","title":"AWS Inspector"},{"location":"Notes/Other%20Services/28%20AWS%20Policy%20Simulator/","text":"AWS Policy Simulator Can evaluate the policy, permissions and actions Does not make the actual changes in the resources Since it does not make any actual request, it only response if the action is allowed or denied Any changes in the policy emulator does not change the actual policy In AWS Organizations, the SCP can be simulated To use in cli Need the context keys Use iam simulate-custom-policy command","title":"28 AWS Policy Simulator"},{"location":"Notes/Other%20Services/28%20AWS%20Policy%20Simulator/#aws-policy-simulator","text":"Can evaluate the policy, permissions and actions Does not make the actual changes in the resources Since it does not make any actual request, it only response if the action is allowed or denied Any changes in the policy emulator does not change the actual policy In AWS Organizations, the SCP can be simulated To use in cli Need the context keys Use iam simulate-custom-policy command","title":"AWS Policy Simulator"},{"location":"Notes/Other%20Services/29%20AWS%20CodeStar/","text":"AWS CodeStar Quickly develop, build and deploy app within AWS Can use template of ec2, beanstalk or lambda From dashboard we can see day to day activity and dev tasks (Jira/Github Issue Trackers) Integrated with, Github/CodeCommit CodeBuild, CodeDeploy, CloudFormation, CodePipeline, CloudWatch Jira/Github Issue Tracker Cloud9 IDE CodeStar is free, only charges for underlying services","title":"29 AWS CodeStar"},{"location":"Notes/Other%20Services/29%20AWS%20CodeStar/#aws-codestar","text":"Quickly develop, build and deploy app within AWS Can use template of ec2, beanstalk or lambda From dashboard we can see day to day activity and dev tasks (Jira/Github Issue Trackers) Integrated with, Github/CodeCommit CodeBuild, CodeDeploy, CloudFormation, CodePipeline, CloudWatch Jira/Github Issue Tracker Cloud9 IDE CodeStar is free, only charges for underlying services","title":"AWS CodeStar"},{"location":"Notes/Other%20Services/30%20Step%20Function/","text":"Step Function JSON state machine Orchestrate virtual workflow by lambda functions Can resolve the timeout issue of the long running lambda function Also work with ECS EC2 API Gateway On Premise Server Has feature to implement Human Approval Feature , but not native Use to automate the recurring task States Task State : Used to do work, task or run processes Choice State : Make choices between branches of execution Failed or Succeed State : Stop the execution with fail or success state Pass State : Do not run any process, simply take input and pass it to output. If needs any fix, do the fixing and send to the output Wait State : Make a delay before next time. The delayed time can be specified Parallel State : Start parallel branch execution Map State : Iterate the states. ItemPath used here To run process, Task State or Parallel State can be used. Handle Error Both the Task State and Parallel state has Catch and Retry field. To handle error and do a retry we can make use of these state machine features. Handle Error Capture Error By Types States.ALL: Matches any error States.Timeout: Task running longer or no heartbeat received States.TaskFailed: Execution failure States.Permissions: No previllage to execute code Retrier For retry, we can define different policy for the error. With different error, we can go through different error policy. ErrorEquals : Array of error names. When error occur, it goes to the corresponding retry policy. IntervalSeconds : Interval time before retry. Default is 1 second. MaxAttempts : Maximum number of retry, default is 3. BackoffRate : Additional delay with each new retry, default is 2 seconds Catcher ErrorEquals : Array of error names. When error occurs, it goes to the exactly same named retry policy. Next : Next state machine name ResultPath : Determine next state machine input path and in case of error, pass it to the next step Wait for task token When a step function is relied on the services that might take time, so the function needs to hold on, then use the wait for task token service Appennd .waitForTaskToken in the end of resource ARN In the message body add the `TaskToken`` as key so the reciving application knows how to callback the step function After the task is completed, the SendTaskApi will be called with the taskToken Activity Task Activity worker poll task from the step functions After task is completed, it sends the output with TaskToken To keep the task active, use TimeoutSeconds HeartbeatSeconds Wait for task token vs Activity Task Wait for task token is pushing mechanism (to SQS) while Activity Task is poll mechanism Standard vs Express Workflow TBD Best Practices Specify the timeout for the state machine For larger payload between functions, use S3 Let the error be handled by the step functions, not in the task methods","title":"30 Step Function"},{"location":"Notes/Other%20Services/30%20Step%20Function/#step-function","text":"JSON state machine Orchestrate virtual workflow by lambda functions Can resolve the timeout issue of the long running lambda function Also work with ECS EC2 API Gateway On Premise Server Has feature to implement Human Approval Feature , but not native Use to automate the recurring task States Task State : Used to do work, task or run processes Choice State : Make choices between branches of execution Failed or Succeed State : Stop the execution with fail or success state Pass State : Do not run any process, simply take input and pass it to output. If needs any fix, do the fixing and send to the output Wait State : Make a delay before next time. The delayed time can be specified Parallel State : Start parallel branch execution Map State : Iterate the states. ItemPath used here To run process, Task State or Parallel State can be used. Handle Error Both the Task State and Parallel state has Catch and Retry field. To handle error and do a retry we can make use of these state machine features.","title":"Step Function"},{"location":"Notes/Other%20Services/30%20Step%20Function/#handle-error","text":"Capture Error By Types States.ALL: Matches any error States.Timeout: Task running longer or no heartbeat received States.TaskFailed: Execution failure States.Permissions: No previllage to execute code Retrier For retry, we can define different policy for the error. With different error, we can go through different error policy. ErrorEquals : Array of error names. When error occur, it goes to the corresponding retry policy. IntervalSeconds : Interval time before retry. Default is 1 second. MaxAttempts : Maximum number of retry, default is 3. BackoffRate : Additional delay with each new retry, default is 2 seconds Catcher ErrorEquals : Array of error names. When error occurs, it goes to the exactly same named retry policy. Next : Next state machine name ResultPath : Determine next state machine input path and in case of error, pass it to the next step Wait for task token When a step function is relied on the services that might take time, so the function needs to hold on, then use the wait for task token service Appennd .waitForTaskToken in the end of resource ARN In the message body add the `TaskToken`` as key so the reciving application knows how to callback the step function After the task is completed, the SendTaskApi will be called with the taskToken Activity Task Activity worker poll task from the step functions After task is completed, it sends the output with TaskToken To keep the task active, use TimeoutSeconds HeartbeatSeconds Wait for task token vs Activity Task Wait for task token is pushing mechanism (to SQS) while Activity Task is poll mechanism Standard vs Express Workflow TBD Best Practices Specify the timeout for the state machine For larger payload between functions, use S3 Let the error be handled by the step functions, not in the task methods","title":"Handle Error"},{"location":"Notes/Other%20Services/31%20AWS%20CDK/","text":"AWS CDK Cloud Development Kit CDK use a wrapper called CDK Synth that compiles code to CloudFormation Use to model and provision the resources using the languages like TypeScript, JavaScript, Python, Java, C# etc Ultimately, codes in the CDK compiles to Cloudformation Very safe compare to CloudFormation as yaml or json can be error prune, while CDK is written in type safe programming language, so if the code compiles, the resources will be built. Wide range compare to SAM. SAM is focused on serverless app, quickly up and running with lambda function, while the CDK can be used for creating all kinds of resources including serverless. CDK can utilize SAM for local testing CDK will use CDK Synth to generate CloudFormation template Later SAM cli will be used to run them offline CDK Constructs It is 3 layers First layer, have to create everything from scratch Second layer, built something on behalf of us Third layer, built a lot of things, on behalf of us For example, when we need to create a S3 bucket, we can use layer 1. For complex task like making a lambda api, the CDK will do a lot of things on behalf of us, like creating a Security Group, creating bucket etc. CDK Bootstrapping Before deploy a CDK, we must need a CDK toolkit for that particular account + region CDK toolkit is the necessary reources for creating CDK in AWS, example resources are S3 bucket IAM Roles Now, before deploying the first CDK, CDK Bootstrapping generate CDK Toolkit , that contains all these resources Unit Testing CDK Assertions Module are avaliable with popular test frameworks (Like Jest in JS world) Two types of test Fine-grained Assertions: Test properties of the resources Snapshot Test: Compare with previous template snapshot Can test with my stack or template, that is not in my stack Life Cycle Get template Add code to the template Synthesize (optional but good for testing errors) Build the app (optional, required to build app in some of the programming language) Deploy the app","title":"31 AWS CDK"},{"location":"Notes/Other%20Services/31%20AWS%20CDK/#aws-cdk","text":"Cloud Development Kit CDK use a wrapper called CDK Synth that compiles code to CloudFormation Use to model and provision the resources using the languages like TypeScript, JavaScript, Python, Java, C# etc Ultimately, codes in the CDK compiles to Cloudformation Very safe compare to CloudFormation as yaml or json can be error prune, while CDK is written in type safe programming language, so if the code compiles, the resources will be built. Wide range compare to SAM. SAM is focused on serverless app, quickly up and running with lambda function, while the CDK can be used for creating all kinds of resources including serverless. CDK can utilize SAM for local testing CDK will use CDK Synth to generate CloudFormation template Later SAM cli will be used to run them offline CDK Constructs It is 3 layers First layer, have to create everything from scratch Second layer, built something on behalf of us Third layer, built a lot of things, on behalf of us For example, when we need to create a S3 bucket, we can use layer 1. For complex task like making a lambda api, the CDK will do a lot of things on behalf of us, like creating a Security Group, creating bucket etc. CDK Bootstrapping Before deploy a CDK, we must need a CDK toolkit for that particular account + region CDK toolkit is the necessary reources for creating CDK in AWS, example resources are S3 bucket IAM Roles Now, before deploying the first CDK, CDK Bootstrapping generate CDK Toolkit , that contains all these resources Unit Testing CDK Assertions Module are avaliable with popular test frameworks (Like Jest in JS world) Two types of test Fine-grained Assertions: Test properties of the resources Snapshot Test: Compare with previous template snapshot Can test with my stack or template, that is not in my stack Life Cycle Get template Add code to the template Synthesize (optional but good for testing errors) Build the app (optional, required to build app in some of the programming language) Deploy the app","title":"AWS CDK"},{"location":"Notes/Other%20Services/32%20AWS%20Security%20Center/","text":"AWS Security Center Receive the security update Can report security concerns","title":"32 AWS Security Center"},{"location":"Notes/Other%20Services/32%20AWS%20Security%20Center/#aws-security-center","text":"Receive the security update Can report security concerns","title":"AWS Security Center"},{"location":"Notes/Other%20Services/33%20AWS%20Data%20Pipeline/","text":"AWS Data Pipeline Web service Use to move data Example ec2 to s3 Use Data Nodes to determine the source and the destination","title":"33 AWS Data Pipeline"},{"location":"Notes/Other%20Services/33%20AWS%20Data%20Pipeline/#aws-data-pipeline","text":"Web service Use to move data Example ec2 to s3 Use Data Nodes to determine the source and the destination","title":"AWS Data Pipeline"},{"location":"Notes/Other%20Services/34%20Code%20Artifact/","text":"Code Artifact Storing and retrieving dependencies are artifact management Artifact management in AWS Integrated with common dependency management tools, like Maven, Gradle, NPM, Yarn, pip, NuGet etc. Developers and CodeBuild can directly retrieve dependencies from the CodeArtifact Can be used as proxy First developer/codeBuild ask the dependency to CodeArtifact If it's not available in CodeArtifact cache, then it fetch from third party dependency management tools and serve the CodeBuild or Developers. Also, cache the dependencies. CodeArtifact can send event to EventBridge , allows pass the events to Lambda, SNS, SQS, triggering pipeline etc. Access to other account Allow all code artifacts or none. It does not allow specific or some artifacts Use resource based policy to allow other accounts Upstream Repositories [TBD] Can have up to upstream repositories Can have maximum 1 external connection Retention Domains","title":"34 Code Artifact"},{"location":"Notes/Other%20Services/34%20Code%20Artifact/#code-artifact","text":"Storing and retrieving dependencies are artifact management Artifact management in AWS Integrated with common dependency management tools, like Maven, Gradle, NPM, Yarn, pip, NuGet etc. Developers and CodeBuild can directly retrieve dependencies from the CodeArtifact Can be used as proxy First developer/codeBuild ask the dependency to CodeArtifact If it's not available in CodeArtifact cache, then it fetch from third party dependency management tools and serve the CodeBuild or Developers. Also, cache the dependencies. CodeArtifact can send event to EventBridge , allows pass the events to Lambda, SNS, SQS, triggering pipeline etc. Access to other account Allow all code artifacts or none. It does not allow specific or some artifacts Use resource based policy to allow other accounts Upstream Repositories [TBD] Can have up to upstream repositories Can have maximum 1 external connection Retention Domains","title":"Code Artifact"},{"location":"Notes/Other%20Services/35%20Code%20Guru/","text":"Code Guru A ML powered service for Reviewer: Automated code reviews Profiler: Recommandation for performance tuning Agent Configuration Not Important [TBD]","title":"35 Code Guru"},{"location":"Notes/Other%20Services/35%20Code%20Guru/#code-guru","text":"A ML powered service for Reviewer: Automated code reviews Profiler: Recommandation for performance tuning Agent Configuration Not Important [TBD]","title":"Code Guru"},{"location":"Notes/Other%20Services/36%20Cloud9/","text":"Cloud9 Cloud Code Editor Has close integration with AWS Manage resource Provision resouce By default installed git, Node.js, Python etc.","title":"36 Cloud9"},{"location":"Notes/Other%20Services/36%20Cloud9/#cloud9","text":"Cloud Code Editor Has close integration with AWS Manage resource Provision resouce By default installed git, Node.js, Python etc.","title":"Cloud9"},{"location":"Notes/Other%20Services/37%20AWS%20Amplify/","text":"AWS Amplify Use for create web/mobile/full-stack application Components Amplify Studio for full stack app Amplify CLI for guided backend Amplify Libraries for AWS services integration like S3, cognito Amplify Hosting is like netlify or vercel Features Built in athentication using CUP Easy integration of API and Data Store using AppSync and DynamoDB Hosting with CI-CD Testing using Cypress","title":"37 AWS Amplify"},{"location":"Notes/Other%20Services/37%20AWS%20Amplify/#aws-amplify","text":"Use for create web/mobile/full-stack application Components Amplify Studio for full stack app Amplify CLI for guided backend Amplify Libraries for AWS services integration like S3, cognito Amplify Hosting is like netlify or vercel Features Built in athentication using CUP Easy integration of API and Data Store using AppSync and DynamoDB Hosting with CI-CD Testing using Cypress","title":"AWS Amplify"},{"location":"Notes/Other%20Services/38%20AWS%20Nitro%20Enclaves/","text":"AWS Nitro Enclaves Used for very very sensitive data computation TBD","title":"38 AWS Nitro Enclaves"},{"location":"Notes/Other%20Services/38%20AWS%20Nitro%20Enclaves/#aws-nitro-enclaves","text":"Used for very very sensitive data computation TBD","title":"AWS Nitro Enclaves"},{"location":"Notes/Other%20Services/39%20AWS%20Distro/","text":"AWS Distro Similar to x-ray but allows tracing different backend application without any re-instruments","title":"39 AWS Distro"},{"location":"Notes/Other%20Services/39%20AWS%20Distro/#aws-distro","text":"Similar to x-ray but allows tracing different backend application without any re-instruments","title":"AWS Distro"},{"location":"Notes/Other%20Services/40%20AWS%20Macie/","text":"AWS Macie AWS Macie is a data security tool Use machine learning and pattern matching for detecting sensetive data Can be used in the S3 objects","title":"40 AWS Macie"},{"location":"Notes/Other%20Services/40%20AWS%20Macie/#aws-macie","text":"AWS Macie is a data security tool Use machine learning and pattern matching for detecting sensetive data Can be used in the S3 objects","title":"AWS Macie"},{"location":"Notes/Other%20Services/41%20AWS%20Private%20Certificate%20Authority/","text":"AWS Private Certificate Authority Public key infrastructure service IAM integration Cloud trail for auditing Support private certificates","title":"41 AWS Private Certificate Authority"},{"location":"Notes/Other%20Services/41%20AWS%20Private%20Certificate%20Authority/#aws-private-certificate-authority","text":"Public key infrastructure service IAM integration Cloud trail for auditing Support private certificates","title":"AWS Private Certificate Authority"},{"location":"Notes/Other%20Services/42%20AWS%20AppConfig/","text":"AWS AppConfig Use for manage and scale configuration Encrypeted by AWS owned keys by default Can use additional layer for encryption using customer keys","title":"42 AWS AppConfig"},{"location":"Notes/Other%20Services/42%20AWS%20AppConfig/#aws-appconfig","text":"Use for manage and scale configuration Encrypeted by AWS owned keys by default Can use additional layer for encryption using customer keys","title":"AWS AppConfig"},{"location":"Notes/Other%20Services/43%20AWS%20CoPilot/","text":"AWS CoPilot Guide and help to simplify container based application","title":"43 AWS CoPilot"},{"location":"Notes/Other%20Services/43%20AWS%20CoPilot/#aws-copilot","text":"Guide and help to simplify container based application","title":"AWS CoPilot"},{"location":"Notes/Other%20Services/01%20CI-CD_.md/01%20Codepipeline/","text":"CI/CD Code Pipeline Flow Code -> Build -> Test -> Deploy -> Provision Every stage, the codes are called Artifact and stored in the S3. The next stage take the artifact from the previous stage and after processing, create a new artifact to pass it to the next stage. In codePipeline, a single stage can have multiple action groups In code pipeline, for manual approval (SNS + Email), the user need two permission GetPipeline PutApproveResult We can trigger code pipeline using, Events Using event-bridge from code-commit Using code-star github app Webhook Script to hit the webhook of the code pipeline when code is updated Polling [Not Recommanded] Codepipeline always check the repository in a time interval and check if code is updated Troubleshooting of code pipeline Check the console Check the IAM permission Audit the failed API call using CloudTrail AWS Codepipeline AWS Codecommit Like Github Responsible for Code portion AWS Codebuild Like Jenkins Responsible for Build and Test portion AWS Code Deploy Responsible for Deploy portion Ues AWS Beanstalk or AWS Cloudformation to provision the code Can be use for deploying code to EC2 Instance On premise sever Lambda Function Use for Rapid release of new feature Updating Lambda Function Avoid downtime during Application deployment Type of code deploy Linear (This deployment send traffic incrementally) All at once (All traffic shifts to new deployment) Canary (Can be define, how many traffic will flow new and updated deployments) While deploying a code base Code commit can be a source stage Beanstalk can be a deploy stage Blue/Green Deployment Isolation between blue and green Roll incoming traffic during deployments Minimum downtime Cloudformation can be used in stages, to deploy a test environment and delete after testing","title":"01 Codepipeline"},{"location":"Notes/Other%20Services/01%20CI-CD_.md/01%20Codepipeline/#cicd","text":"","title":"CI/CD"},{"location":"Notes/Other%20Services/01%20CI-CD_.md/01%20Codepipeline/#code-pipeline","text":"Flow Code -> Build -> Test -> Deploy -> Provision Every stage, the codes are called Artifact and stored in the S3. The next stage take the artifact from the previous stage and after processing, create a new artifact to pass it to the next stage. In codePipeline, a single stage can have multiple action groups In code pipeline, for manual approval (SNS + Email), the user need two permission GetPipeline PutApproveResult We can trigger code pipeline using, Events Using event-bridge from code-commit Using code-star github app Webhook Script to hit the webhook of the code pipeline when code is updated Polling [Not Recommanded] Codepipeline always check the repository in a time interval and check if code is updated Troubleshooting of code pipeline Check the console Check the IAM permission Audit the failed API call using CloudTrail AWS Codepipeline AWS Codecommit Like Github Responsible for Code portion AWS Codebuild Like Jenkins Responsible for Build and Test portion AWS Code Deploy Responsible for Deploy portion Ues AWS Beanstalk or AWS Cloudformation to provision the code Can be use for deploying code to EC2 Instance On premise sever Lambda Function Use for Rapid release of new feature Updating Lambda Function Avoid downtime during Application deployment Type of code deploy Linear (This deployment send traffic incrementally) All at once (All traffic shifts to new deployment) Canary (Can be define, how many traffic will flow new and updated deployments) While deploying a code base Code commit can be a source stage Beanstalk can be a deploy stage Blue/Green Deployment Isolation between blue and green Roll incoming traffic during deployments Minimum downtime Cloudformation can be used in stages, to deploy a test environment and delete after testing","title":"Code Pipeline"},{"location":"Notes/Other%20Services/01%20CI-CD_.md/02%20CodeCommit/","text":"CodeCommit AWS service similar to github By default encrypted at rest Permissions of codecommit, codecommit:* : Allow all the actions codecommit:CreateBranch : Allow to create branches codecommit:DeleteBranch : Allow to delete branches codecommit:CreateRepository : Allow to create repository codecommit:DeleteRepository : Allow to delete repository To give access of codecommit to other developers, we can dod any of the followings Create git credentials with the AWS Credential profile (to take the access key credentials) AWS CodeCommit supports, Access Keys, SSH and Https git credentials Generate a new SSH keys and associate public SSH keys to the IAM user Generate HTTPS git credentials and specify them in git credential manager For developer with another account, Create cross account role (AWS STS, AssumeRole API) Give repository permission to that role Provide the role arn to the developer","title":"02 CodeCommit"},{"location":"Notes/Other%20Services/01%20CI-CD_.md/02%20CodeCommit/#codecommit","text":"AWS service similar to github By default encrypted at rest Permissions of codecommit, codecommit:* : Allow all the actions codecommit:CreateBranch : Allow to create branches codecommit:DeleteBranch : Allow to delete branches codecommit:CreateRepository : Allow to create repository codecommit:DeleteRepository : Allow to delete repository To give access of codecommit to other developers, we can dod any of the followings Create git credentials with the AWS Credential profile (to take the access key credentials) AWS CodeCommit supports, Access Keys, SSH and Https git credentials Generate a new SSH keys and associate public SSH keys to the IAM user Generate HTTPS git credentials and specify them in git credential manager For developer with another account, Create cross account role (AWS STS, AssumeRole API) Give repository permission to that role Provide the role arn to the developer","title":"CodeCommit"},{"location":"Notes/Other%20Services/01%20CI-CD_.md/03%20CodeDeploy/","text":"Codedeploy CodeDeploy can be used to deploy code to EC2 instance/on-premise server/AWS Lambda. Supports In Place EC2 Instance On Premise Servers Blue/Green Lambda ECS Rolling Deployments Usage EC2 Instances On premise servers Serverless Lambda functions ECS Services Code Origins S3 Buckets Github Repositories Bitbucket Repositories CodeCommit Setup Need the code-deploy agent in the server A config file must be included in the root of the source code, like appspec.yml Process On commit, codeDeploy will pull the code Deploy according to the config files CodeDeploy agent will report the success/failure CodeDeploy only do the deployment, do not provision resource Blue/green deployments are only available for the EC2 Instances not the on-premise servers When need to deploy in multiple environments, need to create multiple codeDeploy group AppSpec With appspec.yml , we define how we get the codebase and deploy it. In the File Section we define the source like S3 or github. Then, have a sequence of following hooks and we can define our actions in these hook, Application Stop : Stopping the existing version of app DownloadBundle : Get/download the new codebase BeforeInstall : Task to do before installing the new app AfterInstall : Task to do after installing the app ApplicationStart : Task to start the new app ValidateService : A health check to determine if the app is running properly All hooks are, ApplicationStop -> DownloadBundle -> BeforeInstall -> Install -> AfterInstall -> ApplicationStart -> ValidateService -> BeforeAllowTraffic -> AllowTraffic -> AfterAllowTraffic Mandatory properties for the AppSpec.yml file are, Task Definition Container Name Container Port Types Of Deployment In Place Deployment : Also known as Half at a time . First half of the instance get deployed and then the other half of the application deployed. One At a Time : Slowest but lowest availability impact. Blue Green Deployment : Initially it keeps the previous instances and application. A new set of instance will be created and load balancer send traffic on both of these. If everything goes fine, all the traffic will go to the new instances. There must be a ELB and ASG CodeDeploy create a new ASG When new ASG is healthy and serving without error, the old ASG is deleted and trafics go to the new ASG Blue green deployment is not supported by on premise servers Get Secure Parameters In CodeDeploy from Parameter Store Create IAM role to access the Parameter Store Use ssm get-parameters option Rollback When rollback happen, it does not go back to the previous version, instead redeploy the last successfully delployed version Missing Files: During rollback, if the existing files are removed or no permissions, Put these files in the instances Create a new application instance Troubleshooting InvalidSignatureException when the time of AWS and the time of the EC2 instance is not synced For deployment issues, logs are available in the server of path /opt/codedeploy-agent","title":"03 CodeDeploy"},{"location":"Notes/Other%20Services/01%20CI-CD_.md/03%20CodeDeploy/#codedeploy","text":"CodeDeploy can be used to deploy code to EC2 instance/on-premise server/AWS Lambda.","title":"Codedeploy"},{"location":"Notes/Other%20Services/01%20CI-CD_.md/03%20CodeDeploy/#supports","text":"In Place EC2 Instance On Premise Servers Blue/Green Lambda ECS Rolling Deployments","title":"Supports"},{"location":"Notes/Other%20Services/01%20CI-CD_.md/03%20CodeDeploy/#usage","text":"EC2 Instances On premise servers Serverless Lambda functions ECS Services","title":"Usage"},{"location":"Notes/Other%20Services/01%20CI-CD_.md/03%20CodeDeploy/#code-origins","text":"S3 Buckets Github Repositories Bitbucket Repositories CodeCommit Setup Need the code-deploy agent in the server A config file must be included in the root of the source code, like appspec.yml Process On commit, codeDeploy will pull the code Deploy according to the config files CodeDeploy agent will report the success/failure CodeDeploy only do the deployment, do not provision resource Blue/green deployments are only available for the EC2 Instances not the on-premise servers When need to deploy in multiple environments, need to create multiple codeDeploy group AppSpec With appspec.yml , we define how we get the codebase and deploy it. In the File Section we define the source like S3 or github. Then, have a sequence of following hooks and we can define our actions in these hook, Application Stop : Stopping the existing version of app DownloadBundle : Get/download the new codebase BeforeInstall : Task to do before installing the new app AfterInstall : Task to do after installing the app ApplicationStart : Task to start the new app ValidateService : A health check to determine if the app is running properly All hooks are, ApplicationStop -> DownloadBundle -> BeforeInstall -> Install -> AfterInstall -> ApplicationStart -> ValidateService -> BeforeAllowTraffic -> AllowTraffic -> AfterAllowTraffic Mandatory properties for the AppSpec.yml file are, Task Definition Container Name Container Port Types Of Deployment In Place Deployment : Also known as Half at a time . First half of the instance get deployed and then the other half of the application deployed. One At a Time : Slowest but lowest availability impact. Blue Green Deployment : Initially it keeps the previous instances and application. A new set of instance will be created and load balancer send traffic on both of these. If everything goes fine, all the traffic will go to the new instances. There must be a ELB and ASG CodeDeploy create a new ASG When new ASG is healthy and serving without error, the old ASG is deleted and trafics go to the new ASG Blue green deployment is not supported by on premise servers Get Secure Parameters In CodeDeploy from Parameter Store Create IAM role to access the Parameter Store Use ssm get-parameters option Rollback When rollback happen, it does not go back to the previous version, instead redeploy the last successfully delployed version Missing Files: During rollback, if the existing files are removed or no permissions, Put these files in the instances Create a new application instance Troubleshooting InvalidSignatureException when the time of AWS and the time of the EC2 instance is not synced For deployment issues, logs are available in the server of path /opt/codedeploy-agent","title":"Code Origins"},{"location":"Notes/Other%20Services/01%20CI-CD_.md/04%20CodeBuild/","text":"CodeBuild TBD: Need to add the code build detail property explaination Build can be defined in code-pipeline/code-build Source/destinations Source can be CodeCommit, S3, Bitbucket or Github Build instruction is written in the buildspec.yml Log can be stored in S3 or Cloudwatch Logs Logs are available in Cloudwatch logs Metrics are available in Cloudwatch metrics Thresholds are available in Cloudwatch Alarm (How long the codebuild should run) EventBridge can be used for trigger notifications CodeBuild agent can be used to test the code build locally By default CodeBuild launched outside the VPC, so can not access the reources Providing vpc id, subnet id and security group ids, it can run inside the VPC and acess resoures As environment varibale we can use Plaintext Parameter store Secret manager By enabling Codebuild Timeout , can ensure the code build is not running long time Run On Proxy Server To run the codeBuild in the proxy server, Configure ssl-bump Update server security policy for ssl-bump Specify the proxy element in the buildspec.yml When the developer does not have the access of the code and can not run edit buildspec.yml , he can use cli to run the code build using the parameter buildspecOverride Access VPC Resources During Testing By default the codeBuild can not access the vpc resource. To give access, we have to provide vpc specific config like vpc id, subnet id, sg id etc.","title":"04 CodeBuild"},{"location":"Notes/Other%20Services/01%20CI-CD_.md/04%20CodeBuild/#codebuild","text":"TBD: Need to add the code build detail property explaination Build can be defined in code-pipeline/code-build Source/destinations Source can be CodeCommit, S3, Bitbucket or Github Build instruction is written in the buildspec.yml Log can be stored in S3 or Cloudwatch Logs Logs are available in Cloudwatch logs Metrics are available in Cloudwatch metrics Thresholds are available in Cloudwatch Alarm (How long the codebuild should run) EventBridge can be used for trigger notifications CodeBuild agent can be used to test the code build locally By default CodeBuild launched outside the VPC, so can not access the reources Providing vpc id, subnet id and security group ids, it can run inside the VPC and acess resoures As environment varibale we can use Plaintext Parameter store Secret manager By enabling Codebuild Timeout , can ensure the code build is not running long time Run On Proxy Server To run the codeBuild in the proxy server, Configure ssl-bump Update server security policy for ssl-bump Specify the proxy element in the buildspec.yml When the developer does not have the access of the code and can not run edit buildspec.yml , he can use cli to run the code build using the parameter buildspecOverride Access VPC Resources During Testing By default the codeBuild can not access the vpc resource. To give access, we have to provide vpc specific config like vpc id, subnet id, sg id etc.","title":"CodeBuild"},{"location":"Notes/Other%20Services/02%20Elastic%20Beanstalk/01%20Elastic%20beanstalk%20Overview/","text":"Elastic beanstalk Allow deploy app from docker container Handles Deployment Capacity provisioning Load balancing Auto scaling Health monitoring We can take full control of the underlying resources An automation example with Beanstalk Create LAMP stack Download latest PHP from S3 Set up ELB Store the Application Files to S3 Store the Server Logs Files to S3 or Cloudwatch , optionally Can use the worker process environment for the long running tasks and also fo decoupling the application Environment files Dockerrun.aws.json used to to configure multi-container docker environments env.yaml used to configure environment name, solution stack and environment links cron.yaml used to define scheduled worker tasks Instance Profile Used to ensure the interaction with other aws services Environment Manifest Used to define environment, stack name, point to launch config etc. If beanstalk does not support the environment by default, can be make use of custom environment named packer For dockerized app, beanstalk has more preferences over ecs when there are requirements of managing To add a resource as a part of Elastic Beanstalk, we can add them in .ebextensions . For example, to allow usage of Elastic Cache , we can add the config in .ebextentions . In this case, any time we deploy a new template, a new version of elastic cache will be created and delete the old ones. Easiest way to enable https for the Elastic Beanstalk is integrating the Load Balance by updating the config file With lifecycle policy, can be determine, how may old build of elastic beanstalk will kept and also how many days Platform Update Elastic beanstalk regularly update their platform time to time with new versions. Once our application is running in a legacy version and want to update the underlying version, there are two methods, Update Environments Platform Versions : Recommended approach to go to latest version. Perform a Blue/Green Deployment : Recommended approach to go to a specific version. Best Practices To preserve database on environment deletion In production, create database separately In dev/test, use database retention as Create Snapshot","title":"01 Elastic beanstalk Overview"},{"location":"Notes/Other%20Services/02%20Elastic%20Beanstalk/01%20Elastic%20beanstalk%20Overview/#elastic-beanstalk","text":"Allow deploy app from docker container Handles Deployment Capacity provisioning Load balancing Auto scaling Health monitoring We can take full control of the underlying resources An automation example with Beanstalk Create LAMP stack Download latest PHP from S3 Set up ELB Store the Application Files to S3 Store the Server Logs Files to S3 or Cloudwatch , optionally Can use the worker process environment for the long running tasks and also fo decoupling the application Environment files Dockerrun.aws.json used to to configure multi-container docker environments env.yaml used to configure environment name, solution stack and environment links cron.yaml used to define scheduled worker tasks Instance Profile Used to ensure the interaction with other aws services Environment Manifest Used to define environment, stack name, point to launch config etc. If beanstalk does not support the environment by default, can be make use of custom environment named packer For dockerized app, beanstalk has more preferences over ecs when there are requirements of managing To add a resource as a part of Elastic Beanstalk, we can add them in .ebextensions . For example, to allow usage of Elastic Cache , we can add the config in .ebextentions . In this case, any time we deploy a new template, a new version of elastic cache will be created and delete the old ones. Easiest way to enable https for the Elastic Beanstalk is integrating the Load Balance by updating the config file With lifecycle policy, can be determine, how may old build of elastic beanstalk will kept and also how many days Platform Update Elastic beanstalk regularly update their platform time to time with new versions. Once our application is running in a legacy version and want to update the underlying version, there are two methods, Update Environments Platform Versions : Recommended approach to go to latest version. Perform a Blue/Green Deployment : Recommended approach to go to a specific version. Best Practices To preserve database on environment deletion In production, create database separately In dev/test, use database retention as Create Snapshot","title":"Elastic beanstalk"},{"location":"Notes/Other%20Services/02%20Elastic%20Beanstalk/02%20Elastic%20beanstalk%20Deployment/","text":"Deployment Manual Deploy We can deploy code to elastic beanstalk by zip file or war file. We can define the format( zip / war ) by a config file from .elasticbeanstalk/config.yml To deploy we can simply use the eb deploy . Deployment Modes Types of deployment modes: All at once All instances will be updated at a time There's a downtime during this types of deployment Rolling Update couple of instances When first cluster of instances are healthy, move to the next instances Rolling with additional batches Similar to rolling, but spins up a new set of instance So the previous instances are there until new instances are healthy For roll back, this is slower than Blue/Green Immutable Spins up new instances in new ASG When new instances are healthy, move all previous instances Blue/Green A new environment will be deployed A partial percentage of the traffic will route first If the new environment goes right, all traffic will go to new env and previous instances will be removed Traffic Splitting (Canary) vs Blue/Green Canary use ASG to split traffic, Blue/Green use Route53 to split traffic Canary is automated wherase the Blue/Green is lot of manual processing Blue/Green vs Rolling with additional batches Rolling with additional batches require new set of instances while blue/green on create new environment on existing instance Rolling with additional batches is time consuming for spinning up new instances","title":"02 Elastic beanstalk Deployment"},{"location":"Notes/Other%20Services/02%20Elastic%20Beanstalk/02%20Elastic%20beanstalk%20Deployment/#deployment","text":"","title":"Deployment"},{"location":"Notes/Other%20Services/02%20Elastic%20Beanstalk/02%20Elastic%20beanstalk%20Deployment/#manual-deploy","text":"We can deploy code to elastic beanstalk by zip file or war file. We can define the format( zip / war ) by a config file from .elasticbeanstalk/config.yml To deploy we can simply use the eb deploy .","title":"Manual Deploy"},{"location":"Notes/Other%20Services/02%20Elastic%20Beanstalk/02%20Elastic%20beanstalk%20Deployment/#deployment-modes","text":"Types of deployment modes: All at once All instances will be updated at a time There's a downtime during this types of deployment Rolling Update couple of instances When first cluster of instances are healthy, move to the next instances Rolling with additional batches Similar to rolling, but spins up a new set of instance So the previous instances are there until new instances are healthy For roll back, this is slower than Blue/Green Immutable Spins up new instances in new ASG When new instances are healthy, move all previous instances Blue/Green A new environment will be deployed A partial percentage of the traffic will route first If the new environment goes right, all traffic will go to new env and previous instances will be removed Traffic Splitting (Canary) vs Blue/Green Canary use ASG to split traffic, Blue/Green use Route53 to split traffic Canary is automated wherase the Blue/Green is lot of manual processing Blue/Green vs Rolling with additional batches Rolling with additional batches require new set of instances while blue/green on create new environment on existing instance Rolling with additional batches is time consuming for spinning up new instances","title":"Deployment Modes"},{"location":"Notes/Other%20Services/02%20Elastic%20Beanstalk/03%20Elastic%20Beanstalk%20Lifecycle/","text":"Elastic Beanstalk Lifecycle Each time we deploy new code to elastic beanstalk, a new version is created. There is a default version quota and we can not deploy any more when we reach the limit. To configure, we can make use of the lifecycle policy. By lifecycle policy, Define how many versions will be kept Define how many older days, we will kept the versions of the application Where do we kept the legacy versions (like s3) after being expired by number of versions or days","title":"03 Elastic Beanstalk Lifecycle"},{"location":"Notes/Other%20Services/02%20Elastic%20Beanstalk/03%20Elastic%20Beanstalk%20Lifecycle/#elastic-beanstalk-lifecycle","text":"Each time we deploy new code to elastic beanstalk, a new version is created. There is a default version quota and we can not deploy any more when we reach the limit. To configure, we can make use of the lifecycle policy. By lifecycle policy, Define how many versions will be kept Define how many older days, we will kept the versions of the application Where do we kept the legacy versions (like s3) after being expired by number of versions or days","title":"Elastic Beanstalk Lifecycle"},{"location":"Notes/Route%2053/01%20Route%2053%20Fundamentals/","text":"Route 53 Fundamentals Managed DNS Also work as a Domain Registerer Records A hostname to IPv4 AAAA hostname to IPv6 CNAME hostname to hostname Alias hostname to AWS resource Health Check Active-Active Failover All resources should be available all the time All records should be Same name Same Records Same Routing Policy Active-Passive Failover Primary Resource available all the time In case of Failover , Secondary Resource always in standby Support A (Address Record) AAAA (IP V6 address record) CNAME (Canonical name record) CAA (Certification Authority Authorization) MX (Mail Exchange Record) NAPTR (name Authority Pointer Record) NS (Name Server record) SoA (Start of authority record) SPF (Sender Policy Framework) SRV (Service locator) TXT (Text Record) Not Supported DNSSEC ### TTL Stands for Time To Live Default value is 300 sec During this time the browser does not make the request to DNS server for IP When TTL is High The server make less query to the DNS server DNS server has low traffic To change IP, need to wait long time When TTL is low DNS server make DNS query more frequently Easy to change the IP CNAME vs Alias CNAME Points hostname to hostname Only works for non-root domain Alias Points to AWS Resource Works for both root and non-root domain Alias records is a AWS feature for its resources Routing Policy Simple Routing Policy Send traffic randomly to the resources Weighted Routing Policy Send traffic multiple resource with specified percentage Latency Based Routing Policy Send traffic to the resource that has lowest latency Failover Routing Policy Active passive failover (Check the upper section) Geo location Routing Policy Send traffic based on users location Geo Proximity Routing Policy Send traffic based on resource location Multi-value Routing Policy Simple routing policy with health check 3rd Party Domains To import 3rd party domain to Route 53 Create a hosted zone in Route53 Update NS record on 3rd party website to use Route53","title":"01 Route 53 Fundamentals"},{"location":"Notes/Route%2053/01%20Route%2053%20Fundamentals/#route-53-fundamentals","text":"Managed DNS Also work as a Domain Registerer Records A hostname to IPv4 AAAA hostname to IPv6 CNAME hostname to hostname Alias hostname to AWS resource Health Check Active-Active Failover All resources should be available all the time All records should be Same name Same Records Same Routing Policy Active-Passive Failover Primary Resource available all the time In case of Failover , Secondary Resource always in standby Support A (Address Record) AAAA (IP V6 address record) CNAME (Canonical name record) CAA (Certification Authority Authorization) MX (Mail Exchange Record) NAPTR (name Authority Pointer Record) NS (Name Server record) SoA (Start of authority record) SPF (Sender Policy Framework) SRV (Service locator) TXT (Text Record) Not Supported DNSSEC ### TTL Stands for Time To Live Default value is 300 sec During this time the browser does not make the request to DNS server for IP When TTL is High The server make less query to the DNS server DNS server has low traffic To change IP, need to wait long time When TTL is low DNS server make DNS query more frequently Easy to change the IP","title":"Route 53 Fundamentals"},{"location":"Notes/Route%2053/01%20Route%2053%20Fundamentals/#cname-vs-alias","text":"CNAME Points hostname to hostname Only works for non-root domain Alias Points to AWS Resource Works for both root and non-root domain Alias records is a AWS feature for its resources","title":"CNAME vs Alias"},{"location":"Notes/Route%2053/01%20Route%2053%20Fundamentals/#routing-policy","text":"Simple Routing Policy Send traffic randomly to the resources Weighted Routing Policy Send traffic multiple resource with specified percentage Latency Based Routing Policy Send traffic to the resource that has lowest latency Failover Routing Policy Active passive failover (Check the upper section) Geo location Routing Policy Send traffic based on users location Geo Proximity Routing Policy Send traffic based on resource location Multi-value Routing Policy Simple routing policy with health check","title":"Routing Policy"},{"location":"Notes/Route%2053/01%20Route%2053%20Fundamentals/#3rd-party-domains","text":"To import 3rd party domain to Route 53 Create a hosted zone in Route53 Update NS record on 3rd party website to use Route53","title":"3rd Party Domains"},{"location":"Notes/S3/01%20S3%20Overview%20/","text":"S3 S3 is Evenly Consistent S3 AlowMethod has 5 methods support GET PUT POST DELETE HEAD To prevent accidental delete Enable MFA Delete Enable Versioning Put appropriate IAM Role In a bucket, various objects can have various storage class To ensure the data is successfully inserted in the S3 , use HTTP 200 status MD5 Checksum","title":"01 S3 Overview "},{"location":"Notes/S3/01%20S3%20Overview%20/#s3","text":"S3 is Evenly Consistent S3 AlowMethod has 5 methods support GET PUT POST DELETE HEAD To prevent accidental delete Enable MFA Delete Enable Versioning Put appropriate IAM Role In a bucket, various objects can have various storage class To ensure the data is successfully inserted in the S3 , use HTTP 200 status MD5 Checksum","title":"S3"},{"location":"Notes/S3/02%20S3%20Data%20Retrival/","text":"Data Retrieval AWS allows two types of archive in s3 for long time data persisting. AWS Glacier AWS Glacier Deep Archive AWS Glacier 4 Types of Data Retrieval Vault Lock Used for long term record retention AWS Glacier allows to lock the storage with various compliance control Expedited Retrieval Data can be retrieved withing 1-5 minutes Comparatively quick retrieval for urgent need Bulk Retrieval Retrieve data withing 5-12 hours Lowest cost Used to retrieved large amount of data with lowest cost Standard Retrieval Data is retrieved within few hours 3-5 hours Minimum storage charge 90 days duration To transfer object object from S3 Glacier to S3 Regular Class First need to restore the objects Then copy to the S3 Regular Class S3 Regular Class means not the Glacier / Deep Archive Provisioned Capacity allows to use Expedited Retrieval whenever necessary AWS Glacier Deep Archive Standard - 12 hours Bulk - 48 hours Minimum storage charge 180 days duration","title":"02 S3 Data Retrival"},{"location":"Notes/S3/02%20S3%20Data%20Retrival/#data-retrieval","text":"AWS allows two types of archive in s3 for long time data persisting. AWS Glacier AWS Glacier Deep Archive","title":"Data Retrieval"},{"location":"Notes/S3/02%20S3%20Data%20Retrival/#aws-glacier","text":"4 Types of Data Retrieval Vault Lock Used for long term record retention AWS Glacier allows to lock the storage with various compliance control Expedited Retrieval Data can be retrieved withing 1-5 minutes Comparatively quick retrieval for urgent need Bulk Retrieval Retrieve data withing 5-12 hours Lowest cost Used to retrieved large amount of data with lowest cost Standard Retrieval Data is retrieved within few hours 3-5 hours Minimum storage charge 90 days duration To transfer object object from S3 Glacier to S3 Regular Class First need to restore the objects Then copy to the S3 Regular Class S3 Regular Class means not the Glacier / Deep Archive Provisioned Capacity allows to use Expedited Retrieval whenever necessary","title":"AWS Glacier"},{"location":"Notes/S3/02%20S3%20Data%20Retrival/#aws-glacier-deep-archive","text":"Standard - 12 hours Bulk - 48 hours Minimum storage charge 180 days duration","title":"AWS Glacier Deep Archive"},{"location":"Notes/S3/03%20S3%20Buckets%20And%20Objects/","text":"Buckets And Objects Buckets Store objects (aka files ) in Buckets (aka directories ) Should be globally unique name Although S3 is Global , buckets are Region Specific Naming convention No Uppercase No Underscore 3 to 63 characters No IP Start with Lowercase Number Objects Objects has Bucket Name , Prefix , and File Name For a object When Object name is s3://bucket_name/key Here bucket_name is the Bucket Name No Prefix here key is the file name When Object name is s3://bucket_name/folder_1/folder_2/key Here bucket_name is the Bucket Name folder_1 and folder_2 is the Nested Folder , where the File placed key is the file name There is no Folder concept in the S3 , UI tricks to think that way Object can be at most 5TB size Can not upload a Object more than 5GB without multipart Object Metadata contains User Metadata System Metadata Tag can be added Max 10 Tags supported Use for Security Lifecycle Policy When Versioning is enabled , then the Object has Version ID","title":"03 S3 Buckets And Objects"},{"location":"Notes/S3/03%20S3%20Buckets%20And%20Objects/#buckets-and-objects","text":"","title":"Buckets And Objects"},{"location":"Notes/S3/03%20S3%20Buckets%20And%20Objects/#buckets","text":"Store objects (aka files ) in Buckets (aka directories ) Should be globally unique name Although S3 is Global , buckets are Region Specific Naming convention No Uppercase No Underscore 3 to 63 characters No IP Start with Lowercase Number","title":"Buckets"},{"location":"Notes/S3/03%20S3%20Buckets%20And%20Objects/#objects","text":"Objects has Bucket Name , Prefix , and File Name For a object When Object name is s3://bucket_name/key Here bucket_name is the Bucket Name No Prefix here key is the file name When Object name is s3://bucket_name/folder_1/folder_2/key Here bucket_name is the Bucket Name folder_1 and folder_2 is the Nested Folder , where the File placed key is the file name There is no Folder concept in the S3 , UI tricks to think that way Object can be at most 5TB size Can not upload a Object more than 5GB without multipart Object Metadata contains User Metadata System Metadata Tag can be added Max 10 Tags supported Use for Security Lifecycle Policy When Versioning is enabled , then the Object has Version ID","title":"Objects"},{"location":"Notes/S3/04%20S3%20Versioning/","text":"S3 Versioning Enable Version of S3 files Should be enabled in Bucket Level Useful To protect file against un-intended deletes Roll back to any version If Versioning is enable to a Non-versioned bucket, the existing objects get a version of NULL If Versioning is disabled to a Bucket , existing Versioned Object exist When Versioning is enabled Latest version has a flag Latest Version Deleted File has a flag Deleted Marker","title":"04 S3 Versioning"},{"location":"Notes/S3/04%20S3%20Versioning/#s3-versioning","text":"Enable Version of S3 files Should be enabled in Bucket Level Useful To protect file against un-intended deletes Roll back to any version If Versioning is enable to a Non-versioned bucket, the existing objects get a version of NULL If Versioning is disabled to a Bucket , existing Versioned Object exist When Versioning is enabled Latest version has a flag Latest Version Deleted File has a flag Deleted Marker","title":"S3 Versioning"},{"location":"Notes/S3/05%20S3%20Encryption/","text":"Encryption 4 types of Encryption is available SSE-S3 Server side encryption handled by AWS ( S3 ) Data key is managed by S3 Use AES-256 algorithm Header should be x-amz-server-side-encryption: AES-256 Completely free SSE-KMS Server side encryption Key is managed by KMS Header should be x-amz-server-side-encryption: aws:kms KMS create a S3 Bucket Key , later it is used to create a lot of data keys These data keys are used to encrypt files in S3 Useful because Using KMS , we can determine who has access keys Audit Trail Using the bucket key can reduce 99% of KMS call SSE-C Server side encryption Key is provided by us S3 does not store the key To use Must be use HTTPS Every time we pass the Encryption Key using HTTP Header as x-amz-server-side-encryption-customer-key Every time we pass the Algorithm Name using HTTP Header as x-amz-server-side-encryption-customer-algorithm Every time we pass the MD5 Key using HTTP Header as x-amz-server-side-encryption-customer-key-md5 HMAC (Hash based Message Authentication Code) is a salted version of encryption keys AWS generate and store the HMAC to validate the encryption and decryption key HMAC is only for validate the original key, can not be used to encrypt or decrypt the object Since in SSE-C , the key is managed by the client, if the key is lost, all the data will also be lost Client Side Encryption We encrypt the object before uploading We decrypt object after retrieving from the S3 To encrypt and decrypt object in client we can use S3 Encryption Client Or other tools Encryption in Transit Also known as SSL/TLS S3 exposes both HTTP HTTPS Default Encryption We can use Bucket Level Default Encryption So any object uploaded to the bucket will be automatically encrypted, even though the we do not pass appropriate header Default encryption only accepts SSE-S3 SSE-KMS It does not accept SSE-C and Client Side Encryption as Default Encryption Bucket Policy evaluated before the Default Encryption Bucket Policy is the old way of Default Encryption Example of in house key management for S3 Create customer managed CMK Encrypt data with the CMK Store encrypted data and data key in S3 Delete the data keys For decrypt, use CMK to decrypt data key Now delete the data using the Decrypted data key","title":"05 S3 Encryption"},{"location":"Notes/S3/05%20S3%20Encryption/#encryption","text":"4 types of Encryption is available SSE-S3 Server side encryption handled by AWS ( S3 ) Data key is managed by S3 Use AES-256 algorithm Header should be x-amz-server-side-encryption: AES-256 Completely free SSE-KMS Server side encryption Key is managed by KMS Header should be x-amz-server-side-encryption: aws:kms KMS create a S3 Bucket Key , later it is used to create a lot of data keys These data keys are used to encrypt files in S3 Useful because Using KMS , we can determine who has access keys Audit Trail Using the bucket key can reduce 99% of KMS call SSE-C Server side encryption Key is provided by us S3 does not store the key To use Must be use HTTPS Every time we pass the Encryption Key using HTTP Header as x-amz-server-side-encryption-customer-key Every time we pass the Algorithm Name using HTTP Header as x-amz-server-side-encryption-customer-algorithm Every time we pass the MD5 Key using HTTP Header as x-amz-server-side-encryption-customer-key-md5 HMAC (Hash based Message Authentication Code) is a salted version of encryption keys AWS generate and store the HMAC to validate the encryption and decryption key HMAC is only for validate the original key, can not be used to encrypt or decrypt the object Since in SSE-C , the key is managed by the client, if the key is lost, all the data will also be lost Client Side Encryption We encrypt the object before uploading We decrypt object after retrieving from the S3 To encrypt and decrypt object in client we can use S3 Encryption Client Or other tools Encryption in Transit Also known as SSL/TLS S3 exposes both HTTP HTTPS Default Encryption We can use Bucket Level Default Encryption So any object uploaded to the bucket will be automatically encrypted, even though the we do not pass appropriate header Default encryption only accepts SSE-S3 SSE-KMS It does not accept SSE-C and Client Side Encryption as Default Encryption Bucket Policy evaluated before the Default Encryption Bucket Policy is the old way of Default Encryption Example of in house key management for S3 Create customer managed CMK Encrypt data with the CMK Store encrypted data and data key in S3 Delete the data keys For decrypt, use CMK to decrypt data key Now delete the data using the Decrypted data key","title":"Encryption"},{"location":"Notes/S3/06%20S3%20Security%20And%20Bucket%20Policy/","text":"Security User Based IAM policy determine, which API calls are allowed from console Resource Based Allow Cross Account Object Access Control List (Object ACL) is finer grain Bucket Access Control List (Bucket ACL) is less common A IAM principal allow access S3 if Any IAM permission allow Any Resource Policy allow No explicit deny in anywhere Supports VPC Endpoint , do it is possible to interact with S3 without public internet API Calls can be logged by Cloud Trail S3 Access Logs can be stored in another S3 Bucket User can use MFA to prevent unintended deletion Presigned URL can be used for premium content Presigned URL is generated by user credential token It is valid for limited time Bucket Policy JSON Based Policy Keys in the Policy Effect means is the action Allowed or Denied Principal means who is trying to access/taking-actions It can be User Resource Action means what type of action will be happen Can be READ Object GET Object (single/multiple) UPDATE Object DELETE Object Resource means on which Bucket or Object is targeted Hands On Object can not be uploaded if it is not SSE-S3 Effect should be Deny The Principal be * , since it is applicable for anyone Service is AWS S3 Action be S3 Put Object ARN should be s3://bucket_name/* We have to deny if it does not match two condition The header x-aws-server-side-encryption can not be null The header x-aws-server-side-encryption is not equal AES-256 Bucket Settings for Block Public Access Use to prevent company data leaks We can block public access to a bucket through (No need to remember the names) new ACL any ACL new Public Bucket Access Point Policies Allow cross account permission To allow a bucket in production account from the development account, On production account, create IAM role nd specify the development account as trusted entity Create a policy that will allow the created role to access S3 bucket In development account, using STS assume role create and attach the policy to the IAM user","title":"06 S3 Security And Bucket Policy"},{"location":"Notes/S3/06%20S3%20Security%20And%20Bucket%20Policy/#security","text":"User Based IAM policy determine, which API calls are allowed from console Resource Based Allow Cross Account Object Access Control List (Object ACL) is finer grain Bucket Access Control List (Bucket ACL) is less common A IAM principal allow access S3 if Any IAM permission allow Any Resource Policy allow No explicit deny in anywhere Supports VPC Endpoint , do it is possible to interact with S3 without public internet API Calls can be logged by Cloud Trail S3 Access Logs can be stored in another S3 Bucket User can use MFA to prevent unintended deletion Presigned URL can be used for premium content Presigned URL is generated by user credential token It is valid for limited time","title":"Security"},{"location":"Notes/S3/06%20S3%20Security%20And%20Bucket%20Policy/#bucket-policy","text":"JSON Based Policy Keys in the Policy Effect means is the action Allowed or Denied Principal means who is trying to access/taking-actions It can be User Resource Action means what type of action will be happen Can be READ Object GET Object (single/multiple) UPDATE Object DELETE Object Resource means on which Bucket or Object is targeted Hands On Object can not be uploaded if it is not SSE-S3 Effect should be Deny The Principal be * , since it is applicable for anyone Service is AWS S3 Action be S3 Put Object ARN should be s3://bucket_name/* We have to deny if it does not match two condition The header x-aws-server-side-encryption can not be null The header x-aws-server-side-encryption is not equal AES-256","title":"Bucket Policy"},{"location":"Notes/S3/06%20S3%20Security%20And%20Bucket%20Policy/#bucket-settings-for-block-public-access","text":"Use to prevent company data leaks We can block public access to a bucket through (No need to remember the names) new ACL any ACL new Public Bucket Access Point Policies","title":"Bucket Settings for Block Public Access"},{"location":"Notes/S3/06%20S3%20Security%20And%20Bucket%20Policy/#allow-cross-account-permission","text":"To allow a bucket in production account from the development account, On production account, create IAM role nd specify the development account as trusted entity Create a policy that will allow the created role to access S3 bucket In development account, using STS assume role create and attach the policy to the IAM user","title":"Allow cross account permission"},{"location":"Notes/S3/07%20S3%20Website/","text":"S3 Website S3 can be used to host static website To use S3 to host static website, we need to allow public read access Site URL can be <bucket-name>.s3-website.<AWS-region>.amazonaws.com <bucket-name>.s3-website-<AWS-region>.amazonaws.com Only difference is after s3-website Could be . Could be - Hands on Enable Static Site Hosting Upload file index.html and error.html Enable public access Add bucket policy to enable GET Object","title":"07 S3 Website"},{"location":"Notes/S3/07%20S3%20Website/#s3-website","text":"S3 can be used to host static website To use S3 to host static website, we need to allow public read access Site URL can be <bucket-name>.s3-website.<AWS-region>.amazonaws.com <bucket-name>.s3-website-<AWS-region>.amazonaws.com Only difference is after s3-website Could be . Could be - Hands on Enable Static Site Hosting Upload file index.html and error.html Enable public access Add bucket policy to enable GET Object","title":"S3 Website"},{"location":"Notes/S3/08%20S3%20CORS/","text":"S3 CORS Consider while using S3 for Static Website Hosting To allow cross-region request, we need to enable CORS in S3 We can define specific origin or any origin using * For AllowMethod , we can use the followings GET PUT POST DELETE HEAD To allow sites, http and https , we have to put 2 Allow Origin URL CORS Rules AllowedOrigin : Specify the origins, can make the cross domain request AllowedMethod : Specify the cross-domain request methods, GET , PUT , POST , DELETE , HEAD AllowedHeader : Specify the preflight allowed headers CORS Rules Element MaxAgeSeconds : Amount of time browser cache the response ExposeHeader : :TODO","title":"08 S3 CORS"},{"location":"Notes/S3/08%20S3%20CORS/#s3-cors","text":"Consider while using S3 for Static Website Hosting To allow cross-region request, we need to enable CORS in S3 We can define specific origin or any origin using * For AllowMethod , we can use the followings GET PUT POST DELETE HEAD To allow sites, http and https , we have to put 2 Allow Origin URL","title":"S3 CORS"},{"location":"Notes/S3/08%20S3%20CORS/#cors-rules","text":"AllowedOrigin : Specify the origins, can make the cross domain request AllowedMethod : Specify the cross-domain request methods, GET , PUT , POST , DELETE , HEAD AllowedHeader : Specify the preflight allowed headers","title":"CORS Rules"},{"location":"Notes/S3/08%20S3%20CORS/#cors-rules-element","text":"MaxAgeSeconds : Amount of time browser cache the response ExposeHeader : :TODO","title":"CORS Rules Element"},{"location":"Notes/S3/09%20S3%20Consistency%20Model/","text":"Consistency Model S3 is Eventual Consistency Model No way to get Strong Consistency Model in S3 We can read a object after write and after the success response If we read a object after write but before success response, we might not get the object for Eventual Consistency Eventual Consistency also applicable for DELETE and PUT object","title":"09 S3 Consistency Model"},{"location":"Notes/S3/09%20S3%20Consistency%20Model/#consistency-model","text":"S3 is Eventual Consistency Model No way to get Strong Consistency Model in S3 We can read a object after write and after the success response If we read a object after write but before success response, we might not get the object for Eventual Consistency Eventual Consistency also applicable for DELETE and PUT object","title":"Consistency Model"},{"location":"Notes/S3/10%20S3%20MFA%20Delete/","text":"MFA Delete To enable MFA Delete version in the Bucket must be enabled MFA Delete must be enable/disable from the Root Account MFA Delete must be enable/disable from the AWS CLI MFA Use for Permanently delete an object Suspend versioning","title":"10 S3 MFA Delete"},{"location":"Notes/S3/10%20S3%20MFA%20Delete/#mfa-delete","text":"To enable MFA Delete version in the Bucket must be enabled MFA Delete must be enable/disable from the Root Account MFA Delete must be enable/disable from the AWS CLI MFA Use for Permanently delete an object Suspend versioning","title":"MFA Delete"},{"location":"Notes/S3/11%20S3%20Access%20Logs/","text":"Access Logs Also known as Server Access Logs Logs may be required for audit purposes Access Logs log any authorized/deny request Logs can be analyzed using Athena Do not store logs in the same bucket (Think::: It will make the bucket grow exponentially) Includes Requester Bucket name Request time Request action Referrer Turnaround time Error code information Only the owner can have the object level access logs","title":"11 S3 Access Logs"},{"location":"Notes/S3/11%20S3%20Access%20Logs/#access-logs","text":"Also known as Server Access Logs Logs may be required for audit purposes Access Logs log any authorized/deny request Logs can be analyzed using Athena Do not store logs in the same bucket (Think::: It will make the bucket grow exponentially) Includes Requester Bucket name Request time Request action Referrer Turnaround time Error code information Only the owner can have the object level access logs","title":"Access Logs"},{"location":"Notes/S3/12%20S3%20Replication/","text":"S3 Replication S3 allows CRR (Cross-region-replication) Use for compliance Lower latency access Replication across accounts SRR (Same-region-replication) Use for Log aggregation (centralized multiple log buckets data) Replication between production, test and staging accounts When cross-region-replication is enabled, it does not replicate the existing objects, only the newly objects Can be replicated to a Bucket of another account Replication is asynchronous To enable replication Versioning must be enabled in both Source Bucket Destination Bucket Should have an proper IAM Policy Activating Replication does not replicate the existing objects Delete operation does not replicate No Chaining Replication happen Bucket 1 replicate to Bucket 2 Bucket 2 replicate to Bucket 3 This does not imply Bucket 1 replicate to Bucket 3 S3 lifecycle events does not replicate","title":"12 S3 Replication"},{"location":"Notes/S3/12%20S3%20Replication/#s3-replication","text":"S3 allows CRR (Cross-region-replication) Use for compliance Lower latency access Replication across accounts SRR (Same-region-replication) Use for Log aggregation (centralized multiple log buckets data) Replication between production, test and staging accounts When cross-region-replication is enabled, it does not replicate the existing objects, only the newly objects Can be replicated to a Bucket of another account Replication is asynchronous To enable replication Versioning must be enabled in both Source Bucket Destination Bucket Should have an proper IAM Policy Activating Replication does not replicate the existing objects Delete operation does not replicate No Chaining Replication happen Bucket 1 replicate to Bucket 2 Bucket 2 replicate to Bucket 3 This does not imply Bucket 1 replicate to Bucket 3 S3 lifecycle events does not replicate","title":"S3 Replication"},{"location":"Notes/S3/13%20S3%20Pre%20Signed%20Url/","text":"Pre Signed URL Use the permission of the person who generate the url Can generate url using From CLI, for download Using SDK, for upload Default timeout 1 hour Can update url validation by using --expires-in flag Use for Premium content in S3 Temporary user upload file in precise location Ever changing user list, download files by generating url dynamically","title":"13 S3 Pre Signed Url"},{"location":"Notes/S3/13%20S3%20Pre%20Signed%20Url/#pre-signed-url","text":"Use the permission of the person who generate the url Can generate url using From CLI, for download Using SDK, for upload Default timeout 1 hour Can update url validation by using --expires-in flag Use for Premium content in S3 Temporary user upload file in precise location Ever changing user list, download files by generating url dynamically","title":"Pre Signed URL"},{"location":"Notes/S3/14%20S3%20Storage%20Tiers/","text":"Storage Tiers 6 types of storage tiers S3 Standard - General Purpose (Use minimum 3 AZ) S3 Standard - Infrequent Access (IA) Use when data is accessed infrequently Use when rapid access is required S3 One Zone - Infrequent Access (Use Single AZ) S3 Intelligent Tiering Glacier Glacier Archive Also there is a deprecated class S3 Reduced Redundancy Storage S3 allows one bucket but multiple storage tier for different folder","title":"14 S3 Storage Tiers"},{"location":"Notes/S3/14%20S3%20Storage%20Tiers/#storage-tiers","text":"6 types of storage tiers S3 Standard - General Purpose (Use minimum 3 AZ) S3 Standard - Infrequent Access (IA) Use when data is accessed infrequently Use when rapid access is required S3 One Zone - Infrequent Access (Use Single AZ) S3 Intelligent Tiering Glacier Glacier Archive Also there is a deprecated class S3 Reduced Redundancy Storage S3 allows one bucket but multiple storage tier for different folder","title":"Storage Tiers"},{"location":"Notes/S3/15%20S3%20Lifecycle%20Policy/","text":"Lifecycle Policy We can change object storage class using the Lifecycle Policy Can not change storage class from Glacier or Glacier Archive To abort/delete in-complete multipart files after certain time period, lifecycle policy can be used Actions Transition Action Change the storage class after certain times Example Move object to Standard IA after 30 days of creation Archive object after 6 months Expiration Action Delete object after certain times Example Remove access logs after 365 days Removing in-completed multipart files Rules Rules can be applied to object prefix name Rules can be applied to object tags","title":"15 S3 Lifecycle Policy"},{"location":"Notes/S3/15%20S3%20Lifecycle%20Policy/#lifecycle-policy","text":"We can change object storage class using the Lifecycle Policy Can not change storage class from Glacier or Glacier Archive To abort/delete in-complete multipart files after certain time period, lifecycle policy can be used","title":"Lifecycle Policy"},{"location":"Notes/S3/15%20S3%20Lifecycle%20Policy/#actions","text":"Transition Action Change the storage class after certain times Example Move object to Standard IA after 30 days of creation Archive object after 6 months Expiration Action Delete object after certain times Example Remove access logs after 365 days Removing in-completed multipart files","title":"Actions"},{"location":"Notes/S3/15%20S3%20Lifecycle%20Policy/#rules","text":"Rules can be applied to object prefix name Rules can be applied to object tags","title":"Rules"},{"location":"Notes/S3/16%20S3%20Performance/","text":"S3 Performance Prefix Randomized prefix of s3 buckets improve the performance Each prefix has 3500 put request per sec 5500 get request per sec Baseline S3 prefix is the text between Bucket Name and File Name If object name is bucketName/folder1/folder2/object.jpeg , then the prefix is folder1/folder2 By default S3 is automatically scales at hight request rates For each prefix it is possible to 3500 PUT/COPY/POST/DELETE request 5500 GET/HEAD request There is no limit of prefix The more prefix we use, the more request we can made S3 KMS Limitation If we use SSE-KMS encryption, we need to consider there is Encrypt request to KMS while uploading a file Decrypt request to KMS while downloading a file There is a quota of Number Of KMS Request per second. Multi-Part Upload Recommended to use when file size size is more than 100MB Must use when file size is more than 5GB In this case, while uploading file Files divided in separate parts Upload these parts in parallel S3 join them after being uploaded S3 Transfer Acceleration Use for upload file First file uploaded to the nearest EDGE Location Then using AWS Private Network , goes to desired bucket Compatible with multi-part upload Byte Range Fetches Use for file fetches / download Parallelize Get by requesting specific byte ranges Can be used for retrieve only partial data Like header Response Code 503 when a new object is trying to update, while the same object has million of versions List Items Parameters --size-only : Used to determine if the local and cloud items are synced --exclude : Pass a pattern to exclude items --summary : Display number of retrieved items, total items etc --page-size : Default is 1000 page. If there are too many items, we can specify number of pages --max-items : Numbers of items to be displayed. If there are too many items, we can specify number items to be printed --starting-token : Previous results signature, so the next results will be shown When there are too many items, we can make use of --page-size and --max-items S3 Inventory When an S3 bucket has versioning enabled and a single object has millions of versions, it can throttling and throw 503 error. To determine these objects, can be used the S3 Inventory .","title":"16 S3 Performance"},{"location":"Notes/S3/16%20S3%20Performance/#s3-performance","text":"","title":"S3 Performance"},{"location":"Notes/S3/16%20S3%20Performance/#prefix","text":"Randomized prefix of s3 buckets improve the performance Each prefix has 3500 put request per sec 5500 get request per sec","title":"Prefix"},{"location":"Notes/S3/16%20S3%20Performance/#baseline","text":"S3 prefix is the text between Bucket Name and File Name If object name is bucketName/folder1/folder2/object.jpeg , then the prefix is folder1/folder2 By default S3 is automatically scales at hight request rates For each prefix it is possible to 3500 PUT/COPY/POST/DELETE request 5500 GET/HEAD request There is no limit of prefix The more prefix we use, the more request we can made","title":"Baseline"},{"location":"Notes/S3/16%20S3%20Performance/#s3-kms-limitation","text":"If we use SSE-KMS encryption, we need to consider there is Encrypt request to KMS while uploading a file Decrypt request to KMS while downloading a file There is a quota of Number Of KMS Request per second.","title":"S3 KMS Limitation"},{"location":"Notes/S3/16%20S3%20Performance/#multi-part-upload","text":"Recommended to use when file size size is more than 100MB Must use when file size is more than 5GB In this case, while uploading file Files divided in separate parts Upload these parts in parallel S3 join them after being uploaded","title":"Multi-Part Upload"},{"location":"Notes/S3/16%20S3%20Performance/#s3-transfer-acceleration","text":"Use for upload file First file uploaded to the nearest EDGE Location Then using AWS Private Network , goes to desired bucket Compatible with multi-part upload","title":"S3 Transfer Acceleration"},{"location":"Notes/S3/16%20S3%20Performance/#byte-range-fetches","text":"Use for file fetches / download Parallelize Get by requesting specific byte ranges Can be used for retrieve only partial data Like header","title":"Byte Range Fetches"},{"location":"Notes/S3/16%20S3%20Performance/#response-code","text":"503 when a new object is trying to update, while the same object has million of versions","title":"Response Code"},{"location":"Notes/S3/16%20S3%20Performance/#list-items-parameters","text":"--size-only : Used to determine if the local and cloud items are synced --exclude : Pass a pattern to exclude items --summary : Display number of retrieved items, total items etc --page-size : Default is 1000 page. If there are too many items, we can specify number of pages --max-items : Numbers of items to be displayed. If there are too many items, we can specify number items to be printed --starting-token : Previous results signature, so the next results will be shown When there are too many items, we can make use of --page-size and --max-items","title":"List Items Parameters"},{"location":"Notes/S3/16%20S3%20Performance/#s3-inventory","text":"When an S3 bucket has versioning enabled and a single object has millions of versions, it can throttling and throw 503 error. To determine these objects, can be used the S3 Inventory .","title":"S3 Inventory"},{"location":"Notes/S3/17%20S3%20Select%20and%20Glacier%20Select/","text":"S3 Select Server side filtering Can run simple SQL statements to filter Can filter by rows and columns Do not need to load all data to filter Example There is a CSV file in S3 Using S3 Select we can select certain rows and columns Fos S3 Select we can store files as CSV JSON For CSV and JSON files, supported compression is G-ZIP B-ZIP2 Glacier Select Same as S3 Select but happen in Glacier","title":"17 S3 Select and Glacier Select"},{"location":"Notes/S3/17%20S3%20Select%20and%20Glacier%20Select/#s3-select","text":"Server side filtering Can run simple SQL statements to filter Can filter by rows and columns Do not need to load all data to filter Example There is a CSV file in S3 Using S3 Select we can select certain rows and columns Fos S3 Select we can store files as CSV JSON For CSV and JSON files, supported compression is G-ZIP B-ZIP2","title":"S3 Select"},{"location":"Notes/S3/17%20S3%20Select%20and%20Glacier%20Select/#glacier-select","text":"Same as S3 Select but happen in Glacier","title":"Glacier Select"},{"location":"Notes/S3/18%20S3%20Athena/","text":"Athena Serverless service to run analytics in S3 directly Use SQL language to perform query Has JDBC / ODBC driver Built on Presto query engine To reduce cost of query Good to partition data Separate work groups based on users Benefits of using the parti -","title":"18 S3 Athena"},{"location":"Notes/S3/18%20S3%20Athena/#athena","text":"Serverless service to run analytics in S3 directly Use SQL language to perform query Has JDBC / ODBC driver Built on Presto query engine To reduce cost of query Good to partition data Separate work groups based on users Benefits of using the parti -","title":"Athena"},{"location":"Notes/S3/19%20S3%20Athena%20Vs%20Select/","text":"Athena Vs Select Both can run SQL. But S3 Select run very simple SQL Statements whereas Athena can run complex SQL Query Athena Athena used for data analytics Analytics is happening in directly S3 Serverless service S3 Select S3 Select is for Data Filtering Not-serverless, it's server-side filtering","title":"19 S3 Athena Vs Select"},{"location":"Notes/S3/19%20S3%20Athena%20Vs%20Select/#athena-vs-select","text":"Both can run SQL. But S3 Select run very simple SQL Statements whereas Athena can run complex SQL Query","title":"Athena Vs Select"},{"location":"Notes/S3/19%20S3%20Athena%20Vs%20Select/#athena","text":"Athena used for data analytics Analytics is happening in directly S3 Serverless service","title":"Athena"},{"location":"Notes/S3/19%20S3%20Athena%20Vs%20Select/#s3-select","text":"S3 Select is for Data Filtering Not-serverless, it's server-side filtering","title":"S3 Select"},{"location":"Notes/S3/20%20S3%20Event%20Notification/","text":"Event Notification Anything happen like GET/PUT/COPY/DELETE can raise an event Event can be passed to SNS SQS Lambda Function Versioning should be enabled to ensure the each events notification","title":"20 S3 Event Notification"},{"location":"Notes/S3/20%20S3%20Event%20Notification/#event-notification","text":"Anything happen like GET/PUT/COPY/DELETE can raise an event Event can be passed to SNS SQS Lambda Function Versioning should be enabled to ensure the each events notification","title":"Event Notification"},{"location":"Notes/S3/21%20S3%20Object%20Lock%20And%20Glacier%20Lock/","text":"S3 Object Lock WORM i.e. Write Once Read Many model Block a object version, no one can delete or modify Glacier Vault Lock WORM i.e. Write Once Read Many model Block a object, so no one can delete or modify Common Theory Lock can be applied on both Bucket Level Object Level Lock has two modes Governance Mode Users with special permission can override the rules and retention period Compliance Mode No one, even the root user can not override the rules and retention period.","title":"21 S3 Object Lock And Glacier Lock"},{"location":"Notes/S3/21%20S3%20Object%20Lock%20And%20Glacier%20Lock/#s3-object-lock","text":"WORM i.e. Write Once Read Many model Block a object version, no one can delete or modify","title":"S3 Object Lock"},{"location":"Notes/S3/21%20S3%20Object%20Lock%20And%20Glacier%20Lock/#glacier-vault-lock","text":"WORM i.e. Write Once Read Many model Block a object, so no one can delete or modify","title":"Glacier Vault Lock"},{"location":"Notes/S3/21%20S3%20Object%20Lock%20And%20Glacier%20Lock/#common-theory","text":"Lock can be applied on both Bucket Level Object Level Lock has two modes Governance Mode Users with special permission can override the rules and retention period Compliance Mode No one, even the root user can not override the rules and retention period.","title":"Common Theory"},{"location":"Notes/S3/22%20S3%20Protect%20Object%20Deletion/","text":"Protect Object Deletion Use MFA delete Use S3 Versioning Use S3 lock or Glacier Lock Use retention period When versioning is enabled and can be enabled different retention period for different version IAM and Bucket Policy","title":"22 S3 Protect Object Deletion"},{"location":"Notes/S3/22%20S3%20Protect%20Object%20Deletion/#protect-object-deletion","text":"Use MFA delete Use S3 Versioning Use S3 lock or Glacier Lock Use retention period When versioning is enabled and can be enabled different retention period for different version IAM and Bucket Policy","title":"Protect Object Deletion"},{"location":"Notes/S3/23%20S3%20Object%20ACL/","text":"Object ACL To get full access of a object of a bucket by owner needs ACL Policy of bucket-owner-full-control ACL policy can be used to manage buckets object level access","title":"23 S3 Object ACL"},{"location":"Notes/S3/23%20S3%20Object%20ACL/#object-acl","text":"To get full access of a object of a bucket by owner needs ACL Policy of bucket-owner-full-control ACL policy can be used to manage buckets object level access","title":"Object ACL"},{"location":"Notes/S3/24%20S3%20VPC%20Gateway/","text":"S3 VPC Gateway Need an Endpoint Policy Use this Endpoint Policy for Trusted S3 Bucket","title":"24 S3 VPC Gateway"},{"location":"Notes/S3/24%20S3%20VPC%20Gateway/#s3-vpc-gateway","text":"Need an Endpoint Policy Use this Endpoint Policy for Trusted S3 Bucket","title":"S3 VPC Gateway"},{"location":"Notes/S3/25%20S3%20Event%20Processing/","text":"Event Processing SQS + lambda If lambda fail to process, send it to DLQ FIFO SQS + lambda If lambda fail to process, send it to DLQ Fan Out Pattern Event first goes to the SNS SNS pass the event to multiple SQS or consumer Let's say, we have 3 SQS service, get same message from application If application crashed after send 2 SQS , the 3rd SQS does not get the message In this case, Fan Out came up Here application only send the message to SNS SNS pass the messages to the respective SQS S3 event can be passed to SQS SNS lambda","title":"25 S3 Event Processing"},{"location":"Notes/S3/25%20S3%20Event%20Processing/#event-processing","text":"SQS + lambda If lambda fail to process, send it to DLQ FIFO SQS + lambda If lambda fail to process, send it to DLQ Fan Out Pattern Event first goes to the SNS SNS pass the event to multiple SQS or consumer Let's say, we have 3 SQS service, get same message from application If application crashed after send 2 SQS , the 3rd SQS does not get the message In this case, Fan Out came up Here application only send the message to SNS SNS pass the messages to the respective SQS S3 event can be passed to SQS SNS lambda","title":"Event Processing"},{"location":"Notes/S3/26%20S3%20Permissions/","text":"S3 Permissions For object level permission use ACL To make the owner of the bucket to be the owner of all object, make the object ownership to the default bucket owner","title":"26 S3 Permissions"},{"location":"Notes/S3/26%20S3%20Permissions/#s3-permissions","text":"For object level permission use ACL To make the owner of the bucket to be the owner of all object, make the object ownership to the default bucket owner","title":"S3 Permissions"},{"location":"Notes/S3/27%20S3%20Object%20Lambda/","text":"S3 Object Lambda Allow modify data on-the-fly during data fetching Eliminate usage of additional storage or services","title":"27 S3 Object Lambda"},{"location":"Notes/S3/27%20S3%20Object%20Lambda/#s3-object-lambda","text":"Allow modify data on-the-fly during data fetching Eliminate usage of additional storage or services","title":"S3 Object Lambda"},{"location":"Notes/Serverless/01%20Lambda_deprecated/","text":"Lambda Virtual functions No needs to manage servers Run on-demand Automatic scaling Ram uses can be up to 3MB Increasing ram, improves the performance of CPU and Network Almost all programming language are supported by lambda Using Custom Runtime API it can support other languages Docker is not supported . Docker can run in the followings ECS Fargate Cloudwatch Event EventBridge can be used to run CRON job function that is in the Lambda Cloudwatch can be used to debug the code Lambda Limits Execution Memory allocation: 128MB to 3008MB (64MB increments) Max execution time: 15 minutes Env variable size: 4 KB Disk Capacity (/temp): 512MB Concurrent execution: 1000 (can be increased) Deployment Compressed deployment size: 50MB Uncompressed deployment size: 250MB Lambda@Edge Required when Deployed a CDN using Cloudfront Want to run Lambda Function alongside Make 4 types Requests Between User and Cloudfront Viewer Request Viewer Response Between Cloudfront and Origin Origin Request Origin Response Use-cases Web additional security Dynamic app at the Edge SEO Intelligent routing across Origin and Data Center Bot Mitigation at EDGE A/B Testing User Authentication and Authorization User Prioritization User tracking and analytics","title":"01 Lambda deprecated"},{"location":"Notes/Serverless/01%20Lambda_deprecated/#lambda","text":"Virtual functions No needs to manage servers Run on-demand Automatic scaling Ram uses can be up to 3MB Increasing ram, improves the performance of CPU and Network Almost all programming language are supported by lambda Using Custom Runtime API it can support other languages Docker is not supported . Docker can run in the followings ECS Fargate Cloudwatch Event EventBridge can be used to run CRON job function that is in the Lambda Cloudwatch can be used to debug the code","title":"Lambda"},{"location":"Notes/Serverless/01%20Lambda_deprecated/#lambda-limits","text":"Execution Memory allocation: 128MB to 3008MB (64MB increments) Max execution time: 15 minutes Env variable size: 4 KB Disk Capacity (/temp): 512MB Concurrent execution: 1000 (can be increased) Deployment Compressed deployment size: 50MB Uncompressed deployment size: 250MB","title":"Lambda Limits"},{"location":"Notes/Serverless/01%20Lambda_deprecated/#lambdaedge","text":"Required when Deployed a CDN using Cloudfront Want to run Lambda Function alongside Make 4 types Requests Between User and Cloudfront Viewer Request Viewer Response Between Cloudfront and Origin Origin Request Origin Response Use-cases Web additional security Dynamic app at the Edge SEO Intelligent routing across Origin and Data Center Bot Mitigation at EDGE A/B Testing User Authentication and Authorization User Prioritization User tracking and analytics","title":"Lambda@Edge"},{"location":"Notes/Serverless/02%20DynamoDB_deprecated/","text":"DynamoDB Managed database, replicated across 3 AZ NoSQL database Enable event driven programming using DynamoDB Streams In DynamoDB database is already created, only needs to create Table Each Table should have a primary key (Must be decided while creating the table) Can have infinite number of rows Max size of item can be 400KB Data types Scalar String Number Binary Boolean Null Document List Map Set String Set Number Set Binary Set To improve performance we can use DAX (mili seconds to micro seconds) Use partition keys of high cardinality, so large number of distinct values for each item DAX Stands for DynamoDB Accelerator Caching mechanism for DynamoDB DynamoDB Stream Raise event on Create , Update , Delete Can be used to trigger events on DB Changes Transaction All or nothing of operations Global Table Replicated in multiple region Needs to enable DynamoDB Stream Active Active Replication Changes in any region, impact all other regions Security Available in VPC Endpoints Fully controlled by IAM Encryption At rest by KMS In flight by SSL / TLS Provisioned Throughput 2 types of Provision Throughput RCU Read Capacity Unit Each RCU can handle one of the followings 1 Strongly Consistent read 4KB/s 2 Eventually Consistent read 4KB/s WCU Write Capacity Unit 1 write 1KB/s Can be used on-demand throughput (price is 2.5X more) Throughput can be exceeded using Burst Credit If Burst Credit is empty, it throws ProvisionThroughputException In case of ProvisionThroughputException , it is recommended to retry Can be used DynamoDB Auto Scaling No need to provision throughput Comparatively expensive","title":"02 DynamoDB deprecated"},{"location":"Notes/Serverless/02%20DynamoDB_deprecated/#dynamodb","text":"Managed database, replicated across 3 AZ NoSQL database Enable event driven programming using DynamoDB Streams In DynamoDB database is already created, only needs to create Table Each Table should have a primary key (Must be decided while creating the table) Can have infinite number of rows Max size of item can be 400KB Data types Scalar String Number Binary Boolean Null Document List Map Set String Set Number Set Binary Set To improve performance we can use DAX (mili seconds to micro seconds) Use partition keys of high cardinality, so large number of distinct values for each item DAX Stands for DynamoDB Accelerator Caching mechanism for DynamoDB DynamoDB Stream Raise event on Create , Update , Delete Can be used to trigger events on DB Changes Transaction All or nothing of operations Global Table Replicated in multiple region Needs to enable DynamoDB Stream Active Active Replication Changes in any region, impact all other regions Security Available in VPC Endpoints Fully controlled by IAM Encryption At rest by KMS In flight by SSL / TLS","title":"DynamoDB"},{"location":"Notes/Serverless/02%20DynamoDB_deprecated/#provisioned-throughput","text":"2 types of Provision Throughput RCU Read Capacity Unit Each RCU can handle one of the followings 1 Strongly Consistent read 4KB/s 2 Eventually Consistent read 4KB/s WCU Write Capacity Unit 1 write 1KB/s Can be used on-demand throughput (price is 2.5X more) Throughput can be exceeded using Burst Credit If Burst Credit is empty, it throws ProvisionThroughputException In case of ProvisionThroughputException , it is recommended to retry Can be used DynamoDB Auto Scaling No need to provision throughput Comparatively expensive","title":"Provisioned Throughput"},{"location":"Notes/Serverless/03%20API%20Gateway_deprecated/","text":"API Gateway Support web-socket protocol Handle API versioning Multiple Environment Security (Authentication, Authorization) Using API keys, handle request throattling Swagger / Open API to import Definition Transform and validate the Request and Response Generate SDK and API Specification Cache API response Integration Lambda Invoke Lambda function Expose REST API backed by Lambda HTTP Endpoint AWS Service Expose any AWS Service as API Gateway Endpoint Types 3 types of API Gateway Endpoints Edge Optimized This is default behavior API is only one region But to improve latency, request is routed through Cloudfront Edge Locations Regional API is in the one region With combination of Cloudfront We can get Edge Optimized behavior In this case, we have more control over Caching Strategies Distribution Private Use inside the VPC as VPC Endpoint Resource policy is used to define access Security IAM When users/roles is within AWS Account Handle Authentication and Authorization Leverage Sig v4 It's the IAM credentials in the HTTP Header Custom Authorizer or Lambda Authorizer When users are from 3rd party Lambda Authorized can be cached CUP or Cognito User Pool When user pools are manages by Facebook, Google login No need to write custom code Only provide Authentication Authorization must be provided from the backend code Access of developer and users can be separated using IAM Permission Developer can manage and deploy API User can call API SSL/TLS though AWS Certificate Manager is free for API Gateway","title":"03 API Gateway deprecated"},{"location":"Notes/Serverless/03%20API%20Gateway_deprecated/#api-gateway","text":"Support web-socket protocol Handle API versioning Multiple Environment Security (Authentication, Authorization) Using API keys, handle request throattling Swagger / Open API to import Definition Transform and validate the Request and Response Generate SDK and API Specification Cache API response","title":"API Gateway"},{"location":"Notes/Serverless/03%20API%20Gateway_deprecated/#integration","text":"Lambda Invoke Lambda function Expose REST API backed by Lambda HTTP Endpoint AWS Service Expose any AWS Service as API Gateway","title":"Integration"},{"location":"Notes/Serverless/03%20API%20Gateway_deprecated/#endpoint-types","text":"3 types of API Gateway Endpoints Edge Optimized This is default behavior API is only one region But to improve latency, request is routed through Cloudfront Edge Locations Regional API is in the one region With combination of Cloudfront We can get Edge Optimized behavior In this case, we have more control over Caching Strategies Distribution Private Use inside the VPC as VPC Endpoint Resource policy is used to define access","title":"Endpoint Types"},{"location":"Notes/Serverless/03%20API%20Gateway_deprecated/#security","text":"IAM When users/roles is within AWS Account Handle Authentication and Authorization Leverage Sig v4 It's the IAM credentials in the HTTP Header Custom Authorizer or Lambda Authorizer When users are from 3rd party Lambda Authorized can be cached CUP or Cognito User Pool When user pools are manages by Facebook, Google login No need to write custom code Only provide Authentication Authorization must be provided from the backend code Access of developer and users can be separated using IAM Permission Developer can manage and deploy API User can call API SSL/TLS though AWS Certificate Manager is free for API Gateway","title":"Security"},{"location":"Notes/Serverless/04%20AWS%20Cognito/","text":"AWS Cognito 3 tools to discuss Cognito User Pools Users are stored as username and password in cognito Sign In functionality Integrate with api gateway Allow tracking user device, ip, locations etc. Cognito Identity Pools Users included 3rd party provides, like SAML, facebook, google, etc Provide AWS Credentials to access AWS Resource directly Integrate with CUP as Identity Provider Cognito Sync Sync data to multiple device Deprecated, using AppSync instead User pools are list of user with credentials Identity pools are users from 3rd party Cognito User Pool and Identity Pool are independent with each other Has Guest User facility, so users can access limited resource without authentication Cognito Supports OIDC (Open ID Connect) SAML based identity providers (SAML, LDAP, Microsoft AD) Social identity providers To get data insight of the cognito, used the Cognito Streams To put data in redshift, we can make use of both the new kinesis stream or the cognito stream CUP Serverless database of users for mobile Verification through email/phone/MFA Can enable Federated Identity (Google, Facebook, SAML) -> Becomes Identity Pools from User Pools Verify user credentials and pass JWT Can be integrated with API Gateway or Application Load Balancer for authentication natively Can trigger lambda function in different life cycle Federated Identity Pools (aka Cognito Identity Pools) Supports Pulic providers CUP Open ID or SAML compatible providers Developer Authenticated Identities (Custom Login Server) Provide direct access to AWS Resource from client side Steps Identity Provider generate token for valid user(could be CUP ) Federated Identity Verify the token Using STS generate temporary credentials for the APP App can use these credentials and access AWS Resource To allow access of not authenticated user, allows, guest access Developer Authenticated Identities AWS Cognito provides Developer Authenticated Identity It works along with 3rd party identity provider like facebook, google etc With Developer Authenticated Identity AWS can sync the user resource of a particular users resource Can be used to sync between end user device and backend Steps To Use CUP Create CUP In API Gateway, create authorized for cognito user pool id Send token in the header to authorize the request UI Cognito provide a built in login and sign up page for both CUP and Identity pool We can customize the built in pages We can change the logo by going through the cognito app settings For using custom domain Should be https connection Need ACM certificates from us-east-1 [No other options] Custom domain should be included in the app-integration section Compromised Credentials Amazon Cognito can determine whether the password has been compromised. We can set the settings block use from the advanced section. Also, we can determine the actions like sign in , sign up and password change . Using risk factor, low, high or medium, it can invoke using the MFA for the user To enforce additional security only for the suspicious authentication , can be used Adaptive Authentication ALB Integration Listener should be HTTPs For not authenticated users, Move to authenticate route (Default Behavior) Deny Allow For OIDC, the verification process is different [TODO: 410] ALB redirect user to authentication endpoint , it provides grant code Then token endpoint provides id token and access token using grant code Later, user info endpoint gives user claims using the access token Overall flow, Authentication Endpoint: Generate grant code Token Endpoint: Generate id + access toke User Info Endpoint: Generate user claims User Pools vs Identity Pools [TODO: 413]","title":"04 AWS Cognito"},{"location":"Notes/Serverless/04%20AWS%20Cognito/#aws-cognito","text":"3 tools to discuss Cognito User Pools Users are stored as username and password in cognito Sign In functionality Integrate with api gateway Allow tracking user device, ip, locations etc. Cognito Identity Pools Users included 3rd party provides, like SAML, facebook, google, etc Provide AWS Credentials to access AWS Resource directly Integrate with CUP as Identity Provider Cognito Sync Sync data to multiple device Deprecated, using AppSync instead User pools are list of user with credentials Identity pools are users from 3rd party Cognito User Pool and Identity Pool are independent with each other Has Guest User facility, so users can access limited resource without authentication Cognito Supports OIDC (Open ID Connect) SAML based identity providers (SAML, LDAP, Microsoft AD) Social identity providers To get data insight of the cognito, used the Cognito Streams To put data in redshift, we can make use of both the new kinesis stream or the cognito stream","title":"AWS Cognito"},{"location":"Notes/Serverless/04%20AWS%20Cognito/#cup","text":"Serverless database of users for mobile Verification through email/phone/MFA Can enable Federated Identity (Google, Facebook, SAML) -> Becomes Identity Pools from User Pools Verify user credentials and pass JWT Can be integrated with API Gateway or Application Load Balancer for authentication natively Can trigger lambda function in different life cycle","title":"CUP"},{"location":"Notes/Serverless/04%20AWS%20Cognito/#federated-identity-pools-aka-cognito-identity-pools","text":"Supports Pulic providers CUP Open ID or SAML compatible providers Developer Authenticated Identities (Custom Login Server) Provide direct access to AWS Resource from client side Steps Identity Provider generate token for valid user(could be CUP ) Federated Identity Verify the token Using STS generate temporary credentials for the APP App can use these credentials and access AWS Resource To allow access of not authenticated user, allows, guest access","title":"Federated Identity Pools (aka Cognito Identity Pools)"},{"location":"Notes/Serverless/04%20AWS%20Cognito/#developer-authenticated-identities","text":"AWS Cognito provides Developer Authenticated Identity It works along with 3rd party identity provider like facebook, google etc With Developer Authenticated Identity AWS can sync the user resource of a particular users resource Can be used to sync between end user device and backend","title":"Developer Authenticated Identities"},{"location":"Notes/Serverless/04%20AWS%20Cognito/#steps-to-use-cup","text":"Create CUP In API Gateway, create authorized for cognito user pool id Send token in the header to authorize the request","title":"Steps To Use CUP"},{"location":"Notes/Serverless/04%20AWS%20Cognito/#ui","text":"Cognito provide a built in login and sign up page for both CUP and Identity pool We can customize the built in pages We can change the logo by going through the cognito app settings For using custom domain Should be https connection Need ACM certificates from us-east-1 [No other options] Custom domain should be included in the app-integration section","title":"UI"},{"location":"Notes/Serverless/04%20AWS%20Cognito/#compromised-credentials","text":"Amazon Cognito can determine whether the password has been compromised. We can set the settings block use from the advanced section. Also, we can determine the actions like sign in , sign up and password change . Using risk factor, low, high or medium, it can invoke using the MFA for the user To enforce additional security only for the suspicious authentication , can be used Adaptive Authentication","title":"Compromised Credentials"},{"location":"Notes/Serverless/04%20AWS%20Cognito/#alb-integration","text":"Listener should be HTTPs For not authenticated users, Move to authenticate route (Default Behavior) Deny Allow For OIDC, the verification process is different [TODO: 410] ALB redirect user to authentication endpoint , it provides grant code Then token endpoint provides id token and access token using grant code Later, user info endpoint gives user claims using the access token Overall flow, Authentication Endpoint: Generate grant code Token Endpoint: Generate id + access toke User Info Endpoint: Generate user claims","title":"ALB Integration"},{"location":"Notes/Serverless/04%20AWS%20Cognito/#user-pools-vs-identity-pools-todo-413","text":"","title":"User Pools vs Identity Pools [TODO: 413]"},{"location":"Notes/Serverless/05%20SAM_legacy/","text":"SAM Serverless Application Model Framework for developing and deploying serverless applications All config is in the YAML formal Lambda function DynamoDB table API Gateway Cognito User Pool SAM help to run followings locally API Gateway DynamoDB Table SAM help to integrate Code Deploy , so Lambda Functions can deploy easily Allow test the Lambda function locally Can invoke function and events locally SAM Templates can be used to test the app through before deploy Using SAM Built In Code Deploy , application can be deployed to the cloud","title":"05 SAM legacy"},{"location":"Notes/Serverless/05%20SAM_legacy/#sam","text":"Serverless Application Model Framework for developing and deploying serverless applications All config is in the YAML formal Lambda function DynamoDB table API Gateway Cognito User Pool SAM help to run followings locally API Gateway DynamoDB Table SAM help to integrate Code Deploy , so Lambda Functions can deploy easily Allow test the Lambda function locally Can invoke function and events locally SAM Templates can be used to test the app through before deploy","title":"SAM"},{"location":"Notes/Serverless/05%20SAM_legacy/#using-sam-built-in-code-deploy-application-can-be-deployed-to-the-cloud","text":"","title":"Using SAM Built In Code Deploy, application can be deployed to the cloud"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/","text":"Lambda Virtual functions No needs to manage servers Run on-demand Automatic scaling Ram uses can be up to 10GB Increasing ram, improves the performance of CPU and Network Almost all programming language are supported by lambda Using Custom Runtime API it can support other languages Docker is not supported . Docker can run in the followings ECS Fargate Cloudwatch Event EventBridge can be used to run CRON job function that is in the Lambda Cloudwatch can be used to debug the code Process events using Asynchronus Synchronus Event source mapping (Synchronusly) Runtime Lambda has native support of the following runtimes, Node.js Python Ruby Java Go .NET We can provide our own custom runtime by Include runtime in function deployment package named bootstrap These runtime should be resided in new lambda layer For Lambda Container Image, we must include the Lambda Runtime API in the container image Unless Lambda Runtime API is implemented, the docker container should run in ECS or Fargate Does not support multi-architecture container image Lambda Limits Execution Memory allocation: 128MB to 10GB (1MB increments) Increasing RAM also increase the CPU and Network Max execution time: 15 minutes (default is 3 seconds) Env variable size: 4 KB Disk Capacity (/tmp): 10GB Concurrent execution: 1000 (can be increased) When we reserve we have to consider 100 for there functions, so usable is 900 Deployment Compressed deployment size: 50MB Uncompressed deployment size: 250MB Synchronus Invocation When the lambda function directly invoked and return the results right away These do the synchronus invocation SDK CLI API Gateway ALB S3 Batch (?) Cognito Step Function In these cases, if a error is occoured, should be handled in the client side ALB to Lambda ALB turns the HTTP request to JSON and pass to the lambda and also convert the JSON results of the lambda to HTTP response. Enabling Multi Header Value will turn multiple query paramers or query string into array In this case (Using lambda with ALB), there should be a target group to point the lambda function The ALB security group should have the permission to execute (invoke) the lambda function In this case, the ALB act as a trigger of the lambda function Asynchronus invocation Happnes through event queue [Origin can be S3, SNS, Cloudwatch Event] Retry lambda on errors (3 times) 1st time invoked and encounter errors 2nd one just after 1 minute 3rd one after 2 minutes wait DLQ (Dead letter queue can be used to put the error events) Services uses the asynchronus invocation S3 event notification SNS Cloudwatch events / event brdige CodeCommit/CodePipeline Simple Email Service Event Source Mapping Important Event source mapping poll data and return results As source, it can use Streams [Kinesis and DynamoDB] Queue [SQS] Streams For low traffic, use batch before processing For high traffic, multiple batches can be processed in parallel If there's an error, entire process will be reprocessed or expired eventually To ensure in-order processing, processing of the affected shard is paused untill the error is resolved Can be configured as, Discard old events Restrict number of retries Split batch on error (Resolve lambda timeout issue) Discarded events can go to destination Queue Use long polling Batch size can be specified (1 - 10) Recommandation timeout for the queue is 6x compare to the lambda function timeout DLQ should be set up in the SQS, not in the lambda (DLQ for lambda is only for async invocation and this is a synchronus operation ???) For FIFO queue, the processing will be in-order Occasionally, the event-mapping might recieve same item from queue twice If a batch returned due to error, the messages will return as individual message and also will be proceed in different group Lambda delete the items after being proceed DLQ can be configured if the event is not processed Lambda@Edge Deployed globally Required when Deployed a CDN using Cloudfront Want to run Lambda Function alongside Make 4 types Requests Between User and Cloudfront Viewer Request Viewer Response Between Cloudfront and Origin Origin Request Origin Response Use-cases Web additional security Dynamic app at the Edge SEO Intelligent routing across Origin and Data Center Bot Mitigation at EDGE [Detect bot approach] A/B Testing User Authentication and Authorization User Prioritization User tracking and analytics CloudFront Functions vs Labmda@Edge Provided by CloudFront CloudFront Functions are in JS while the Lambda@Edge can be Node.js or Python CloudFront only manipulate viewer request and response while the Lambda@Edge can manipulate both viewer and origin request/response CloudFront can scale for millions of request while Lambda@Edge scale for thousands of request Lambda@Edge has longer execution time than CloudFront function Lambda At VPC By default Lambda functions are deployed in a AWS owned VPC It can access external services but restricted to access the resources inside a VPC created by the user in private subnet In order to access the resource in the VPC, Deploy the lambda function in the subnet Define VPC ID, subnet and Security group in the Lambda Lambda will create an ENI Lambda will need AWSLambdaVPCAccessExecutionRole In the VPC, the resources (Like RDS, ElasticCache, ELB etc) security group should allow Lambda security group By default, the Lambda deployed in a subnet (even though the subnet is public), does not have the internet access To manage access the internet from a Lambda, that is deployed in the subnet, need to use the NAT Gateway or NAT Instance (this will talk with the Internet Gatewayh) However a Lambda, that is deployed in the subnet, can access other AWS services using the VPC endpoint. One exception is CloudWatch logs, that will work without any NAT instace / NAT Gateway / VPC Endpoint. Execution Context A temporary runtime, can be used to Establish database connection Set up HTTP Clients / SDK clients Exist after a function is done execution so can be used by concurrent other lambda function The /tmp is created to read/write some files temporary Code thoese are outside the handler function are available in the execution context Error Types During Deployment InvalidParameterValueException : Invalid request parameters. can be permission error. CodeStorageExceededException : Exceed the total code size (compressed 50MB, un-compressed 250MB) ResourceConflictException : Already a function exists ServiceException : Internal server error Limits Concurrency : Calculated by, concurrency = ( number of invocation per second * number of seconds per invocation took ). BY default lambda has 500 to 3000 concurrency vary from region. With burst capacity, we can exceed it another 500 concurrency. For more concurrency, need to make a request to increase the concurrency to aws. Concurrency limit is calculated by whole account. If the account has limit of 1000, aws will reserve 100 and other 900 can be used. We can distribute all these 900 For kinesis stream shard, if the lambda function process the message, then concurrency means number of shards Event vs Context Object Event Contains data, will be proceed by the event Contains information of the invoking service Convert event data to object (for python dict, for JS json) Context Properties of the method, like runtimes, memory limit, function name etc Passed by lambda during runtime Destinations Synchronous Invocation Destination is client Asynchronous Invocation DLQ for only error or failed processing New Destination [Recommanded] for both success or failed processing SQS SNS Lambda EventBridge Bus Event Source Mapping DLQ for failed process New Destination [Recommanded] for both success or failed processing SQS SNS Tracing with X-Ray Enabling by \"Active Tracing\" Environment variables X_AMZN_TRACE_ID contains tracing header AWS_XRAY_CONTEXT_MISSING log error by default (?) AWS_XRAY_DAEMON_ADDRESS contains x-ray daemon ip address and port Lambda Layers A zip archive, contains runtime or library Using layer we can define a custom runtime for a programming language that is not available in AWS Lambda by default Big dependencies can be placed in the layer, so everytime we chage the function and upload the zip file, we do not have to upload all these dependencies Lambda File System Mounting To access the EFS file system, The function should be running in the private subnet A EFS access point should be used to access the file system One function instance is one connection for the lambda, for a lot of functions, there could be a burst limit Cocurrency Can be set in the function level In AWS the concurrency can be changed through the support ticket Once a function is throttled, it will show the throtlle error for all the concurrent functions When throttling Fon synchronus invocation, it will throw 429 (ThrottleError) For asynchronus invocation, it will retry and eventually end up in the DLQ Cold Start When first time a lambda function is invoked, it will take some times to execute the codes outside the handler function like db connection, http connection setup etc. To reduce the cold start time Increase memory allocation Reduce deployment package size Move operations like db connections, outside the function Provisioned Concurrency - can be implemented so a certain function will always run and can server the initial requests Function Dependencies Before deployin, external libraries (Like AWS X-Ray SDK, DB Clients, projectspecific modules etc.) need to be packaged in zip file If the zip file size is more than 50 MB, then it first need to be uploaded in the S3 and then use the reference For native libraries, first need to compiled in Amazon linux AWS SDK is already integrated with Lambda by default Deploy Using Cloudformation We can add codes directly to the cloudformation template, but with this inline including, we can not add any dependency Code can be stored as zip file in S3 and referenced from the cloudforation template. This require include the S3 bucket name S3 buckets object full path Version code [if enabled] If the code is updeted in the S3 but the S3 version is not updated in the cloudformation, then the new lambda function will not be included When the S3 bucket (where the zipped code is located) and cloudformation is in different account, Add an execution role in the cloudformtaion allowing fetch and list the S3 bucket Update bucket policy to allow the cloudformation owner account Lambda Containerization Lambda function container should be deployed through ECR and size can be up to GB The base image must include the Lambda Runtime API (Recommanded to use AWS provided image) Containers can be tested locally using the Lambda Runtime Interface Emulator Optimization Use AWS provided base image, since it is cached Keep the less changing commands on the top of the docker file Using single repository helps ECR to compare layers Versioning and Alias Versioning Everytime we make a update to the code/configuration of a lambda function, a new version of the lambda function created. When we invoke the function, it usually invoke the latest version of the function, Although we can invoke any previous/specific version of the lambda function. Each version of the lambda function will have their own AWS ARN. Alias When we want to point a specific version of a lambda fuction, we can make use of the Alias. Alias can be dev, test, prod, rc etc. By these alias, we can also implement the blue green deployment. Like an alias can send traffic to different versions with specific percentage or weight. Others Alias can only point to different version/versions of lambda function. It can not point to another alias. $latest version is mutable, it always point to the updated version of the lambda function. Other versions are immutable. If we update any code/configuration, the lambda function versions will be updated. Lambda And CodeDeploy Allow automating traffic shifting Very much integrated with SAM 3 types of traffic shifting Linear : Traffic gradually move with times untill completely migrate all traffic Linear10PercentEvery3Minutes Linear10PercentEvery10Minutes Canary : First mirgrate a certain proportion of traffic and later migrate all traffic Canary10Percent5Minutes: Move 10 percent for 5 minutes and later 100% goes to target function Canary10Percent30Minutes: Move 10 percent for 30 minutes and later 100% goes to new function All At Once : Immediately move all the traffic to target function To setup code deploy, required Function Name Function Alias Current Function Version Target Function Version Lambda Function URL TBD: 310 & 311 Lambda And CodeGuru Provide runtime performance of the lambda function Support Java and Python runtimes Best Practices Connect db outside the handler Initialize AWS SDK outside the handler Pull dependencies outside of the function handler Use env variables for db strings, s3 url etc For sensetive values like password, use the KMS for encryption Never ever call a Lambda function recursively Gotcha Environment Variables : Regular application environment variables Stage Variables : Related to API Gateway, can be dev , prod , v1 , v2 etc. Also these stage variables acn be mapped with the alias of lambda function Aliases : Pointer to specific lambda version Misc To do encryption in /tmp need to use KMS, lambda does not handle by default Stage variables are for API gateway while aliases are for lambda function","title":"01 Lambda Overview"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#lambda","text":"Virtual functions No needs to manage servers Run on-demand Automatic scaling Ram uses can be up to 10GB Increasing ram, improves the performance of CPU and Network Almost all programming language are supported by lambda Using Custom Runtime API it can support other languages Docker is not supported . Docker can run in the followings ECS Fargate Cloudwatch Event EventBridge can be used to run CRON job function that is in the Lambda Cloudwatch can be used to debug the code Process events using Asynchronus Synchronus Event source mapping (Synchronusly)","title":"Lambda"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#runtime","text":"Lambda has native support of the following runtimes, Node.js Python Ruby Java Go .NET We can provide our own custom runtime by Include runtime in function deployment package named bootstrap These runtime should be resided in new lambda layer For Lambda Container Image, we must include the Lambda Runtime API in the container image Unless Lambda Runtime API is implemented, the docker container should run in ECS or Fargate Does not support multi-architecture container image","title":"Runtime"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#lambda-limits","text":"Execution Memory allocation: 128MB to 10GB (1MB increments) Increasing RAM also increase the CPU and Network Max execution time: 15 minutes (default is 3 seconds) Env variable size: 4 KB Disk Capacity (/tmp): 10GB Concurrent execution: 1000 (can be increased) When we reserve we have to consider 100 for there functions, so usable is 900 Deployment Compressed deployment size: 50MB Uncompressed deployment size: 250MB","title":"Lambda Limits"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#synchronus-invocation","text":"When the lambda function directly invoked and return the results right away These do the synchronus invocation SDK CLI API Gateway ALB S3 Batch (?) Cognito Step Function In these cases, if a error is occoured, should be handled in the client side","title":"Synchronus Invocation"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#alb-to-lambda","text":"ALB turns the HTTP request to JSON and pass to the lambda and also convert the JSON results of the lambda to HTTP response. Enabling Multi Header Value will turn multiple query paramers or query string into array In this case (Using lambda with ALB), there should be a target group to point the lambda function The ALB security group should have the permission to execute (invoke) the lambda function In this case, the ALB act as a trigger of the lambda function","title":"ALB to Lambda"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#asynchronus-invocation","text":"Happnes through event queue [Origin can be S3, SNS, Cloudwatch Event] Retry lambda on errors (3 times) 1st time invoked and encounter errors 2nd one just after 1 minute 3rd one after 2 minutes wait DLQ (Dead letter queue can be used to put the error events) Services uses the asynchronus invocation S3 event notification SNS Cloudwatch events / event brdige CodeCommit/CodePipeline Simple Email Service","title":"Asynchronus invocation"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#event-source-mapping","text":"Important Event source mapping poll data and return results As source, it can use Streams [Kinesis and DynamoDB] Queue [SQS] Streams For low traffic, use batch before processing For high traffic, multiple batches can be processed in parallel If there's an error, entire process will be reprocessed or expired eventually To ensure in-order processing, processing of the affected shard is paused untill the error is resolved Can be configured as, Discard old events Restrict number of retries Split batch on error (Resolve lambda timeout issue) Discarded events can go to destination Queue Use long polling Batch size can be specified (1 - 10) Recommandation timeout for the queue is 6x compare to the lambda function timeout DLQ should be set up in the SQS, not in the lambda (DLQ for lambda is only for async invocation and this is a synchronus operation ???) For FIFO queue, the processing will be in-order Occasionally, the event-mapping might recieve same item from queue twice If a batch returned due to error, the messages will return as individual message and also will be proceed in different group Lambda delete the items after being proceed DLQ can be configured if the event is not processed","title":"Event Source Mapping"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#lambdaedge","text":"Deployed globally Required when Deployed a CDN using Cloudfront Want to run Lambda Function alongside Make 4 types Requests Between User and Cloudfront Viewer Request Viewer Response Between Cloudfront and Origin Origin Request Origin Response Use-cases Web additional security Dynamic app at the Edge SEO Intelligent routing across Origin and Data Center Bot Mitigation at EDGE [Detect bot approach] A/B Testing User Authentication and Authorization User Prioritization User tracking and analytics CloudFront Functions vs Labmda@Edge Provided by CloudFront CloudFront Functions are in JS while the Lambda@Edge can be Node.js or Python CloudFront only manipulate viewer request and response while the Lambda@Edge can manipulate both viewer and origin request/response CloudFront can scale for millions of request while Lambda@Edge scale for thousands of request Lambda@Edge has longer execution time than CloudFront function","title":"Lambda@Edge"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#lambda-at-vpc","text":"By default Lambda functions are deployed in a AWS owned VPC It can access external services but restricted to access the resources inside a VPC created by the user in private subnet In order to access the resource in the VPC, Deploy the lambda function in the subnet Define VPC ID, subnet and Security group in the Lambda Lambda will create an ENI Lambda will need AWSLambdaVPCAccessExecutionRole In the VPC, the resources (Like RDS, ElasticCache, ELB etc) security group should allow Lambda security group By default, the Lambda deployed in a subnet (even though the subnet is public), does not have the internet access To manage access the internet from a Lambda, that is deployed in the subnet, need to use the NAT Gateway or NAT Instance (this will talk with the Internet Gatewayh) However a Lambda, that is deployed in the subnet, can access other AWS services using the VPC endpoint. One exception is CloudWatch logs, that will work without any NAT instace / NAT Gateway / VPC Endpoint.","title":"Lambda At VPC"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#execution-context","text":"A temporary runtime, can be used to Establish database connection Set up HTTP Clients / SDK clients Exist after a function is done execution so can be used by concurrent other lambda function The /tmp is created to read/write some files temporary Code thoese are outside the handler function are available in the execution context","title":"Execution Context"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#error-types-during-deployment","text":"InvalidParameterValueException : Invalid request parameters. can be permission error. CodeStorageExceededException : Exceed the total code size (compressed 50MB, un-compressed 250MB) ResourceConflictException : Already a function exists ServiceException : Internal server error","title":"Error Types During Deployment"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#limits","text":"Concurrency : Calculated by, concurrency = ( number of invocation per second * number of seconds per invocation took ). BY default lambda has 500 to 3000 concurrency vary from region. With burst capacity, we can exceed it another 500 concurrency. For more concurrency, need to make a request to increase the concurrency to aws. Concurrency limit is calculated by whole account. If the account has limit of 1000, aws will reserve 100 and other 900 can be used. We can distribute all these 900 For kinesis stream shard, if the lambda function process the message, then concurrency means number of shards","title":"Limits"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#event-vs-context-object","text":"Event Contains data, will be proceed by the event Contains information of the invoking service Convert event data to object (for python dict, for JS json) Context Properties of the method, like runtimes, memory limit, function name etc Passed by lambda during runtime","title":"Event vs Context Object"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#destinations","text":"Synchronous Invocation Destination is client Asynchronous Invocation DLQ for only error or failed processing New Destination [Recommanded] for both success or failed processing SQS SNS Lambda EventBridge Bus Event Source Mapping DLQ for failed process New Destination [Recommanded] for both success or failed processing SQS SNS","title":"Destinations"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#tracing-with-x-ray","text":"Enabling by \"Active Tracing\" Environment variables X_AMZN_TRACE_ID contains tracing header AWS_XRAY_CONTEXT_MISSING log error by default (?) AWS_XRAY_DAEMON_ADDRESS contains x-ray daemon ip address and port","title":"Tracing with X-Ray"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#lambda-layers","text":"A zip archive, contains runtime or library Using layer we can define a custom runtime for a programming language that is not available in AWS Lambda by default Big dependencies can be placed in the layer, so everytime we chage the function and upload the zip file, we do not have to upload all these dependencies","title":"Lambda Layers"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#lambda-file-system-mounting","text":"To access the EFS file system, The function should be running in the private subnet A EFS access point should be used to access the file system One function instance is one connection for the lambda, for a lot of functions, there could be a burst limit","title":"Lambda File System Mounting"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#cocurrency","text":"Can be set in the function level In AWS the concurrency can be changed through the support ticket Once a function is throttled, it will show the throtlle error for all the concurrent functions When throttling Fon synchronus invocation, it will throw 429 (ThrottleError) For asynchronus invocation, it will retry and eventually end up in the DLQ Cold Start When first time a lambda function is invoked, it will take some times to execute the codes outside the handler function like db connection, http connection setup etc. To reduce the cold start time Increase memory allocation Reduce deployment package size Move operations like db connections, outside the function Provisioned Concurrency - can be implemented so a certain function will always run and can server the initial requests","title":"Cocurrency"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#function-dependencies","text":"Before deployin, external libraries (Like AWS X-Ray SDK, DB Clients, projectspecific modules etc.) need to be packaged in zip file If the zip file size is more than 50 MB, then it first need to be uploaded in the S3 and then use the reference For native libraries, first need to compiled in Amazon linux AWS SDK is already integrated with Lambda by default","title":"Function Dependencies"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#deploy-using-cloudformation","text":"We can add codes directly to the cloudformation template, but with this inline including, we can not add any dependency Code can be stored as zip file in S3 and referenced from the cloudforation template. This require include the S3 bucket name S3 buckets object full path Version code [if enabled] If the code is updeted in the S3 but the S3 version is not updated in the cloudformation, then the new lambda function will not be included When the S3 bucket (where the zipped code is located) and cloudformation is in different account, Add an execution role in the cloudformtaion allowing fetch and list the S3 bucket Update bucket policy to allow the cloudformation owner account","title":"Deploy Using Cloudformation"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#lambda-containerization","text":"Lambda function container should be deployed through ECR and size can be up to GB The base image must include the Lambda Runtime API (Recommanded to use AWS provided image) Containers can be tested locally using the Lambda Runtime Interface Emulator Optimization Use AWS provided base image, since it is cached Keep the less changing commands on the top of the docker file Using single repository helps ECR to compare layers","title":"Lambda Containerization"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#versioning-and-alias","text":"Versioning Everytime we make a update to the code/configuration of a lambda function, a new version of the lambda function created. When we invoke the function, it usually invoke the latest version of the function, Although we can invoke any previous/specific version of the lambda function. Each version of the lambda function will have their own AWS ARN. Alias When we want to point a specific version of a lambda fuction, we can make use of the Alias. Alias can be dev, test, prod, rc etc. By these alias, we can also implement the blue green deployment. Like an alias can send traffic to different versions with specific percentage or weight. Others Alias can only point to different version/versions of lambda function. It can not point to another alias. $latest version is mutable, it always point to the updated version of the lambda function. Other versions are immutable. If we update any code/configuration, the lambda function versions will be updated.","title":"Versioning and Alias"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#lambda-and-codedeploy","text":"Allow automating traffic shifting Very much integrated with SAM 3 types of traffic shifting Linear : Traffic gradually move with times untill completely migrate all traffic Linear10PercentEvery3Minutes Linear10PercentEvery10Minutes Canary : First mirgrate a certain proportion of traffic and later migrate all traffic Canary10Percent5Minutes: Move 10 percent for 5 minutes and later 100% goes to target function Canary10Percent30Minutes: Move 10 percent for 30 minutes and later 100% goes to new function All At Once : Immediately move all the traffic to target function To setup code deploy, required Function Name Function Alias Current Function Version Target Function Version","title":"Lambda And CodeDeploy"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#lambda-function-url","text":"TBD: 310 & 311","title":"Lambda Function URL"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#lambda-and-codeguru","text":"Provide runtime performance of the lambda function Support Java and Python runtimes","title":"Lambda And CodeGuru"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#best-practices","text":"Connect db outside the handler Initialize AWS SDK outside the handler Pull dependencies outside of the function handler Use env variables for db strings, s3 url etc For sensetive values like password, use the KMS for encryption Never ever call a Lambda function recursively","title":"Best Practices"},{"location":"Notes/Serverless/01%20Lambda/01%20Lambda%20Overview/#gotcha","text":"Environment Variables : Regular application environment variables Stage Variables : Related to API Gateway, can be dev , prod , v1 , v2 etc. Also these stage variables acn be mapped with the alias of lambda function Aliases : Pointer to specific lambda version Misc To do encryption in /tmp need to use KMS, lambda does not handle by default Stage variables are for API gateway while aliases are for lambda function","title":"Gotcha"},{"location":"Notes/Serverless/01%20Lambda/02%20Lambda%20Attach%20With%20VPC/","text":"Attach Wit VPC By default, the lambda function can not access the VPC resources. It is deployed outside of the VPC. Even we attach it along with the VPC, it will get the access to the VPC resources, but by default can not access the internet. Attach ENI with VPC Attach NAT with VPC Enable all outbound SG Access Resources To get access to our VPC resources it uses ENI and access the VPC resources through it. Lambda function should attach to An ENI in the VPC subnets, with with our function will be attached An Create ENI To VPC execution role We will need a A role will be Access Internet When it EC2 Instance , we can deploy it in a public subnet and it will get a public IP along with the internet access. But when it comes to Lambda function, even though we connect it with the public subnet ENI, it will not get a public IP and also not get the internet access. To make sure a lambda function can access internet from the VPC we have to Linked it with private/public (private is the best practice) subnet ENI Attach NAT Gateway , so this will link with IGW and ensure the internet connection Make sure lambda associated SG allow the internet connection After getting public internet access, we can access other AWS resource using public internet or using the AWS Private net (VPC Endpoints)","title":"02 Lambda Attach With VPC"},{"location":"Notes/Serverless/01%20Lambda/02%20Lambda%20Attach%20With%20VPC/#attach-wit-vpc","text":"By default, the lambda function can not access the VPC resources. It is deployed outside of the VPC. Even we attach it along with the VPC, it will get the access to the VPC resources, but by default can not access the internet. Attach ENI with VPC Attach NAT with VPC Enable all outbound SG Access Resources To get access to our VPC resources it uses ENI and access the VPC resources through it. Lambda function should attach to An ENI in the VPC subnets, with with our function will be attached An Create ENI To VPC execution role We will need a A role will be Access Internet When it EC2 Instance , we can deploy it in a public subnet and it will get a public IP along with the internet access. But when it comes to Lambda function, even though we connect it with the public subnet ENI, it will not get a public IP and also not get the internet access. To make sure a lambda function can access internet from the VPC we have to Linked it with private/public (private is the best practice) subnet ENI Attach NAT Gateway , so this will link with IGW and ensure the internet connection Make sure lambda associated SG allow the internet connection After getting public internet access, we can access other AWS resource using public internet or using the AWS Private net (VPC Endpoints)","title":"Attach Wit VPC"},{"location":"Notes/Serverless/01%20Lambda/03%20Lambda%20Versioning%20and%20Alias/","text":"Versioning Everytime we make a update to the code/configuration of a lambda function, a new version of the lambda function created. When we invoke the function, it usually invoke the latest version of the function, Although we can invoke any previous/specific version of the lambda function. Each version of the lambda function will have their own AWS ARN. Alias When we want to point a specific version of a lambda fuction, we can make use of the Alias. Alias can be dev, test, prod, rc etc. By these alias, we can also implement the blue green deployment. Like an alias can send traffic to both the test and prod with specific percentage or weight. Gotcha Alias can only point to different version/versions of lambda function. It can not point to another alias. $latest version is mutable, it always point to the updated version of the lambda function. Other versions are immutable. If we update any code/configuration, the lambda function versions will be updated.","title":"03 Lambda Versioning and Alias"},{"location":"Notes/Serverless/01%20Lambda/03%20Lambda%20Versioning%20and%20Alias/#versioning","text":"Everytime we make a update to the code/configuration of a lambda function, a new version of the lambda function created. When we invoke the function, it usually invoke the latest version of the function, Although we can invoke any previous/specific version of the lambda function. Each version of the lambda function will have their own AWS ARN.","title":"Versioning"},{"location":"Notes/Serverless/01%20Lambda/03%20Lambda%20Versioning%20and%20Alias/#alias","text":"When we want to point a specific version of a lambda fuction, we can make use of the Alias. Alias can be dev, test, prod, rc etc. By these alias, we can also implement the blue green deployment. Like an alias can send traffic to both the test and prod with specific percentage or weight.","title":"Alias"},{"location":"Notes/Serverless/01%20Lambda/03%20Lambda%20Versioning%20and%20Alias/#gotcha","text":"Alias can only point to different version/versions of lambda function. It can not point to another alias. $latest version is mutable, it always point to the updated version of the lambda function. Other versions are immutable. If we update any code/configuration, the lambda function versions will be updated.","title":"Gotcha"},{"location":"Notes/Serverless/01%20Lambda/04%20Lambda%20Performance/","text":"Performance Timeout Default time out for a lambda function is 5 minutes Can be extend the time to 15 minutes Memory Lambda memory start from 128MB and then 64MB of increments. CPU The more memory we add to the lambda function, the more CPU we will get. At 1792MB ram, we will get a full CPU. After memory of 1792, we will have more than 1 CPU, and to utilize the CPU, we should make use of the multi-threading. For CPU bound application, we need to increase RAM. Execution Context Execution context is the application run time used to maintain for a small amount of time. If we define some task outside of the function, like db connection, sdk client initialization etc, these will be persisted and can be used in other functions. We can store some files in /tmp directory and these will be available in the execution context. Maximum size of the /tmp directory will be 512 MB. Lambda Layers A zip archive, used to store additional code, packages, runtime other than the original function. Can have maximum 5 layers. Along with the functions, total size of functions is limited (50MB compressed or 250MB un-compressed). We can use layers to reduce the each time deployment time.","title":"04 Lambda Performance"},{"location":"Notes/Serverless/01%20Lambda/04%20Lambda%20Performance/#performance","text":"Timeout Default time out for a lambda function is 5 minutes Can be extend the time to 15 minutes Memory Lambda memory start from 128MB and then 64MB of increments. CPU The more memory we add to the lambda function, the more CPU we will get. At 1792MB ram, we will get a full CPU. After memory of 1792, we will have more than 1 CPU, and to utilize the CPU, we should make use of the multi-threading. For CPU bound application, we need to increase RAM. Execution Context Execution context is the application run time used to maintain for a small amount of time. If we define some task outside of the function, like db connection, sdk client initialization etc, these will be persisted and can be used in other functions. We can store some files in /tmp directory and these will be available in the execution context. Maximum size of the /tmp directory will be 512 MB.","title":"Performance"},{"location":"Notes/Serverless/01%20Lambda/04%20Lambda%20Performance/#lambda-layers","text":"A zip archive, used to store additional code, packages, runtime other than the original function. Can have maximum 5 layers. Along with the functions, total size of functions is limited (50MB compressed or 250MB un-compressed). We can use layers to reduce the each time deployment time.","title":"Lambda Layers"},{"location":"Notes/Serverless/01%20Lambda/05%20Lambda%20Integrations/","text":"Lambda Integrations Lambda support 3 types invocation: RequestResponse (Default Type) : Invoke the function synchronously. Keeps the connection open until the function return response or times out. The API response contains response, data and status Event : Invoke the function asynchronously. Returns a response code not the response data. We can configure a DLQ (Dead letter queue) to investigate the failure DryRun : Validate parameter and verify roles and permissions We can integrate lambda with API gateway by proxy and non-proxy, Proxy : Redirect all the params, payloads, headers etc directly to the Lambda from api gateway. non-proxy : We need to map all the params, payloads etc.","title":"05 Lambda Integrations"},{"location":"Notes/Serverless/01%20Lambda/05%20Lambda%20Integrations/#lambda-integrations","text":"Lambda support 3 types invocation: RequestResponse (Default Type) : Invoke the function synchronously. Keeps the connection open until the function return response or times out. The API response contains response, data and status Event : Invoke the function asynchronously. Returns a response code not the response data. We can configure a DLQ (Dead letter queue) to investigate the failure DryRun : Validate parameter and verify roles and permissions We can integrate lambda with API gateway by proxy and non-proxy, Proxy : Redirect all the params, payloads, headers etc directly to the Lambda from api gateway. non-proxy : We need to map all the params, payloads etc.","title":"Lambda Integrations"},{"location":"Notes/Serverless/01%20Lambda/06%20Lambda%20Best%20Practices/","text":"Lambda Best Practices Separate the lambda handler from the core logic (Handler should be used as the entry point) Use execution context to improve performance Use env variables to pass operational parameters to lambda function Avoid using the recursive code Control the dependencies in functions deployment packages (Can be put them in the lambda layer) Minimize deployment packages Example is, selectively include the libraries that are required For error like ServiceException use retry with ErrorEquals string to match the error","title":"06 Lambda Best Practices"},{"location":"Notes/Serverless/01%20Lambda/06%20Lambda%20Best%20Practices/#lambda-best-practices","text":"Separate the lambda handler from the core logic (Handler should be used as the entry point) Use execution context to improve performance Use env variables to pass operational parameters to lambda function Avoid using the recursive code Control the dependencies in functions deployment packages (Can be put them in the lambda layer) Minimize deployment packages Example is, selectively include the libraries that are required For error like ServiceException use retry with ErrorEquals string to match the error","title":"Lambda Best Practices"},{"location":"Notes/Serverless/01%20Lambda/07%20Lambda%20Deployments/","text":"Lambda Deployments With Cloudformation For inline lambda code, in cloudformation, we can add these codes under the ZipFile property of AWS::Lambda::Function .","title":"07 Lambda Deployments"},{"location":"Notes/Serverless/01%20Lambda/07%20Lambda%20Deployments/#lambda-deployments","text":"With Cloudformation For inline lambda code, in cloudformation, we can add these codes under the ZipFile property of AWS::Lambda::Function .","title":"Lambda Deployments"},{"location":"Notes/Serverless/02%20DynamoDB/01%20DynamoDB%20Overview/","text":"DynamoDB Managed database, replicated across 3 AZ NoSQL database Enable event driven programming using DynamoDB Streams In DynamoDB database is already created, only needs to create Table Each Table should have a primary key (Must be decided while creating the table) Can have infinite number of rows Max size of item can be 400KB Data types Scalar String Number Binary Boolean Null Document List Map Set String Set Number Set Binary Set Filter expressions are for read operations while conditional expression for write operation For write operation, as conditional expressions we can use attribute_exists attribute_not_exists attribute_type contains begins_with IN (includes) between (between low and high) size (length of string) To improve performance we can use DAX (milliseconds to micro seconds) Use partition keys of high cardinality, so large number of distinct values for each item DAX Stands for DynamoDB Accelerator Caching mechanism for DynamoDB DynamoDB Stream Raise event on Create , Update , Delete Can be used to trigger events on DB Changes Data persisted in the stream of 24 hours Transaction All or nothing of operations Global Table Replicated in multiple region Needs to enable DynamoDB Stream Active Active Replication Changes in any region, impact all other regions Security Available in VPC Endpoints Fully controlled by IAM Encryption Server side encryption is enabled by default in all DynamoDB table At rest by KMS AWS Managed Key for DynamoDB Customer Managed Key In flight by SSL / TLS Read Write Capacity RCU and WCU are evenly distributed over all the partitions We can controll the capacity of the table with two of the following ways, Provisioned Mode On Demand Mode To switch between provisioned and on-demand mode, requires 24 hours Provisioned Mode The read-write capacity has to be determine beforehand Only have to pay for the defined provisioned capacity On Demand Mode Read-write capacity will be automatically scaled up/down according to the workloads No capacity planning is required beforehand Payment will be according to the usage but this is expensive compare to the provisioned mode Provisioned Throughput 2 types of Provision Throughput RCU Read Capacity Unit Each RCU can handle one of the followings 1 Strongly Consistent read 4KB/s [Enabled by adding parameter ConsistentRead as true ] 2 Eventually Consistent read 4KB/s WCU Write Capacity Unit 1 write 1KB/s Can be used on-demand throughput (price is 2.5X more) Throughput can be exceeded using Burst Credit If Burst Credit is empty, it throws ProvisionThroughputException In case of ProvisionThroughputException , it is recommended to retry with expotential backoff Can be used DynamoDB Auto Scaling No need to provision throughput Comparatively expensive Calculate WCU WCU stands for write capacity unit Item size should be rounded to upper KB Formula (number_of_items / time_in_seconds) * (size_of_item_in_KB / 1KB) Examples Write 10 items per second with item size of 2 KB: (10 items / 1 sec) * (2 KB / 1 KB) = 20 WCUs Write 6 items per socond with item size of 4.5 KB: (6 items * 1 sec) * (5 KB / 1 KB) = 30 WCUs Write 120 items per minute with item size 2 KB: (120 items / 60 secs) * (2 KB / 1 KB) = 4 WCUs Calculate RCU RCU stands for read capacity unit Fromula For strongly consistent: number of read per second * (size_of_item_in_KB / 4KB) For eventual consistent: (number of read per second / 2) * (size_of_item_in_KB / 4KB) Examples 10 strongly consistent read per second with size 4 KB: 10 * (4KB / 4KB) = 10 RCUs 16 eventualy consistent read per second with size 12 KB: (16 / 2) * (12 KB / 4 KB) = 24 RCUs 10 strongly consistent read per second with size 6 KB: 10 * (8 KB / 4 KB) = 20 RCUs Throttling When RCU or WCU is exceeded, throw \"ProvisionedThroughputExceedException\" Reasons One partition key is read too many times Hot partition (One partition is getting all the read/write request) Very large item Solution Expotential backoff (Included in SDK) Distribute partition keys For RCU, utilize DAX PartiQl SQL compatible language for DynamoDB Allows CRUD and Batch operation in DynamoDB using SQL INSERT UPDATE SELECT DELETE Aloows running queries on multiple dynamoDB tables Can be used from AWS management console NoSQL workbench for DynamoDB DynamoDB APIs AWS CLI AWS SDK Authorizer Lambda support two types of authorizer Token Based : A bearer token is passed as the caller identity Request Parameter Based : Caller identity is passed to context as combinations of headers, query string parameters etc Table Cleanup Deleting each item requires a lot of RCU and WCU, not efficient Drop the entire table and then recreate is the efficient way Table Cloning Use data pipeline (launch EMR and do the copy operation), not efficient Backup the current table and restore the backup in another table, time consuming but efficient Manually scan and put item in another table, requires writing code Access","title":"01 DynamoDB Overview"},{"location":"Notes/Serverless/02%20DynamoDB/01%20DynamoDB%20Overview/#dynamodb","text":"Managed database, replicated across 3 AZ NoSQL database Enable event driven programming using DynamoDB Streams In DynamoDB database is already created, only needs to create Table Each Table should have a primary key (Must be decided while creating the table) Can have infinite number of rows Max size of item can be 400KB Data types Scalar String Number Binary Boolean Null Document List Map Set String Set Number Set Binary Set Filter expressions are for read operations while conditional expression for write operation For write operation, as conditional expressions we can use attribute_exists attribute_not_exists attribute_type contains begins_with IN (includes) between (between low and high) size (length of string) To improve performance we can use DAX (milliseconds to micro seconds) Use partition keys of high cardinality, so large number of distinct values for each item DAX Stands for DynamoDB Accelerator Caching mechanism for DynamoDB DynamoDB Stream Raise event on Create , Update , Delete Can be used to trigger events on DB Changes Data persisted in the stream of 24 hours Transaction All or nothing of operations Global Table Replicated in multiple region Needs to enable DynamoDB Stream Active Active Replication Changes in any region, impact all other regions Security Available in VPC Endpoints Fully controlled by IAM Encryption Server side encryption is enabled by default in all DynamoDB table At rest by KMS AWS Managed Key for DynamoDB Customer Managed Key In flight by SSL / TLS","title":"DynamoDB"},{"location":"Notes/Serverless/02%20DynamoDB/01%20DynamoDB%20Overview/#read-write-capacity","text":"RCU and WCU are evenly distributed over all the partitions We can controll the capacity of the table with two of the following ways, Provisioned Mode On Demand Mode To switch between provisioned and on-demand mode, requires 24 hours Provisioned Mode The read-write capacity has to be determine beforehand Only have to pay for the defined provisioned capacity On Demand Mode Read-write capacity will be automatically scaled up/down according to the workloads No capacity planning is required beforehand Payment will be according to the usage but this is expensive compare to the provisioned mode","title":"Read Write Capacity"},{"location":"Notes/Serverless/02%20DynamoDB/01%20DynamoDB%20Overview/#provisioned-throughput","text":"2 types of Provision Throughput RCU Read Capacity Unit Each RCU can handle one of the followings 1 Strongly Consistent read 4KB/s [Enabled by adding parameter ConsistentRead as true ] 2 Eventually Consistent read 4KB/s WCU Write Capacity Unit 1 write 1KB/s Can be used on-demand throughput (price is 2.5X more) Throughput can be exceeded using Burst Credit If Burst Credit is empty, it throws ProvisionThroughputException In case of ProvisionThroughputException , it is recommended to retry with expotential backoff Can be used DynamoDB Auto Scaling No need to provision throughput Comparatively expensive Calculate WCU WCU stands for write capacity unit Item size should be rounded to upper KB Formula (number_of_items / time_in_seconds) * (size_of_item_in_KB / 1KB) Examples Write 10 items per second with item size of 2 KB: (10 items / 1 sec) * (2 KB / 1 KB) = 20 WCUs Write 6 items per socond with item size of 4.5 KB: (6 items * 1 sec) * (5 KB / 1 KB) = 30 WCUs Write 120 items per minute with item size 2 KB: (120 items / 60 secs) * (2 KB / 1 KB) = 4 WCUs Calculate RCU RCU stands for read capacity unit Fromula For strongly consistent: number of read per second * (size_of_item_in_KB / 4KB) For eventual consistent: (number of read per second / 2) * (size_of_item_in_KB / 4KB) Examples 10 strongly consistent read per second with size 4 KB: 10 * (4KB / 4KB) = 10 RCUs 16 eventualy consistent read per second with size 12 KB: (16 / 2) * (12 KB / 4 KB) = 24 RCUs 10 strongly consistent read per second with size 6 KB: 10 * (8 KB / 4 KB) = 20 RCUs","title":"Provisioned Throughput"},{"location":"Notes/Serverless/02%20DynamoDB/01%20DynamoDB%20Overview/#throttling","text":"When RCU or WCU is exceeded, throw \"ProvisionedThroughputExceedException\" Reasons One partition key is read too many times Hot partition (One partition is getting all the read/write request) Very large item Solution Expotential backoff (Included in SDK) Distribute partition keys For RCU, utilize DAX","title":"Throttling"},{"location":"Notes/Serverless/02%20DynamoDB/01%20DynamoDB%20Overview/#partiql","text":"SQL compatible language for DynamoDB Allows CRUD and Batch operation in DynamoDB using SQL INSERT UPDATE SELECT DELETE Aloows running queries on multiple dynamoDB tables Can be used from AWS management console NoSQL workbench for DynamoDB DynamoDB APIs AWS CLI AWS SDK","title":"PartiQl"},{"location":"Notes/Serverless/02%20DynamoDB/01%20DynamoDB%20Overview/#authorizer","text":"Lambda support two types of authorizer Token Based : A bearer token is passed as the caller identity Request Parameter Based : Caller identity is passed to context as combinations of headers, query string parameters etc","title":"Authorizer"},{"location":"Notes/Serverless/02%20DynamoDB/01%20DynamoDB%20Overview/#table-cleanup","text":"Deleting each item requires a lot of RCU and WCU, not efficient Drop the entire table and then recreate is the efficient way","title":"Table Cleanup"},{"location":"Notes/Serverless/02%20DynamoDB/01%20DynamoDB%20Overview/#table-cloning","text":"Use data pipeline (launch EMR and do the copy operation), not efficient Backup the current table and restore the backup in another table, time consuming but efficient Manually scan and put item in another table, requires writing code","title":"Table Cloning"},{"location":"Notes/Serverless/02%20DynamoDB/01%20DynamoDB%20Overview/#access","text":"","title":"Access"},{"location":"Notes/Serverless/02%20DynamoDB/02%20DynamoDB%20Caching/","text":"Caching Caching can be used in the dynamodb by DAX In memory caching Resolves the hot key issue (where one item is fetching over and over) Default TTL for DAX is 5 minutes It is recomanded to deploy DAX at least 3 AZ Can be integrated with IAM, KMS, VPC, CloudTrail etc When dax are used with strongly consistent query, result is not cached. DAX Caching Write Through : While write data to dynamoDB, data will be persisted in DAX Write Around : While write data to dynamoDB, data will not be persisted in DAX DAX vs ElasticCache DAX has more pricing compare to the elastic cache For plain query/scan use DAX to cache the object On the other hand, when a lot of filtering, sum, aggregation needs to be cached, use ElasticCache DynamoDB vs ElasticCache vs EFS vs EBS vs Instance Store vs S3 (In terms of Session State) For serverless solution / auto-scaling use the DynamoDB For in-memory solution, use the ElasticCache For disk drive, use EFS, where the EFS will be attached with EC2 instances as network drive Only for local caching, EBS can be used, not suitable for shared caching Same as EBS, instance store can be used for local caching, not suitable for shared caching S3 are meant for object caching and have higher latency, not suitable for small objects","title":"02 DynamoDB Caching"},{"location":"Notes/Serverless/02%20DynamoDB/02%20DynamoDB%20Caching/#caching","text":"Caching can be used in the dynamodb by DAX In memory caching Resolves the hot key issue (where one item is fetching over and over) Default TTL for DAX is 5 minutes It is recomanded to deploy DAX at least 3 AZ Can be integrated with IAM, KMS, VPC, CloudTrail etc When dax are used with strongly consistent query, result is not cached. DAX Caching Write Through : While write data to dynamoDB, data will be persisted in DAX Write Around : While write data to dynamoDB, data will not be persisted in DAX DAX vs ElasticCache DAX has more pricing compare to the elastic cache For plain query/scan use DAX to cache the object On the other hand, when a lot of filtering, sum, aggregation needs to be cached, use ElasticCache DynamoDB vs ElasticCache vs EFS vs EBS vs Instance Store vs S3 (In terms of Session State) For serverless solution / auto-scaling use the DynamoDB For in-memory solution, use the ElasticCache For disk drive, use EFS, where the EFS will be attached with EC2 instances as network drive Only for local caching, EBS can be used, not suitable for shared caching Same as EBS, instance store can be used for local caching, not suitable for shared caching S3 are meant for object caching and have higher latency, not suitable for small objects","title":"Caching"},{"location":"Notes/Serverless/02%20DynamoDB/03%20DynamoDB%20TTL/","text":"TTL use to expire and delete items from the table. The time of ttl should be defined in each row. The deletion usually takes place the time we defined. But some cases it might take 2 days to delete. When the deletion happened, the GSI/LSI related tot he item will also be deleted. This ttl delete operation does not require any RCU or WCU. With the DynamoDB streams, we can control/recover the deleted items.","title":"03 DynamoDB TTL"},{"location":"Notes/Serverless/02%20DynamoDB/03%20DynamoDB%20TTL/#ttl","text":"use to expire and delete items from the table. The time of ttl should be defined in each row. The deletion usually takes place the time we defined. But some cases it might take 2 days to delete. When the deletion happened, the GSI/LSI related tot he item will also be deleted. This ttl delete operation does not require any RCU or WCU. With the DynamoDB streams, we can control/recover the deleted items.","title":"TTL"},{"location":"Notes/Serverless/02%20DynamoDB/04%20DynamoDB%20Streams/","text":"DynamoDB Streams We can track the changes of dynamodb using the dynamoDB streams. On create, delete or updated, when the dynamodb streams is being enabled we can run a lambda function and do necessary tasks. When we use global dynamodb table and data is being replicated between multiple region, the dynamodb streams must be enabled. StreamViewType determines what information are written to the stream to this table. StreamViewTypes are KEYS_ONLY : Only pass the modified key items NEW_IMAGE : Pass the new value OLD_IMAGE : Pass the existing value NEW_AND_OLD_IMAGES : Pass both the new and old values As stream source, if we make use of lambda, the lambda needs permission to read these stream. In this case, we can make use of a managed policy, AWSLambdaDynamoDBExecutionRole . DynamoDB Streams can be integrated with Lambda Kinesis data streams Kinsis client library applications Others: Data retention period for the DynamoDB streams are 24 hours. DynamoDB streams are not effective for previous changes (before enabling streams)","title":"04 DynamoDB Streams"},{"location":"Notes/Serverless/02%20DynamoDB/04%20DynamoDB%20Streams/#dynamodb-streams","text":"We can track the changes of dynamodb using the dynamoDB streams. On create, delete or updated, when the dynamodb streams is being enabled we can run a lambda function and do necessary tasks. When we use global dynamodb table and data is being replicated between multiple region, the dynamodb streams must be enabled. StreamViewType determines what information are written to the stream to this table. StreamViewTypes are KEYS_ONLY : Only pass the modified key items NEW_IMAGE : Pass the new value OLD_IMAGE : Pass the existing value NEW_AND_OLD_IMAGES : Pass both the new and old values As stream source, if we make use of lambda, the lambda needs permission to read these stream. In this case, we can make use of a managed policy, AWSLambdaDynamoDBExecutionRole . DynamoDB Streams can be integrated with Lambda Kinesis data streams Kinsis client library applications Others: Data retention period for the DynamoDB streams are 24 hours. DynamoDB streams are not effective for previous changes (before enabling streams)","title":"DynamoDB Streams"},{"location":"Notes/Serverless/02%20DynamoDB/05%20DynamoDB%20Performance/","text":"DynamoDB Performance Caching using DAX Avoid SCAN and try to use query instead Reduce the page size in both cases SCAN and query Supports BatchGetItem and BatchWriteItem to make use of bulk read and write operations Atomic counter Update a numeric attribute with UpdateItem operation. No matter what UpdateItem operation does, like update any properties, the atomic counter update the value If UpdateItem operation fails and try again, the atomic counter increase twice DB Locking Optimistic Locking : Before update/delete, make sure the item is same as the client Example : Let's say, someone is updating the price of a product. First get the product { id: 1, price: 10 } . Now the updated price should be 15 . While do the update, the dynamoDB client find out, in the db, the price is already updated 20 by someone. In this case the price will not be updated. This is optimistic locking. Pessimistic Locking : There's an attribute in the object act as version number and in Pessimistic Locking , ensure the version number is same as client before update or delete. Details : Pessimistic Locking lock the document in the DB so no one can modify while it's being operated by an user. Useful for prevent overwriting but interrupt the other users operations. Overly Optimistic Locking : Used in a system where there is only one user/one operation at a time. Conditional Writing : DynamoDB allows conditional writing, where write operation is being happened if certain defined condition matched. Rate limit parallel scan can reduce the cost Getting Write Consumed Capacity When there is an operation of Put , Update , Delete in the DynamoDB, it consumes some of the write capacity. We can get these consumed capacity as the response of the operation. To get the consumed write capacity, after an operation is being completed, we can make use of the following properties, TOTAL : Returns total number of consumed capacity used by the operation INDEXES : Returns total consumed capacity along with the sub total of affected secondary indexes. NONE : This is the default behavior and no consumed write capacity is returns Avoid Hot Partitioning A hot partition occurs when an individual partition is being accessed more frequently from application downstream. To reduce the hot partitioning, we can, Increase Read and Write Capacity of Table Implement Error Retries and Exponential Back-off : SDK already has this feature installed Distribute Read And Write Operations Evenly Across the Table Implement Caching by DAX Caching with DAX is expensive and Distribute Read And Write Operations Evenly Across the Table needs to refactor the whole application. In this case, the easiest and quickest solution is Increase Read and Write Capacity of Table and Implement Error Retries and Exponential Back-off . Transactions DynamoDB provide all-or-nothing types operation with TransactWriteItems and TransactGetItems . The consumstion of read and write operation is 2x RCU and 2x WCU. This is because, the transactional operation requires prepare and commit. Read Mode (TransactGetItems) Eventual consistency Strong consistency Transactional Write Mode (TransactWriteItems) Standard writing Transactional writing With TransactWriteItems , we can do a batch of 25 items within same account, region and multiple tables. We can perform the followings, Put Update Delete ConditionCheck Sharding Use distributed partition key If partition key is not enough, use composite key by including a sort key If composite key is not enough for potential hot partitioning, add random prefix or suffix with the partition key Best Practices Keep number of indexes minimum Avoid indexing for heavy write activity Consumed Capacity After perform a DB operation, to get, how much read/write capacity is being used, we can use ReturnConsumedCapacity using the following properties, TOTAL: Return total number of capacity consumed INDEXES: Return total and indexing based capacity consumed NONE: Default, noting return Generic Approach Reduce scan time by implementing parallel scanning To avoid provision throughput, use smaller page size","title":"05 DynamoDB Performance"},{"location":"Notes/Serverless/02%20DynamoDB/05%20DynamoDB%20Performance/#dynamodb-performance","text":"Caching using DAX Avoid SCAN and try to use query instead Reduce the page size in both cases SCAN and query Supports BatchGetItem and BatchWriteItem to make use of bulk read and write operations Atomic counter Update a numeric attribute with UpdateItem operation. No matter what UpdateItem operation does, like update any properties, the atomic counter update the value If UpdateItem operation fails and try again, the atomic counter increase twice DB Locking Optimistic Locking : Before update/delete, make sure the item is same as the client Example : Let's say, someone is updating the price of a product. First get the product { id: 1, price: 10 } . Now the updated price should be 15 . While do the update, the dynamoDB client find out, in the db, the price is already updated 20 by someone. In this case the price will not be updated. This is optimistic locking. Pessimistic Locking : There's an attribute in the object act as version number and in Pessimistic Locking , ensure the version number is same as client before update or delete. Details : Pessimistic Locking lock the document in the DB so no one can modify while it's being operated by an user. Useful for prevent overwriting but interrupt the other users operations. Overly Optimistic Locking : Used in a system where there is only one user/one operation at a time. Conditional Writing : DynamoDB allows conditional writing, where write operation is being happened if certain defined condition matched. Rate limit parallel scan can reduce the cost","title":"DynamoDB Performance"},{"location":"Notes/Serverless/02%20DynamoDB/05%20DynamoDB%20Performance/#getting-write-consumed-capacity","text":"When there is an operation of Put , Update , Delete in the DynamoDB, it consumes some of the write capacity. We can get these consumed capacity as the response of the operation. To get the consumed write capacity, after an operation is being completed, we can make use of the following properties, TOTAL : Returns total number of consumed capacity used by the operation INDEXES : Returns total consumed capacity along with the sub total of affected secondary indexes. NONE : This is the default behavior and no consumed write capacity is returns","title":"Getting Write Consumed Capacity"},{"location":"Notes/Serverless/02%20DynamoDB/05%20DynamoDB%20Performance/#avoid-hot-partitioning","text":"A hot partition occurs when an individual partition is being accessed more frequently from application downstream. To reduce the hot partitioning, we can, Increase Read and Write Capacity of Table Implement Error Retries and Exponential Back-off : SDK already has this feature installed Distribute Read And Write Operations Evenly Across the Table Implement Caching by DAX Caching with DAX is expensive and Distribute Read And Write Operations Evenly Across the Table needs to refactor the whole application. In this case, the easiest and quickest solution is Increase Read and Write Capacity of Table and Implement Error Retries and Exponential Back-off .","title":"Avoid Hot Partitioning"},{"location":"Notes/Serverless/02%20DynamoDB/05%20DynamoDB%20Performance/#transactions","text":"DynamoDB provide all-or-nothing types operation with TransactWriteItems and TransactGetItems . The consumstion of read and write operation is 2x RCU and 2x WCU. This is because, the transactional operation requires prepare and commit. Read Mode (TransactGetItems) Eventual consistency Strong consistency Transactional Write Mode (TransactWriteItems) Standard writing Transactional writing With TransactWriteItems , we can do a batch of 25 items within same account, region and multiple tables. We can perform the followings, Put Update Delete ConditionCheck","title":"Transactions"},{"location":"Notes/Serverless/02%20DynamoDB/05%20DynamoDB%20Performance/#sharding","text":"Use distributed partition key If partition key is not enough, use composite key by including a sort key If composite key is not enough for potential hot partitioning, add random prefix or suffix with the partition key","title":"Sharding"},{"location":"Notes/Serverless/02%20DynamoDB/05%20DynamoDB%20Performance/#best-practices","text":"Keep number of indexes minimum Avoid indexing for heavy write activity","title":"Best Practices"},{"location":"Notes/Serverless/02%20DynamoDB/05%20DynamoDB%20Performance/#consumed-capacity","text":"After perform a DB operation, to get, how much read/write capacity is being used, we can use ReturnConsumedCapacity using the following properties, TOTAL: Return total number of capacity consumed INDEXES: Return total and indexing based capacity consumed NONE: Default, noting return","title":"Consumed Capacity"},{"location":"Notes/Serverless/02%20DynamoDB/05%20DynamoDB%20Performance/#generic-approach","text":"Reduce scan time by implementing parallel scanning To avoid provision throughput, use smaller page size","title":"Generic Approach"},{"location":"Notes/Serverless/02%20DynamoDB/06%20DynamoDB%20Indexing/","text":"DynamoDB Indexing A DynamoDB table can have two types of primary key, Simple primary key: Consist of a HASH key or partition key Composite primary key: Consist of two keys, HASH or partition key and RANGE or sort key Partition keys used to determine, in which partition the item should be placed. LSI & GSI DynamoDB requires us to specify the primary key for all operations and hence provide faster performance. We can scan on tables without primary key and it is not efficient and should avoid as much as possible. LSI : Stands for local secondary index. We must create the LSI whenever we create table. After a table being created, we can not update the LSI. We can create up to 5 local secondary indexes. LSI supports both the eventual consistency and strongly consistency. GSI : Stands for global secondary index. A global secondary index can be created with entirely new partition key and sort key. Supports maximum 20 GSI Global secondary index took a new partition and hence it needs a completely new throughput capacity for the RCU and WCU. GSI can be created anytime, not bound to the table creation time like the LSI. GSI can only perform eventual consistency Sparse index is useful for querying over a small subsection of a table.","title":"06 DynamoDB Indexing"},{"location":"Notes/Serverless/02%20DynamoDB/06%20DynamoDB%20Indexing/#dynamodb-indexing","text":"A DynamoDB table can have two types of primary key, Simple primary key: Consist of a HASH key or partition key Composite primary key: Consist of two keys, HASH or partition key and RANGE or sort key Partition keys used to determine, in which partition the item should be placed.","title":"DynamoDB Indexing"},{"location":"Notes/Serverless/02%20DynamoDB/06%20DynamoDB%20Indexing/#lsi-gsi","text":"DynamoDB requires us to specify the primary key for all operations and hence provide faster performance. We can scan on tables without primary key and it is not efficient and should avoid as much as possible. LSI : Stands for local secondary index. We must create the LSI whenever we create table. After a table being created, we can not update the LSI. We can create up to 5 local secondary indexes. LSI supports both the eventual consistency and strongly consistency. GSI : Stands for global secondary index. A global secondary index can be created with entirely new partition key and sort key. Supports maximum 20 GSI Global secondary index took a new partition and hence it needs a completely new throughput capacity for the RCU and WCU. GSI can be created anytime, not bound to the table creation time like the LSI. GSI can only perform eventual consistency Sparse index is useful for querying over a small subsection of a table.","title":"LSI &amp; GSI"},{"location":"Notes/Serverless/02%20DynamoDB/07%20DynamoDB%20Permissions/","text":"DynamoDB Permissions With dynamoDB We can grant permission of the table, but only for certain items. These permissioned rows should be included in the generated IAM policy For example, user will only get access of data of that user id We can also permissioned for certain rows For example, user will get location information only closer to him Encryption Using dynamoDB client library, we can can encrypt data in server, before submit to the database Implementation Condition must be applied with condition in IAM policy Track key should be the primary key Set the primary key using the dynamodb:LeadingKeys","title":"07 DynamoDB Permissions"},{"location":"Notes/Serverless/02%20DynamoDB/07%20DynamoDB%20Permissions/#dynamodb-permissions","text":"With dynamoDB We can grant permission of the table, but only for certain items. These permissioned rows should be included in the generated IAM policy For example, user will only get access of data of that user id We can also permissioned for certain rows For example, user will get location information only closer to him Encryption Using dynamoDB client library, we can can encrypt data in server, before submit to the database Implementation Condition must be applied with condition in IAM policy Track key should be the primary key Set the primary key using the dynamodb:LeadingKeys","title":"DynamoDB Permissions"},{"location":"Notes/Serverless/02%20DynamoDB/10.%20DynamoDB%20Misc/","text":"Misc Invalidate API caching by sending Cache-Control: max-age=0 header To ensure, only authorized client can invalidate the cache, we can make use of Require Authentication Using dynamoDB TTL, items can be deleted automatically","title":"10. DynamoDB Misc"},{"location":"Notes/Serverless/02%20DynamoDB/10.%20DynamoDB%20Misc/#misc","text":"Invalidate API caching by sending Cache-Control: max-age=0 header To ensure, only authorized client can invalidate the cache, we can make use of Require Authentication Using dynamoDB TTL, items can be deleted automatically","title":"Misc"},{"location":"Notes/Serverless/02%20DynamoDB/11%20DynamoDB%20CLI/","text":"DynamoDB CLI Commands --projection-expression : only retrieve specific attributes --filter-expression : filter items before return to console --page-size : Since big results can enter time out, using page size, will make the api call separately in the background, shows all results without timeout issue. --max-items : max number of items shows, with next-token --starting-token : start from the token pointer","title":"11 DynamoDB CLI"},{"location":"Notes/Serverless/02%20DynamoDB/11%20DynamoDB%20CLI/#dynamodb-cli","text":"","title":"DynamoDB CLI"},{"location":"Notes/Serverless/02%20DynamoDB/11%20DynamoDB%20CLI/#commands","text":"--projection-expression : only retrieve specific attributes --filter-expression : filter items before return to console --page-size : Since big results can enter time out, using page size, will make the api call separately in the background, shows all results without timeout issue. --max-items : max number of items shows, with next-token --starting-token : start from the token pointer","title":"Commands"},{"location":"Notes/Serverless/03%20API%20Gateway/01%20API%20Gateway%20Overview/","text":"Overview Support web-socket protocol Handle API versioning Multiple Environment Security (Authentication, Authorization) Using API keys, handle request throttling Swagger / Open API to import Definition Transform and validate the Request and Response Generate SDK and API Specification Cache API response Can be orchestrate multiple web app and micro services Can set different usage plan for different users for different level of access Can set different quota and throttling to different level of access To pass a stage variable, use $stagevariables.<variableName> concept Integration Lambda Invoke Lambda function Expose REST API backed by Lambda HTTP Endpoint AWS Service Expose any AWS Service as API Gateway Endpoint Types 3 types of API Gateway Endpoints Edge Optimized (By default use global network using cloudfront) This is default behavior API is only one region But to improve latency, request is routed through Cloudfront Edge Locations Regional API is in the one region Private Use inside the VPC as VPC Endpoint Resource policy is used to define access Advantage of Edge Optimized is - We can get Edge Optimized behavior - In this case, we have more control over - Caching - Strategies - Distribution Premium Users Api keys can be used to allow special previllages to the resources using API keys In this case, the API keys should be associated with the usage plan by invoking CreateUsagePlan Security IAM When users/roles is within AWS Account Handle Authentication and Authorization Leverage Sig v4 It's the IAM credentials in the HTTP Header Authorizer: two types of lambda authorizer Token based: get token as bearer token and later verify (OAuth or SAML) Request parameter based: get caller identity in form of the context, payload or query string CUP or Cognito User Pool When user pools are manages by Facebook, Google login No need to write custom code Only provide Authentication Authorization must be provided from the backend code Access of developer and users can be separated using IAM Permission Developer can manage and deploy API User can call API SSL/TLS though AWS Certificate Manager is free for API Gateway Invoke_Async is deprecated invocation type. Only the invoke is being used.","title":"01 API Gateway Overview"},{"location":"Notes/Serverless/03%20API%20Gateway/01%20API%20Gateway%20Overview/#overview","text":"Support web-socket protocol Handle API versioning Multiple Environment Security (Authentication, Authorization) Using API keys, handle request throttling Swagger / Open API to import Definition Transform and validate the Request and Response Generate SDK and API Specification Cache API response Can be orchestrate multiple web app and micro services Can set different usage plan for different users for different level of access Can set different quota and throttling to different level of access To pass a stage variable, use $stagevariables.<variableName> concept","title":"Overview"},{"location":"Notes/Serverless/03%20API%20Gateway/01%20API%20Gateway%20Overview/#integration","text":"Lambda Invoke Lambda function Expose REST API backed by Lambda HTTP Endpoint AWS Service Expose any AWS Service as API Gateway","title":"Integration"},{"location":"Notes/Serverless/03%20API%20Gateway/01%20API%20Gateway%20Overview/#endpoint-types","text":"3 types of API Gateway Endpoints Edge Optimized (By default use global network using cloudfront) This is default behavior API is only one region But to improve latency, request is routed through Cloudfront Edge Locations Regional API is in the one region Private Use inside the VPC as VPC Endpoint Resource policy is used to define access Advantage of Edge Optimized is - We can get Edge Optimized behavior - In this case, we have more control over - Caching - Strategies - Distribution","title":"Endpoint Types"},{"location":"Notes/Serverless/03%20API%20Gateway/01%20API%20Gateway%20Overview/#premium-users","text":"Api keys can be used to allow special previllages to the resources using API keys In this case, the API keys should be associated with the usage plan by invoking CreateUsagePlan","title":"Premium Users"},{"location":"Notes/Serverless/03%20API%20Gateway/01%20API%20Gateway%20Overview/#security","text":"IAM When users/roles is within AWS Account Handle Authentication and Authorization Leverage Sig v4 It's the IAM credentials in the HTTP Header Authorizer: two types of lambda authorizer Token based: get token as bearer token and later verify (OAuth or SAML) Request parameter based: get caller identity in form of the context, payload or query string CUP or Cognito User Pool When user pools are manages by Facebook, Google login No need to write custom code Only provide Authentication Authorization must be provided from the backend code Access of developer and users can be separated using IAM Permission Developer can manage and deploy API User can call API SSL/TLS though AWS Certificate Manager is free for API Gateway Invoke_Async is deprecated invocation type. Only the invoke is being used.","title":"Security"},{"location":"Notes/Serverless/03%20API%20Gateway/02%20API%20Gateway%20Response%20Code/","text":"Response Code For 4xx , client errors 400 Bad Request 403 Access Denied and WAF filtered 429 Quota Exceeded, throttle For 5xx , server errors 502 Gateway exception Heavy load Out of order invocations Incompatible output from the lambda 503 Service unavailable 504 Integration Failure Integration Timeout (Request time after 29 seconds)","title":"02 API Gateway Response Code"},{"location":"Notes/Serverless/03%20API%20Gateway/02%20API%20Gateway%20Response%20Code/#response-code","text":"For 4xx , client errors 400 Bad Request 403 Access Denied and WAF filtered 429 Quota Exceeded, throttle For 5xx , server errors 502 Gateway exception Heavy load Out of order invocations Incompatible output from the lambda 503 Service unavailable 504 Integration Failure Integration Timeout (Request time after 29 seconds)","title":"Response Code"},{"location":"Notes/Serverless/03%20API%20Gateway/03%20API%20Gateway%20Deployment/","text":"Deployment To make differenent types of deployment for the API gateway, we can make use of stage variable. For example we need dev, prod, rc, releases. We first define the stage variables and attach them to fthe HTTP endpoints. To make the API's live, we have to deploy them. Deployment is going under stages. we can put name and configurations on these stages. After each deployments, the previous deployment history is being persisted and we can role back if something goes wrong with latest deployment. We can define the stages by stage varibales. Stage variables can be used for different types of configuations like, lambda alias, http endpoints, parameter mapping templates etc. In lambda variable, we can access the stage variables using the context parameters. Integration Between Stage Variables and Lambda Alias In api gateway we can create stage variable with the same name of the lambda alias, we can map them. Staging For each stage, we can define separate settings for caching, logging, x-ray-tracing etc. Canary Deployment This is a blue/green deployment of API Gateway + Lambda . With canary deployment, initially we send small amount of traffic to the new function and do separate monitoring and logging for that function. If everything goes fine, we then fully move to the new function,","title":"03 API Gateway Deployment"},{"location":"Notes/Serverless/03%20API%20Gateway/03%20API%20Gateway%20Deployment/#deployment","text":"To make differenent types of deployment for the API gateway, we can make use of stage variable. For example we need dev, prod, rc, releases. We first define the stage variables and attach them to fthe HTTP endpoints. To make the API's live, we have to deploy them. Deployment is going under stages. we can put name and configurations on these stages. After each deployments, the previous deployment history is being persisted and we can role back if something goes wrong with latest deployment. We can define the stages by stage varibales. Stage variables can be used for different types of configuations like, lambda alias, http endpoints, parameter mapping templates etc. In lambda variable, we can access the stage variables using the context parameters.","title":"Deployment"},{"location":"Notes/Serverless/03%20API%20Gateway/03%20API%20Gateway%20Deployment/#integration-between-stage-variables-and-lambda-alias","text":"In api gateway we can create stage variable with the same name of the lambda alias, we can map them.","title":"Integration Between Stage Variables and Lambda Alias"},{"location":"Notes/Serverless/03%20API%20Gateway/03%20API%20Gateway%20Deployment/#staging","text":"For each stage, we can define separate settings for caching, logging, x-ray-tracing etc.","title":"Staging"},{"location":"Notes/Serverless/03%20API%20Gateway/03%20API%20Gateway%20Deployment/#canary-deployment","text":"This is a blue/green deployment of API Gateway + Lambda . With canary deployment, initially we send small amount of traffic to the new function and do separate monitoring and logging for that function. If everything goes fine, we then fully move to the new function,","title":"Canary Deployment"},{"location":"Notes/Serverless/03%20API%20Gateway/04%20API%20Gateway%20Caching/","text":"Caching Caching can be used to reduce the number of api calls to the backend. With caching enabled, we response will provided from the cahce if available. We can enable cache on the stage level as well as method level. To invalidate a cache, we need to make use of http header Cache-Control: max-age=0 . The client should have IAM permission to invalidate the cache. In case, we do not require any IAM permission for cache invalidation, then every client can invalide the cache, which is not expected.","title":"04 API Gateway Caching"},{"location":"Notes/Serverless/03%20API%20Gateway/04%20API%20Gateway%20Caching/#caching","text":"Caching can be used to reduce the number of api calls to the backend. With caching enabled, we response will provided from the cahce if available. We can enable cache on the stage level as well as method level. To invalidate a cache, we need to make use of http header Cache-Control: max-age=0 . The client should have IAM permission to invalidate the cache. In case, we do not require any IAM permission for cache invalidation, then every client can invalide the cache, which is not expected.","title":"Caching"},{"location":"Notes/Serverless/03%20API%20Gateway/05%20API%20Gateway%20Monitoring%20And%20Logging/","text":"Monitoring We can integrate the cloudwatch logs at the stage levels and ApI basis. With X-Ray, the full picture of the api-gateway and lambda can be displayed Cloudwatch Metrics: CacheHitCount the more the better caching mechanism, (Number of request served from the cache) CacheMissCount the less the better caching mechanism (Number of requests served from the backend) Count Total number of API request at a certain time frame IntegrationLatency Time frame of sending request to lambda and getting response from lambda Latency Time to receive request in API Gateway and sending response from API Gateway Logging Two types of logging API Logging Typical logs, log streams, log groups Access Logging Can monitor lambda authorizer Who accessed How accessed Latency Two types of latency Latency: Time between receive a request from client and response back Integration Latency: Time between api-gateway to lambda request and response from lambda to api-gateway","title":"05 API Gateway Monitoring And Logging"},{"location":"Notes/Serverless/03%20API%20Gateway/05%20API%20Gateway%20Monitoring%20And%20Logging/#monitoring","text":"We can integrate the cloudwatch logs at the stage levels and ApI basis. With X-Ray, the full picture of the api-gateway and lambda can be displayed Cloudwatch Metrics: CacheHitCount the more the better caching mechanism, (Number of request served from the cache) CacheMissCount the less the better caching mechanism (Number of requests served from the backend) Count Total number of API request at a certain time frame IntegrationLatency Time frame of sending request to lambda and getting response from lambda Latency Time to receive request in API Gateway and sending response from API Gateway","title":"Monitoring"},{"location":"Notes/Serverless/03%20API%20Gateway/05%20API%20Gateway%20Monitoring%20And%20Logging/#logging","text":"Two types of logging API Logging Typical logs, log streams, log groups Access Logging Can monitor lambda authorizer Who accessed How accessed","title":"Logging"},{"location":"Notes/Serverless/03%20API%20Gateway/05%20API%20Gateway%20Monitoring%20And%20Logging/#latency","text":"Two types of latency Latency: Time between receive a request from client and response back Integration Latency: Time between api-gateway to lambda request and response from lambda to api-gateway","title":"Latency"},{"location":"Notes/Serverless/03%20API%20Gateway/07%20API%20Gateway%20Integration%20Types/","text":"Integration Types There are 4 types of integration type, can be used to integrate other services with API Gateway. HTTP or HTTP Custom Integration A custom http endpoint, need to set up mapping HTTP_PROXY Use for http endpoint, no needs for any mapping AWS (For Serverless/Lambda) AWS_PROXY (For Serverless/Lambda) MOCK simulates the api request and response With proxy integration, there is no need for any mapping and it is easier, it just redirect all the requests, parameters and payloads to the defined endpoint. API gateway can use mapping templates to transform request. For example, JSON to XML and vice versa. Suitable for legacy application with modern endpoint.","title":"07 API Gateway Integration Types"},{"location":"Notes/Serverless/03%20API%20Gateway/07%20API%20Gateway%20Integration%20Types/#integration-types","text":"There are 4 types of integration type, can be used to integrate other services with API Gateway. HTTP or HTTP Custom Integration A custom http endpoint, need to set up mapping HTTP_PROXY Use for http endpoint, no needs for any mapping AWS (For Serverless/Lambda) AWS_PROXY (For Serverless/Lambda) MOCK simulates the api request and response With proxy integration, there is no need for any mapping and it is easier, it just redirect all the requests, parameters and payloads to the defined endpoint. API gateway can use mapping templates to transform request. For example, JSON to XML and vice versa. Suitable for legacy application with modern endpoint.","title":"Integration Types"},{"location":"Notes/Serverless/04%20SAM/01%20SAM%20Overview/","text":"OVERVIEW Serverless Application Model Framework for developing and deploying serverless applications All config is in the YAML format Lambda function DynamoDB table API Gateway Cognito User Pool SAM help to run followings locally API Gateway DynamoDB Table SAM help to integrate Code Deploy , so Lambda Functions can deploy easily Allow test the Lambda function locally Can invoke function and events locally SAM Templates can be used to test the app through before deploy Using SAM Built In Code Deploy , application can be deployed to the cloud SAM can run locally using SAM CLI + AWS ToolKit for local testing and debugging SAM Policy Templates : To access resource, we need to add IAM roles to the Lambda, SAM Policy Templates can be used instead Running SAM locally In AWS, a lambda has associated role and aws context to invoke other services In local, to allow access other services from lambda First create a profile When invoke, use the profile as parameter SAM & CodeDeploy CodeDeploy is integrated closely with the SAM Anytime we update a Lambda function, the CodeDeploy play the role of deployment Properties AutoPublishAlias : Detect when new code is being updated DeploymentPreference : Determine how the code deployment will work Linear Canary All At Once Alarms: To trigger rollback Hooks: Can run before and after traffic shifting in the new Lambda function SAM Local Capabilities We can start and invoke Lambda function sam local start-lambda for starting a local endpoint sam local invoke to invoke a lambda with payload invoke is for run once and then quit while start-lambda keeps the endpoint open for function invocation sam local start-api for run local HTTP server for the API Gateway sam local generate-event for generate sample event sources from services like S3, SNS, SQS etc SAR Serverless application repository Used to store SAM app in the repository, so we can use Publicly Defined Account Use for reduce code duplication using Environment variables Directly mention","title":"01 SAM Overview"},{"location":"Notes/Serverless/04%20SAM/01%20SAM%20Overview/#overview","text":"Serverless Application Model Framework for developing and deploying serverless applications All config is in the YAML format Lambda function DynamoDB table API Gateway Cognito User Pool SAM help to run followings locally API Gateway DynamoDB Table SAM help to integrate Code Deploy , so Lambda Functions can deploy easily Allow test the Lambda function locally Can invoke function and events locally SAM Templates can be used to test the app through before deploy Using SAM Built In Code Deploy , application can be deployed to the cloud SAM can run locally using SAM CLI + AWS ToolKit for local testing and debugging SAM Policy Templates : To access resource, we need to add IAM roles to the Lambda, SAM Policy Templates can be used instead","title":"OVERVIEW"},{"location":"Notes/Serverless/04%20SAM/01%20SAM%20Overview/#running-sam-locally","text":"In AWS, a lambda has associated role and aws context to invoke other services In local, to allow access other services from lambda First create a profile When invoke, use the profile as parameter","title":"Running SAM locally"},{"location":"Notes/Serverless/04%20SAM/01%20SAM%20Overview/#sam-codedeploy","text":"CodeDeploy is integrated closely with the SAM Anytime we update a Lambda function, the CodeDeploy play the role of deployment Properties AutoPublishAlias : Detect when new code is being updated DeploymentPreference : Determine how the code deployment will work Linear Canary All At Once Alarms: To trigger rollback Hooks: Can run before and after traffic shifting in the new Lambda function","title":"SAM &amp; CodeDeploy"},{"location":"Notes/Serverless/04%20SAM/01%20SAM%20Overview/#sam-local-capabilities","text":"We can start and invoke Lambda function sam local start-lambda for starting a local endpoint sam local invoke to invoke a lambda with payload invoke is for run once and then quit while start-lambda keeps the endpoint open for function invocation sam local start-api for run local HTTP server for the API Gateway sam local generate-event for generate sample event sources from services like S3, SNS, SQS etc","title":"SAM Local Capabilities"},{"location":"Notes/Serverless/04%20SAM/01%20SAM%20Overview/#sar","text":"Serverless application repository Used to store SAM app in the repository, so we can use Publicly Defined Account Use for reduce code duplication using Environment variables Directly mention","title":"SAR"},{"location":"Notes/Serverless/04%20SAM/02%20SAM%20Properties/","text":"SAM Properties Transform : Use to specify the SAM version. Also, this key indicate, this is a SAM template and need to be transformed to the CloudFormation. Mappings : A literal for mapping keys and associated values, can be later used in parameters, tables lookup or condition Parameters : Used to refer values, can be passed during the runtime of the template Format Version : The cloudformation template version, to which the SAM template will be transformed SAM Resources Types AWS::Serverless::Application : Use to define nested application AWS::Serverless::Function : Use to define Lambda function AWS::Serverless::LayerVersion : Use to define Lambda layer version (Runtime/library) AWS::Serverless::API : Use to define Api Gateway AWS::Serverless::HttpApi : AWS::Serverless::SimpleTable : Use to define DynamoDB Table AWS::Serverless::StateMachine","title":"02 SAM Properties"},{"location":"Notes/Serverless/04%20SAM/02%20SAM%20Properties/#sam-properties","text":"Transform : Use to specify the SAM version. Also, this key indicate, this is a SAM template and need to be transformed to the CloudFormation. Mappings : A literal for mapping keys and associated values, can be later used in parameters, tables lookup or condition Parameters : Used to refer values, can be passed during the runtime of the template Format Version : The cloudformation template version, to which the SAM template will be transformed","title":"SAM Properties"},{"location":"Notes/Serverless/04%20SAM/02%20SAM%20Properties/#sam-resources-types","text":"AWS::Serverless::Application : Use to define nested application AWS::Serverless::Function : Use to define Lambda function AWS::Serverless::LayerVersion : Use to define Lambda layer version (Runtime/library) AWS::Serverless::API : Use to define Api Gateway AWS::Serverless::HttpApi : AWS::Serverless::SimpleTable : Use to define DynamoDB Table AWS::Serverless::StateMachine","title":"SAM Resources Types"},{"location":"Notes/Serverless/04%20SAM/03%20SAM%20Deployments/","text":"SAM Deployments SAM usage Cloudformation as the underlying deployment mechanism. After develop and test locally, we can deploy code using sam package (Includes in the sam package ) sam deploy Deployment commands, sam init : Initialize app using sam template sam build : Created a deployment ready build directory and resolve dependencies , specially local codeURI (Convert to CloudFormation) sam package : create zip, upload s3, create packaged template sam deploy : deploy sam publish : take packaged template and publish Developer perspective After creating a SAM template as a developer, deploy it by the following process, Build in local environment Package the application Deploy it to the S3 SAM has the built in codeDeploy for helping the safe lambda deployments. Canary : A certain amount of defined traffic will go to new function for a defined time. If everything goes fine, all traffic will shift to the new function Linear : Traffic is shifted in equal increments with an equal number of minutes between each increment. All-at-once : All traffic will be shifted to the new lambda functions. Canary Example Canary10Percent10Minutes : Move 10 percent if the traffic immediately to the new version. After 10 minutes, all traffic is shifted to the new version. CodeDeployDefault.LambdaCanary10Percent5Minutes : Move 10 percent traffic to new lambda function and then all the traffic will be shifted to the new lambda function. Linear Example CodeDeployDefault.LambdaLinear10PercentEvery1Minute Will redirect 10 percent of traffics each minutes and in 10 minutes all traffics will be shifted. CodeDeployDefault.LambdaLinear10PercentEvery2Minutes Will redirect 10 percent of traffics each 2 minutes and in 20 minutes all traffics will be shifted.","title":"03 SAM Deployments"},{"location":"Notes/Serverless/04%20SAM/03%20SAM%20Deployments/#sam-deployments","text":"SAM usage Cloudformation as the underlying deployment mechanism. After develop and test locally, we can deploy code using sam package (Includes in the sam package ) sam deploy Deployment commands, sam init : Initialize app using sam template sam build : Created a deployment ready build directory and resolve dependencies , specially local codeURI (Convert to CloudFormation) sam package : create zip, upload s3, create packaged template sam deploy : deploy sam publish : take packaged template and publish","title":"SAM Deployments"},{"location":"Notes/Serverless/04%20SAM/03%20SAM%20Deployments/#developer-perspective","text":"After creating a SAM template as a developer, deploy it by the following process, Build in local environment Package the application Deploy it to the S3 SAM has the built in codeDeploy for helping the safe lambda deployments. Canary : A certain amount of defined traffic will go to new function for a defined time. If everything goes fine, all traffic will shift to the new function Linear : Traffic is shifted in equal increments with an equal number of minutes between each increment. All-at-once : All traffic will be shifted to the new lambda functions. Canary Example Canary10Percent10Minutes : Move 10 percent if the traffic immediately to the new version. After 10 minutes, all traffic is shifted to the new version. CodeDeployDefault.LambdaCanary10Percent5Minutes : Move 10 percent traffic to new lambda function and then all the traffic will be shifted to the new lambda function. Linear Example CodeDeployDefault.LambdaLinear10PercentEvery1Minute Will redirect 10 percent of traffics each minutes and in 10 minutes all traffics will be shifted. CodeDeployDefault.LambdaLinear10PercentEvery2Minutes Will redirect 10 percent of traffics each 2 minutes and in 20 minutes all traffics will be shifted.","title":"Developer perspective"},{"location":"Notes/Serverless/04%20SAM/04%20SAM%20vs%20Serverless%20Application%20Framework/","text":"SAM vs Serverless Application Framework Both are open source, popular framework for building serverless solution. SAM is native solution for AWS whether serverless is developed and maintained by 3rd party provider SAM is only applicable for AWS, whereas serverless can be used to AZURE or GCP also","title":"04 SAM vs Serverless Application Framework"},{"location":"Notes/Serverless/04%20SAM/04%20SAM%20vs%20Serverless%20Application%20Framework/#sam-vs-serverless-application-framework","text":"Both are open source, popular framework for building serverless solution. SAM is native solution for AWS whether serverless is developed and maintained by 3rd party provider SAM is only applicable for AWS, whereas serverless can be used to AZURE or GCP also","title":"SAM vs Serverless Application Framework"},{"location":"Notes/Solution%20Architect%20Misc/01%20Caching%20Strategies/","text":"Caching Strategies Cloudfront Edge level caching Closest to the user API Gateway Region level caching DB Caching ( Application Level) Redis ( Elastic Cache ) Memcached ( Elastic Cache ) DAX","title":"01 Caching Strategies"},{"location":"Notes/Solution%20Architect%20Misc/01%20Caching%20Strategies/#caching-strategies","text":"Cloudfront Edge level caching Closest to the user API Gateway Region level caching DB Caching ( Application Level) Redis ( Elastic Cache ) Memcached ( Elastic Cache ) DAX","title":"Caching Strategies"},{"location":"Notes/Solution%20Architect%20Misc/02%20Blocking%20IP%20Address/","text":"Blocking IP Address For EC2 instance NACL in Subnet Level Security Group in Instance Level Run Firewall Software in EC2 instance This includes CPU cost When using a ALB NACL in Subnet Level Security Group in ALB Security Group does not work in the instance level, it only shows the ALB IP WAF in ALB Can be used for IP Filtering When using a NLB NACL in Subnet Level Security Group in Instance Level Run Firewall Software in EC2 instance This includes CPU cost When using a Cloudfront NACL and Security Group does not work here NACL and Security Group only sees the Cloudfront IP and ALB IP respectively Cloudfront Geo Restriction WAF in Cloudfront Can be used for IP Filtering","title":"02 Blocking IP Address"},{"location":"Notes/Solution%20Architect%20Misc/02%20Blocking%20IP%20Address/#blocking-ip-address","text":"For EC2 instance NACL in Subnet Level Security Group in Instance Level Run Firewall Software in EC2 instance This includes CPU cost When using a ALB NACL in Subnet Level Security Group in ALB Security Group does not work in the instance level, it only shows the ALB IP WAF in ALB Can be used for IP Filtering When using a NLB NACL in Subnet Level Security Group in Instance Level Run Firewall Software in EC2 instance This includes CPU cost When using a Cloudfront NACL and Security Group does not work here NACL and Security Group only sees the Cloudfront IP and ALB IP respectively Cloudfront Geo Restriction WAF in Cloudfront Can be used for IP Filtering","title":"Blocking IP Address"},{"location":"Notes/Solution%20Architect%20Misc/03%20HPC/","text":"HPC HPC stands for High Performance Computing Data Transfer and Management AWS Direct Connect (Move data GB/s) Snowball / Snow mobile (Move PB Scale data) AWS Data Sync (Move Data from on premise to s3, EFS, FSx) Compute And Networking EC2 instance, that are CPU Optimized and GPU Optimized Spot Instance for huge EC2 Fleet with very low cost EC2 Placement Group for good network Performance Using ENA , EFA Storage EBS Instance Store S3 EFS FSx for Luster","title":"03 HPC"},{"location":"Notes/Solution%20Architect%20Misc/03%20HPC/#hpc","text":"HPC stands for High Performance Computing Data Transfer and Management AWS Direct Connect (Move data GB/s) Snowball / Snow mobile (Move PB Scale data) AWS Data Sync (Move Data from on premise to s3, EFS, FSx) Compute And Networking EC2 instance, that are CPU Optimized and GPU Optimized Spot Instance for huge EC2 Fleet with very low cost EC2 Placement Group for good network Performance Using ENA , EFA Storage EBS Instance Store S3 EFS FSx for Luster","title":"HPC"},{"location":"Notes/Solution%20Architect%20Misc/04%20ENA%20%26%20EFA/","text":"ENA And EFA ENA stands for Elastic Network Adapter EFA stands for Elastic Fabric Adapter EFA is an improved version of ENA and only works with linux Used for Enhanced Networking i.e. SRIOV Higher Bandwidth Higher PPS (Packet Per Second) High performance inter instance performance EFA special use case Leverage MPI , i.e. Message Passing Interface This bypass underlying Linus OS for low latency","title":"04 ENA & EFA"},{"location":"Notes/Solution%20Architect%20Misc/04%20ENA%20%26%20EFA/#ena-and-efa","text":"ENA stands for Elastic Network Adapter EFA stands for Elastic Fabric Adapter EFA is an improved version of ENA and only works with linux Used for Enhanced Networking i.e. SRIOV Higher Bandwidth Higher PPS (Packet Per Second) High performance inter instance performance EFA special use case Leverage MPI , i.e. Message Passing Interface This bypass underlying Linus OS for low latency","title":"ENA And EFA"},{"location":"Notes/Solution%20Architect%20Misc/05%20Automation%20%26%20Orchestration/","text":"Automation & Orchestration AWS Batch Schedule Jobs AWS Parallel Cluster Open source cluster management tools","title":"05 Automation & Orchestration"},{"location":"Notes/Solution%20Architect%20Misc/05%20Automation%20%26%20Orchestration/#automation-orchestration","text":"AWS Batch Schedule Jobs AWS Parallel Cluster Open source cluster management tools","title":"Automation &amp; Orchestration"},{"location":"Notes/Solution%20Architect%20Misc/06%20Highly%20Available%20EC2%20Instance/","text":"Highly Available EC2 Instance // TODO","title":"06 Highly Available EC2 Instance"},{"location":"Notes/Solution%20Architect%20Misc/06%20Highly%20Available%20EC2%20Instance/#highly-available-ec2-instance","text":"// TODO","title":"Highly Available EC2 Instance"},{"location":"Notes/Solution%20Architect%20Misc/07%20Bastion%20Host%20HA/","text":"Bastion Host HA // TODO","title":"07 Bastion Host HA"},{"location":"Notes/Solution%20Architect%20Misc/07%20Bastion%20Host%20HA/#bastion-host-ha","text":"// TODO","title":"Bastion Host HA"},{"location":"Notes/Study%20Point/01%20README/","text":"ALB is not a regional service NLB does not support custom security policy consists of Protocols and ciphers Terminate TLS connection in NLB Require one certificate for each TLS connection to encrypt traffic between client and NLB AWS Certificate manager can be used, since it it automatically renew on expiry CLB (Classic load balancer) Supports the ASG AWS well architect framework includes Monitoring and alerts using Cloudtrail and Cloudwatch Spread EC2 Instances across multiple AZ When web distribution falls under PCI distribution Enable Cloudfront Logs Capture request, sent to the Cloudfront API AWS Public Dataset like satellite imagery, geospatial, genomic is free, need no charge RDP aka Remote Desktop Protocol use port 3389","title":"01 README"},{"location":"Notes/VPC/01%20CIDR/","text":"CIDR Classless Inter-Domain Routing Used in Security groups AWS networking Help define IP Range Has two components Base IP (XX.XX.XX.XX) IP that contains in the range Subnet Mask (/XX) Determine how many bits can be change in the IP Subnet Mask Calculate IP Ranges Formula (2^(32-subnetMaskNumber)) Some Examples /32 allows 1 IP = 2^0 /31 allows 2 IP = 2^1 /30 allows 4 IP = 2^2 /29 allows 8 IP = 2^3 /28 allows 16 IP = 2^4 /27 allows 32 IP = 2^5 /26 allows 64 IP = 2^6 /25 allows 128 IP = 2^7 /24 allows 256 IP = 2^8 /16 allows 65,536 IP = 2^16 /0 allows ALL IP = 2^32 Quick Memo /32 - no IP number can change /24 - last IP number can change /16 - last IP two numbers can change /8 - last IP three numbers can change /0 - All IP numbers can change Exercise Find range 192.168.0.0/24 Last IP number can change Range be 192.168.0.0 to 192.168.0.255 ( 256 IP) Alternate 2^(32-24) = 256 IP Find range 192.168.0.0/16 Last 2 IP number can change Range be 192.168.0.0 to 192.168.255.255 ( 65,536 IP) Alternate 2^(32-16) = 65,536 IP Find range 134.56.78.123/32 No number can change Range be 134.56.78.123 to 134.56.78.123 ( 1 IP) Alternate 2^(32-32) = 1 IP Find range 0.0.0.0/32 All number can change Range be 0.0.0.0 to 255.255.255.255 ( ALL IP) Alternate 2^(32-0) = 2^32 IP = ALL IP Private IP Big Network Range 10.0.0.0 - 10.255.255.255 and CIDR ( 10.0.0.0/8 ) AWS Default Range 172.16.0.0 - 172.31.255.255 and CIDR ( 172.16.0.0/12 ) Home Network Range 192.168.0.0 - 192.168.255.255 and CIDR ( 192.168.0.0/16 ) Public IP Any IP but Private IP","title":"01 CIDR"},{"location":"Notes/VPC/01%20CIDR/#cidr","text":"Classless Inter-Domain Routing Used in Security groups AWS networking Help define IP Range Has two components Base IP (XX.XX.XX.XX) IP that contains in the range Subnet Mask (/XX) Determine how many bits can be change in the IP","title":"CIDR"},{"location":"Notes/VPC/01%20CIDR/#subnet-mask","text":"Calculate IP Ranges Formula (2^(32-subnetMaskNumber)) Some Examples /32 allows 1 IP = 2^0 /31 allows 2 IP = 2^1 /30 allows 4 IP = 2^2 /29 allows 8 IP = 2^3 /28 allows 16 IP = 2^4 /27 allows 32 IP = 2^5 /26 allows 64 IP = 2^6 /25 allows 128 IP = 2^7 /24 allows 256 IP = 2^8 /16 allows 65,536 IP = 2^16 /0 allows ALL IP = 2^32 Quick Memo /32 - no IP number can change /24 - last IP number can change /16 - last IP two numbers can change /8 - last IP three numbers can change /0 - All IP numbers can change Exercise Find range 192.168.0.0/24 Last IP number can change Range be 192.168.0.0 to 192.168.0.255 ( 256 IP) Alternate 2^(32-24) = 256 IP Find range 192.168.0.0/16 Last 2 IP number can change Range be 192.168.0.0 to 192.168.255.255 ( 65,536 IP) Alternate 2^(32-16) = 65,536 IP Find range 134.56.78.123/32 No number can change Range be 134.56.78.123 to 134.56.78.123 ( 1 IP) Alternate 2^(32-32) = 1 IP Find range 0.0.0.0/32 All number can change Range be 0.0.0.0 to 255.255.255.255 ( ALL IP) Alternate 2^(32-0) = 2^32 IP = ALL IP","title":"Subnet Mask"},{"location":"Notes/VPC/01%20CIDR/#private-ip","text":"Big Network Range 10.0.0.0 - 10.255.255.255 and CIDR ( 10.0.0.0/8 ) AWS Default Range 172.16.0.0 - 172.31.255.255 and CIDR ( 172.16.0.0/12 ) Home Network Range 192.168.0.0 - 192.168.255.255 and CIDR ( 192.168.0.0/16 )","title":"Private IP"},{"location":"Notes/VPC/01%20CIDR/#public-ip","text":"Any IP but Private IP","title":"Public IP"},{"location":"Notes/VPC/02%20Default%20VPC/","text":"Default VPC Properties All new account have default VPC If not specified, new instances are launched in Default VPC Has internet connectivity Each new instance is configured as Have public IP Have Public and Private DNS name Default VPC has following components Subnets Route Tables Internet Gateway Network ACL","title":"02 Default VPC"},{"location":"Notes/VPC/02%20Default%20VPC/#default-vpc","text":"Properties All new account have default VPC If not specified, new instances are launched in Default VPC Has internet connectivity Each new instance is configured as Have public IP Have Public and Private DNS name Default VPC has following components Subnets Route Tables Internet Gateway Network ACL","title":"Default VPC"},{"location":"Notes/VPC/04%20VPC%20Overview/","text":"VPC Virtual Private Cloud Max 5 VPC in per region (Soft Limit) Max 5 CIDR Each CIDR Min Size /28 = 16 IP address Maz Size /16 = 65536 IP Address VPC is Private , so Private IP Ranges are Big Network Range 10.0.0.0 - 10.255.255.255 and CIDR ( 10.0.0.0/8 ) AWS Default Range 172.16.0.0 - 172.31.255.255 and CIDR ( 172.16.0.0/12 ) Home Network Range 192.168.0.0 - 192.168.255.255 and CIDR ( 192.168.0.0/16 ) CIDR should not overlap other networks AZ name varies from Account to Account us-east-1a can be different AZ to another account AWS randomize the AZ name When IP Address is not available in defined CIDR range, can be added Secondary CIDR","title":"04 VPC Overview"},{"location":"Notes/VPC/04%20VPC%20Overview/#vpc","text":"Virtual Private Cloud Max 5 VPC in per region (Soft Limit) Max 5 CIDR Each CIDR Min Size /28 = 16 IP address Maz Size /16 = 65536 IP Address VPC is Private , so Private IP Ranges are Big Network Range 10.0.0.0 - 10.255.255.255 and CIDR ( 10.0.0.0/8 ) AWS Default Range 172.16.0.0 - 172.31.255.255 and CIDR ( 172.16.0.0/12 ) Home Network Range 192.168.0.0 - 192.168.255.255 and CIDR ( 192.168.0.0/16 ) CIDR should not overlap other networks AZ name varies from Account to Account us-east-1a can be different AZ to another account AWS randomize the AZ name When IP Address is not available in defined CIDR range, can be added Secondary CIDR","title":"VPC"},{"location":"Notes/VPC/05%20Subnet%20Overview/","text":"Subnet Subnets are tied to specific AZ Multiple Subnets can be provisioned in one AZ One Subnet can not be provisioned in multi-AZ Two types of Subnets Public Subnet use to put Load balancer Private Subnet use to put Applications and DB Servers Every time create a Subnet , loose 5 IP address. First 4 and the last one First one for Network Address Second one for VPC Router , reserved by AWS Third one for AWS Provided DNS , reserved by AWS Fourth one for future use, reserved by AWS Last one for Broadcast Address , although AWS does not support Broadcast Exercise Can /27 handle 29 IP Address ? It has 2^(32-27) = 32 IP Address Since 5 IP address is not usable, we can use 32-5 = 27 IP Address So /27 can not handle more than 27 IP Address To handle 29 IP Address we might need at least /26 i.e. 2^(32-26) = 64 IP Address In this case we can use 64-5 = 59 IP Address , that matches the requirements We can enable Auto Assign Public IPv4 Address features, so any instance being created within the subnet, will have a public IP. In Custom VPC , the feature Auto Assign Public IPv4 Address is disabled by default, whereas in Default VPC it is enabled by default. To ensure the Public Subnet can access the Private Subnet , need to ensure, the SG have the required rules defined to allow traffic Since internet and intranet is corresponding to separate AZ, need separate AZ for them","title":"05 Subnet Overview"},{"location":"Notes/VPC/05%20Subnet%20Overview/#subnet","text":"Subnets are tied to specific AZ Multiple Subnets can be provisioned in one AZ One Subnet can not be provisioned in multi-AZ Two types of Subnets Public Subnet use to put Load balancer Private Subnet use to put Applications and DB Servers Every time create a Subnet , loose 5 IP address. First 4 and the last one First one for Network Address Second one for VPC Router , reserved by AWS Third one for AWS Provided DNS , reserved by AWS Fourth one for future use, reserved by AWS Last one for Broadcast Address , although AWS does not support Broadcast Exercise Can /27 handle 29 IP Address ? It has 2^(32-27) = 32 IP Address Since 5 IP address is not usable, we can use 32-5 = 27 IP Address So /27 can not handle more than 27 IP Address To handle 29 IP Address we might need at least /26 i.e. 2^(32-26) = 64 IP Address In this case we can use 64-5 = 59 IP Address , that matches the requirements We can enable Auto Assign Public IPv4 Address features, so any instance being created within the subnet, will have a public IP. In Custom VPC , the feature Auto Assign Public IPv4 Address is disabled by default, whereas in Default VPC it is enabled by default. To ensure the Public Subnet can access the Private Subnet , need to ensure, the SG have the required rules defined to allow traffic Since internet and intranet is corresponding to separate AZ, need separate AZ for them","title":"Subnet"},{"location":"Notes/VPC/06%20Internet%20Gateway/","text":"Internet Gateway (IG) Use to provide Internet Connection to the VPC It is scalable, highly available and redundant One VPC can have one IG One IG can have one VPC IG work like NAT for instance, that has public IP Default VPC have an attached IG For Custom VPC need to create an IG and attach to the VPC To ensure the EC2 instance have internet connection over IG The route table should have a definition of route for IG The ACL should have inbound and outbound traffic allowence","title":"06 Internet Gateway"},{"location":"Notes/VPC/06%20Internet%20Gateway/#internet-gateway-ig","text":"Use to provide Internet Connection to the VPC It is scalable, highly available and redundant One VPC can have one IG One IG can have one VPC IG work like NAT for instance, that has public IP Default VPC have an attached IG For Custom VPC need to create an IG and attach to the VPC To ensure the EC2 instance have internet connection over IG The route table should have a definition of route for IG The ACL should have inbound and outbound traffic allowence","title":"Internet Gateway (IG)"},{"location":"Notes/VPC/07%20Route%20Table/","text":"Route Table By default there is a Main Route Table when VPC is created Every subnet is associated to the Main Route Table if it is not explicitly set up. By default all the VPC CIDR is routed to the local network The Public Subnet needs to add a route to the IG , if it is not communicating with VPC CIDR Route Table Target VPC Peering start with pcx- VPN Connection start with vgw- Direct Connect start with vgw- Secondary CIDR is local","title":"07 Route Table"},{"location":"Notes/VPC/07%20Route%20Table/#route-table","text":"By default there is a Main Route Table when VPC is created Every subnet is associated to the Main Route Table if it is not explicitly set up. By default all the VPC CIDR is routed to the local network The Public Subnet needs to add a route to the IG , if it is not communicating with VPC CIDR Route Table Target VPC Peering start with pcx- VPN Connection start with vgw- Direct Connect start with vgw- Secondary CIDR is local","title":"Route Table"},{"location":"Notes/VPC/08%20Internet%20Connection%20To%20Public%20Subnet/","text":"Internet Connection To Public Subnet Instance Make sure IG is attached to the VPC Make sure Public Subnet has a route towards IG The instance should have a public IP or elastic IP","title":"08 Internet Connection To Public Subnet"},{"location":"Notes/VPC/08%20Internet%20Connection%20To%20Public%20Subnet/#internet-connection-to-public-subnet-instance","text":"Make sure IG is attached to the VPC Make sure Public Subnet has a route towards IG The instance should have a public IP or elastic IP","title":"Internet Connection To Public Subnet Instance"},{"location":"Notes/VPC/09%20NAT%20Instance/","text":"NAT Instance Network Address Translation Allow Instances in the Private Subnet to access interner Must be launched in the Public Subnet Must be disabled Source and Destination check Need and Elastic IP (ENI) attached to the Nat Instance From the Private Route Table this IP be the target NAT Instance security rules: Allow HTTP from VPC CIDR Allow HTTPS from VPC CIDR Allow All ICMP - IPv4 from VPC CIDR for ping Private Route Table security rules Any connection outgoing to the internet (0.0.0.0/0) , be target to NAT Instance Cons of NAT Instance Not HA Not easy setup Elastic IP to make stable route Internet Traffic depends on EC2 performance ( Network Throughput )","title":"09 NAT Instance"},{"location":"Notes/VPC/09%20NAT%20Instance/#nat-instance","text":"Network Address Translation Allow Instances in the Private Subnet to access interner Must be launched in the Public Subnet Must be disabled Source and Destination check Need and Elastic IP (ENI) attached to the Nat Instance From the Private Route Table this IP be the target NAT Instance security rules: Allow HTTP from VPC CIDR Allow HTTPS from VPC CIDR Allow All ICMP - IPv4 from VPC CIDR for ping Private Route Table security rules Any connection outgoing to the internet (0.0.0.0/0) , be target to NAT Instance Cons of NAT Instance Not HA Not easy setup Elastic IP to make stable route Internet Traffic depends on EC2 performance ( Network Throughput )","title":"NAT Instance"},{"location":"Notes/VPC/10%20NAT%20Gateway/","text":"NAT Gateway Managed By AWS Pricing is based on hour and bandwidth Can not used by the instance of same Subnet (Only from other Subnets ) Require IG Things are Private Subnet -> NAT Gateway -> IG Scale from 5Gbps to 45Gbps No Security Group is required NAT Gateway is resilient withing a Single AZ . For fault-tolerance , require multiple NAT Gateway in Multiple AZ Private Route Table security rules Any connection outgoing to the internet (0.0.0.0/0) , be target to NAT Gateway In VPC Peering , Nat Gateway can not be shared. Need to use separate Nat Gateway","title":"10 NAT Gateway"},{"location":"Notes/VPC/10%20NAT%20Gateway/#nat-gateway","text":"Managed By AWS Pricing is based on hour and bandwidth Can not used by the instance of same Subnet (Only from other Subnets ) Require IG Things are Private Subnet -> NAT Gateway -> IG Scale from 5Gbps to 45Gbps No Security Group is required NAT Gateway is resilient withing a Single AZ . For fault-tolerance , require multiple NAT Gateway in Multiple AZ Private Route Table security rules Any connection outgoing to the internet (0.0.0.0/0) , be target to NAT Gateway In VPC Peering , Nat Gateway can not be shared. Need to use separate Nat Gateway","title":"NAT Gateway"},{"location":"Notes/VPC/11%20DNS%20Resolution/","text":"DNS Resolution in VPC Two settings enableDnsSupport It is DNS Resolution Setting Default is true Helps if DNS Resolution is supported for VPC If true , queries the AWS DNS Server at 169.254.169.253 enableDnsHostname It is DNS Hostname Setting Default value false in Custom VPC true in Default VPC If true , a public hostname assigned to the Instance If false , only IP assigned to the Instance","title":"11 DNS Resolution"},{"location":"Notes/VPC/11%20DNS%20Resolution/#dns-resolution-in-vpc","text":"Two settings enableDnsSupport It is DNS Resolution Setting Default is true Helps if DNS Resolution is supported for VPC If true , queries the AWS DNS Server at 169.254.169.253 enableDnsHostname It is DNS Hostname Setting Default value false in Custom VPC true in Default VPC If true , a public hostname assigned to the Instance If false , only IP assigned to the Instance","title":"DNS Resolution in VPC"},{"location":"Notes/VPC/12%20Route%2053%20Private%20Zone/","text":"Route 53 Private Zone Make sure following 2 VPC DNS Resolution is enabled enableDnsHostname is true enableDnsSupport is true Since it is Private Zone and accessible from VPC . we can create records that is not owned","title":"12 Route 53 Private Zone"},{"location":"Notes/VPC/12%20Route%2053%20Private%20Zone/#route-53-private-zone","text":"Make sure following 2 VPC DNS Resolution is enabled enableDnsHostname is true enableDnsSupport is true Since it is Private Zone and accessible from VPC . we can create records that is not owned","title":"Route 53 Private Zone"},{"location":"Notes/VPC/13%20NACL%20%26%20SG/","text":"NACL & SG NACL stands for Network Access Control List SG stands for Security Group SG associated with Instance NACL associated with Subnet SG is stateful If Inbound Rule allowed IP / IP Ranges , Outbound Rule is automatically allowed If Outbound Rule allowed IP / IP Ranges , Inbound Rule is automatically a llowed NACL stateless i.e. both Inbound Rule and Outbound Rule is separately evaluated NACL is evaluated Lowest number has high preference If no rules found, it goes to * numbered rule Default NACL allow every traffic for both Inbound and Outbound Custom NACL block every traffic for both Inbound and Outbound Each Subnet goes under Default NACL if not explicitly associated Ephemeral PORT should be open for highly restricted NACL To block an IP Address use NACL","title":"13 NACL & SG"},{"location":"Notes/VPC/13%20NACL%20%26%20SG/#nacl-sg","text":"NACL stands for Network Access Control List SG stands for Security Group SG associated with Instance NACL associated with Subnet SG is stateful If Inbound Rule allowed IP / IP Ranges , Outbound Rule is automatically allowed If Outbound Rule allowed IP / IP Ranges , Inbound Rule is automatically a llowed NACL stateless i.e. both Inbound Rule and Outbound Rule is separately evaluated NACL is evaluated Lowest number has high preference If no rules found, it goes to * numbered rule Default NACL allow every traffic for both Inbound and Outbound Custom NACL block every traffic for both Inbound and Outbound Each Subnet goes under Default NACL if not explicitly associated Ephemeral PORT should be open for highly restricted NACL To block an IP Address use NACL","title":"NACL &amp; SG"},{"location":"Notes/VPC/14%20VPC%20Peering/","text":"VPC Peering Connect two VPC using AWS network After VPC Peering communication between two VPC use AWS Network instead of Public Internet Two VPC can not be peered, if Have overlapping CIDR Any transitive peering with on-premise server Edge to Edge routing via a gateway Transitive peering A Peered Connection is to be created between two VPC Peered Connection can be established with an VPC in another Region and another Account ( inter-region , cross-account ) Can use Peered VPC SG reference Connection is not Transitive VPC A is peered to VPC B VPC B is peered to VPC C Does not imply VPC A is peered with VPC C Still we need to peer VPC A with VPC C explicitly Each Subnet Route Table of each peered VPC should be updated. Target of the Peered VPC CIDR should be the Peered Connection To establish VPC Peering Create VPC Peer Connection with own VPC and another VPC Accept the Peer Connection request Update Subnet Route Table for both VPC NAT Gateway can not be shared over VPC Peering","title":"14 VPC Peering"},{"location":"Notes/VPC/14%20VPC%20Peering/#vpc-peering","text":"Connect two VPC using AWS network After VPC Peering communication between two VPC use AWS Network instead of Public Internet Two VPC can not be peered, if Have overlapping CIDR Any transitive peering with on-premise server Edge to Edge routing via a gateway Transitive peering A Peered Connection is to be created between two VPC Peered Connection can be established with an VPC in another Region and another Account ( inter-region , cross-account ) Can use Peered VPC SG reference Connection is not Transitive VPC A is peered to VPC B VPC B is peered to VPC C Does not imply VPC A is peered with VPC C Still we need to peer VPC A with VPC C explicitly Each Subnet Route Table of each peered VPC should be updated. Target of the Peered VPC CIDR should be the Peered Connection To establish VPC Peering Create VPC Peer Connection with own VPC and another VPC Accept the Peer Connection request Update Subnet Route Table for both VPC NAT Gateway can not be shared over VPC Peering","title":"VPC Peering"},{"location":"Notes/VPC/15%20VPC%20Endpoints/","text":"VPC Endpoints Allow using AWS Service using Private Network from VPC No need of IG and NAT Gateway Two types of VPC Endpoints Gateway S3 and DynamoDB uses Gateway Provision ENI (i.e. Private IP Address ) as entry point Need SG Interface Provision Target and use Route Table Service, other than S3 and DynamoDB , uses Gateway To establish a VPC Endpoint Check DNS Resolution Check Route Table When there is VPC Endpoint and a Public Internet Connection , VPC Endpoint got priority. VPC Endpoint does not support cross region request From a single Route Table , can not have multiple VPC Endpoints of the same service. FYI, A service can have multiple endpoints To allow S3 to as VPC Endpoint Gateway , needs Endpoint Policy for trusted VPC","title":"15 VPC Endpoints"},{"location":"Notes/VPC/15%20VPC%20Endpoints/#vpc-endpoints","text":"Allow using AWS Service using Private Network from VPC No need of IG and NAT Gateway Two types of VPC Endpoints Gateway S3 and DynamoDB uses Gateway Provision ENI (i.e. Private IP Address ) as entry point Need SG Interface Provision Target and use Route Table Service, other than S3 and DynamoDB , uses Gateway To establish a VPC Endpoint Check DNS Resolution Check Route Table When there is VPC Endpoint and a Public Internet Connection , VPC Endpoint got priority. VPC Endpoint does not support cross region request From a single Route Table , can not have multiple VPC Endpoints of the same service. FYI, A service can have multiple endpoints To allow S3 to as VPC Endpoint Gateway , needs Endpoint Policy for trusted VPC","title":"VPC Endpoints"},{"location":"Notes/VPC/16%20Flow%20Logs/","text":"Flow Logs Capture IP Traffic and Network information Help monitor and troubleshoot connectivity issues Can be used for VPC level Subnet Level Elastic Network Interface level Store logs in S3 / Cloudwatch Logs Can query the logs using Athena in S3 logs Cloudwatch Log Insights in Cloudwatch Logs Can not enable Flow Logs of VPC that is Peered and belongs to Another Account After creating a flow log, configuration can not be changed For example, can not change IAM Roles Following IP Traffic does not monitor AWS DNS server Traffic of Windows Instance activating licence Traffic of Instance Metadata (169.254.169.254) DHCP traffic Reserved IP Address of Default VPC Router","title":"16 Flow Logs"},{"location":"Notes/VPC/16%20Flow%20Logs/#flow-logs","text":"Capture IP Traffic and Network information Help monitor and troubleshoot connectivity issues Can be used for VPC level Subnet Level Elastic Network Interface level Store logs in S3 / Cloudwatch Logs Can query the logs using Athena in S3 logs Cloudwatch Log Insights in Cloudwatch Logs Can not enable Flow Logs of VPC that is Peered and belongs to Another Account After creating a flow log, configuration can not be changed For example, can not change IAM Roles Following IP Traffic does not monitor AWS DNS server Traffic of Windows Instance activating licence Traffic of Instance Metadata (169.254.169.254) DHCP traffic Reserved IP Address of Default VPC Router","title":"Flow Logs"},{"location":"Notes/VPC/17%20Bastion%20Host/","text":"Bastion Host Use to SSH to the Private Instance Launched in the Public Subnet Security rules should be strict Only PORT 22 be opened for SSH Only My IP should allow Supports TCP","title":"17 Bastion Host"},{"location":"Notes/VPC/17%20Bastion%20Host/#bastion-host","text":"Use to SSH to the Private Instance Launched in the Public Subnet Security rules should be strict Only PORT 22 be opened for SSH Only My IP should allow Supports TCP","title":"Bastion Host"},{"location":"Notes/VPC/18%20Site%20To%20Site%20VPN/","text":"Site To Site VPN Connect Corporate Data Center with AWS Cloud Seems they are both part of same network Traffic between Corporate Data Center with AWS Cloud goes over Public Internet To set up Site To Site VPN Set a Customer Gateway in the Corporate Data Center Set a Virtual Private Network Gateway (i.e. VPG , i.e. VPN Gateway ) in AWS VPC In between Customer Gateway and VPN Gateway , provision a Site To Site VPN Connection Customer Gateway Set in Corporate DC (i.e. Corporate Data Center ) IP Address can be one of followings Static IP If behind NAT , use NAT public address Virtual Private Gateway i.e. VPN Gateway / VPG VPN Concentrator in the AWS side of the VPN Connection VGW is created and attached to VPC Possible to customize Autonomous System Number i.e. ( ASN ) To improve performance need to use ECMP protocol This protocol be enabled in VGW Need to implement this for each VPN Tunnel Result faster data transfers","title":"18 Site To Site VPN"},{"location":"Notes/VPC/18%20Site%20To%20Site%20VPN/#site-to-site-vpn","text":"Connect Corporate Data Center with AWS Cloud Seems they are both part of same network Traffic between Corporate Data Center with AWS Cloud goes over Public Internet To set up Site To Site VPN Set a Customer Gateway in the Corporate Data Center Set a Virtual Private Network Gateway (i.e. VPG , i.e. VPN Gateway ) in AWS VPC In between Customer Gateway and VPN Gateway , provision a Site To Site VPN Connection Customer Gateway Set in Corporate DC (i.e. Corporate Data Center ) IP Address can be one of followings Static IP If behind NAT , use NAT public address Virtual Private Gateway i.e. VPN Gateway / VPG VPN Concentrator in the AWS side of the VPN Connection VGW is created and attached to VPC Possible to customize Autonomous System Number i.e. ( ASN ) To improve performance need to use ECMP protocol This protocol be enabled in VGW Need to implement this for each VPN Tunnel Result faster data transfers","title":"Site To Site VPN"},{"location":"Notes/VPC/19%20Direct%20Connect/","text":"Direct Connect Provide Dedicated Private Connection from Remote Network to VPC Need to establish physical connection between Corporate Data Center and AWS Direct Connect Location Corporate Data Center => AWS Direct Connect Location => VPC In VPC a Virtual Private Gateway is required DC allow both Public and Private resources Supports IPv4 and IPv6 Two types of DC available Dedicated Connection Bandwidth be 1Gbps to 10Gbps A dedicated physical ethernet port will be available in AWS for the customer To establish Dedicated Connection , first request AWS and complete by AWS Direct Connect Partners Hosted Connection To establish Hosted Connection , don't need to request AWS , can be completed by AWS Direct Connect Partners Hosted Connection Direct Connect is a time consuming process (approximately 1 month) Encryption Data In Transit is not encrypted Data In Transit is private between Corporate Data Center , Direct Connect Partner and AWS Cloud Encryption can be established by IPsec-encrypted private connection IPsec-encryption is combination of Direct Connect and VPN This VPN is between Corporate Data Center and AWS Direct Connect Partner Used for Increase bandwidth throughput Working with large data set lower bandwidth cost Consistent network to avoid data drops connection shut down real time data feeds Hybrid environments ( On Premise Data Center and Cloud Data Center ) LAG can be used to aggregate maximum throughput","title":"19 Direct Connect"},{"location":"Notes/VPC/19%20Direct%20Connect/#direct-connect","text":"Provide Dedicated Private Connection from Remote Network to VPC Need to establish physical connection between Corporate Data Center and AWS Direct Connect Location Corporate Data Center => AWS Direct Connect Location => VPC In VPC a Virtual Private Gateway is required DC allow both Public and Private resources Supports IPv4 and IPv6 Two types of DC available Dedicated Connection Bandwidth be 1Gbps to 10Gbps A dedicated physical ethernet port will be available in AWS for the customer To establish Dedicated Connection , first request AWS and complete by AWS Direct Connect Partners Hosted Connection To establish Hosted Connection , don't need to request AWS , can be completed by AWS Direct Connect Partners Hosted Connection Direct Connect is a time consuming process (approximately 1 month) Encryption Data In Transit is not encrypted Data In Transit is private between Corporate Data Center , Direct Connect Partner and AWS Cloud Encryption can be established by IPsec-encrypted private connection IPsec-encryption is combination of Direct Connect and VPN This VPN is between Corporate Data Center and AWS Direct Connect Partner Used for Increase bandwidth throughput Working with large data set lower bandwidth cost Consistent network to avoid data drops connection shut down real time data feeds Hybrid environments ( On Premise Data Center and Cloud Data Center ) LAG can be used to aggregate maximum throughput","title":"Direct Connect"},{"location":"Notes/VPC/20%20Direct%20Connect%20Gateway/","text":"Direct Connect Gateway To establish DC with multiple VPC use DCG DCW does not establish VPC Peering VPC can be in Different Region but Same Account Connected VPC CIDR can not be Overlapped","title":"20 Direct Connect Gateway"},{"location":"Notes/VPC/20%20Direct%20Connect%20Gateway/#direct-connect-gateway","text":"To establish DC with multiple VPC use DCG DCW does not establish VPC Peering VPC can be in Different Region but Same Account Connected VPC CIDR can not be Overlapped","title":"Direct Connect Gateway"},{"location":"Notes/VPC/21%20Site%20to%20Site%20VPN%20vs%20DC%20vs%20DCG/","text":"Site To Site VPN vs DC vs DC Gateway Site To Site VPN Corporate Data Center to AWS Cloud Use Public Internet In VPC use VPG In Customer Data Center use Customer Gateway DC Corporate Data Center to AWS Direct Connect Partner to AWS Cloud Use Physical Connection from Corporate Data Center to AWS Direct Connect Partner Use Private Internet from AWS Direct Connect Partner to AWS Cloud In VPC use VPG DC Gateway If Direct Connect is required with multiple VPC , use DC Gateway","title":"21 Site to Site VPN vs DC vs DCG"},{"location":"Notes/VPC/21%20Site%20to%20Site%20VPN%20vs%20DC%20vs%20DCG/#site-to-site-vpn-vs-dc-vs-dc-gateway","text":"Site To Site VPN Corporate Data Center to AWS Cloud Use Public Internet In VPC use VPG In Customer Data Center use Customer Gateway DC Corporate Data Center to AWS Direct Connect Partner to AWS Cloud Use Physical Connection from Corporate Data Center to AWS Direct Connect Partner Use Private Internet from AWS Direct Connect Partner to AWS Cloud In VPC use VPG DC Gateway If Direct Connect is required with multiple VPC , use DC Gateway","title":"Site To Site VPN vs DC vs DC Gateway"},{"location":"Notes/VPC/22%20Egress%20Only%20Internet%20Gateway/","text":"Egress Only Internet Gateway Same as NAT Gateway , but for IPv6 Egress means outgoing Egress Only Internet Gateway Allow traffic to go outside Deny any traffic coming inside Very much like NAT Instance / NAT Gateway But only works for IPv6 Uses IPv6 does not have Private Address When we provision EC2 Instance with IPv6 Address in Private Subnet Since IPv6 does not have Private Address , it will get a IPv6 Public Address To restrict coming internet from outside use Egress Only Internet Gateway","title":"22 Egress Only Internet Gateway"},{"location":"Notes/VPC/22%20Egress%20Only%20Internet%20Gateway/#egress-only-internet-gateway","text":"Same as NAT Gateway , but for IPv6 Egress means outgoing Egress Only Internet Gateway Allow traffic to go outside Deny any traffic coming inside Very much like NAT Instance / NAT Gateway But only works for IPv6 Uses IPv6 does not have Private Address When we provision EC2 Instance with IPv6 Address in Private Subnet Since IPv6 does not have Private Address , it will get a IPv6 Public Address To restrict coming internet from outside use Egress Only Internet Gateway","title":"Egress Only Internet Gateway"},{"location":"Notes/VPC/23%20AWS%20Private%20Link/","text":"AWS Private Link Also known as VPC Endpoint Service Ideal when, a Service needs to be exposed from a VPC to multiple VPC Problem Scenario In my VPC , I have a Web service Need to expose that service to other VPC Possible solution be Make Web Service Public This is a security hazard Use VPC peering Need to update route table Other services will be accessible as well To establish Private Link Create a Network Load Balancer in Service VPC Create a ENI in the Customer VPC Connect NLB with ENI using the AWS Private Link To make it scalable Launch NLB in multi-AZ Create ENI in multi-AZ Private Link can be used with inter-region VPC Peering","title":"23 AWS Private Link"},{"location":"Notes/VPC/23%20AWS%20Private%20Link/#aws-private-link","text":"Also known as VPC Endpoint Service Ideal when, a Service needs to be exposed from a VPC to multiple VPC Problem Scenario In my VPC , I have a Web service Need to expose that service to other VPC Possible solution be Make Web Service Public This is a security hazard Use VPC peering Need to update route table Other services will be accessible as well To establish Private Link Create a Network Load Balancer in Service VPC Create a ENI in the Customer VPC Connect NLB with ENI using the AWS Private Link To make it scalable Launch NLB in multi-AZ Create ENI in multi-AZ Private Link can be used with inter-region VPC Peering","title":"AWS Private Link"},{"location":"Notes/VPC/24%20Classic%20Link/","text":"AWS Classic Link (Deprecated) It is distractor option in AWS Exam Try to avoid this option in MCQ Allow linking with EC2-Classic instance from VPC EC2-Classic instances are in a single network, shared with customer (Used before VPC )","title":"24 Classic Link"},{"location":"Notes/VPC/24%20Classic%20Link/#aws-classic-link-deprecated","text":"It is distractor option in AWS Exam Try to avoid this option in MCQ Allow linking with EC2-Classic instance from VPC EC2-Classic instances are in a single network, shared with customer (Used before VPC )","title":"AWS Classic Link (Deprecated)"},{"location":"Notes/VPC/25%20AWS%20Cloud%20Hub/","text":"AWS Cloud Hub Allow secure connection (through VPN ) among multiple Corporate Data Centers and AWS Cloud To establish this connection Create a Cloud Hub in AWS Connect Corporate Data Centers with this Cloud Hub Since this is VPN connection Use Public Internet Can use Encryption This is a low-cost Hub and Spoke Model","title":"25 AWS Cloud Hub"},{"location":"Notes/VPC/25%20AWS%20Cloud%20Hub/#aws-cloud-hub","text":"Allow secure connection (through VPN ) among multiple Corporate Data Centers and AWS Cloud To establish this connection Create a Cloud Hub in AWS Connect Corporate Data Centers with this Cloud Hub Since this is VPN connection Use Public Internet Can use Encryption This is a low-cost Hub and Spoke Model","title":"AWS Cloud Hub"},{"location":"Notes/VPC/26%20Transit%20Gateway/","text":"Transit Gateway Simplify network topology Star Gateway (Hub and Spoke Model) for VPC and On Premise Network Using Route Table we can define which VPC can talk to which VPC Can setup Direct Connect VPN Connection Support IP Multi-cast Only service in AWS that supports Ip Multi-cast","title":"26 Transit Gateway"},{"location":"Notes/VPC/26%20Transit%20Gateway/#transit-gateway","text":"Simplify network topology Star Gateway (Hub and Spoke Model) for VPC and On Premise Network Using Route Table we can define which VPC can talk to which VPC Can setup Direct Connect VPN Connection Support IP Multi-cast Only service in AWS that supports Ip Multi-cast","title":"Transit Gateway"},{"location":"Notes/VPC/27%20Cloud%20Hub%20vs%20Transit%20Gateway/","text":"AWS Cloud Hub vs Transit Gateway Cloud Hub One VPC connect with multiple On Premise Network Use VPN to establish connection Transit Gateway Connect multiple VPC with multiple On Premise Network Use VPN and Direct Connect to establish connection Supports IP Multi-cast","title":"27 Cloud Hub vs Transit Gateway"},{"location":"Notes/VPC/27%20Cloud%20Hub%20vs%20Transit%20Gateway/#aws-cloud-hub-vs-transit-gateway","text":"Cloud Hub One VPC connect with multiple On Premise Network Use VPN to establish connection Transit Gateway Connect multiple VPC with multiple On Premise Network Use VPN and Direct Connect to establish connection Supports IP Multi-cast","title":"AWS Cloud Hub vs Transit Gateway"},{"location":"Notes/VPC/28%20Networking%20Cost/","text":"Networking Cost Traffic from Public Internet to Instance is free Traffic from one Instance to another Instance in same AZ is free Traffic from one Instance to another Instance in different AZ 0.02 for Public Network / Elastic IP 0.01 for Private IP Traffic from one Region to another Region is 0.02 Best Practice Uses Private IP instead of Public IP Low cost, better performance Use same AZ as more as possible Risk on HA","title":"28 Networking Cost"},{"location":"Notes/VPC/28%20Networking%20Cost/#networking-cost","text":"Traffic from Public Internet to Instance is free Traffic from one Instance to another Instance in same AZ is free Traffic from one Instance to another Instance in different AZ 0.02 for Public Network / Elastic IP 0.01 for Private IP Traffic from one Region to another Region is 0.02 Best Practice Uses Private IP instead of Public IP Low cost, better performance Use same AZ as more as possible Risk on HA","title":"Networking Cost"},{"location":"Notes/VPC/29%20Global%20Accelerator/","text":"Global Accelerator Improve Availability and Performance of the applications Global Accelerator provides 2 IP address, we can also bring ours How Global Accelerator works Users connect to the Edge Location Edge Location pass traffic to Global Accelerator Global Accelerator pass the traffic to the Endpoint Group Without Global Accelerator user have to go through a lot of ISP Provider to reach the AWS Region Global Accelerator provides the following components Static IP Address Provide 2 static IP Address We can also bring our own Accelerator Direct traffic from the Edge Location to Optimal Endpoint through the AWS Global Network DNS Name DNS Name is provided to the IP Address Network Zone Similar to AZ Each Network Zone provide the Static IP If one Network Zone is blocked/not-available, it use the healthy one Listener Traffic can be distributed using Traffic Dial Endpoint Group Act as a fixed entry point of the using multiple endpoints of multiple region Endpoint","title":"29 Global Accelerator"},{"location":"Notes/VPC/29%20Global%20Accelerator/#global-accelerator","text":"Improve Availability and Performance of the applications Global Accelerator provides 2 IP address, we can also bring ours How Global Accelerator works Users connect to the Edge Location Edge Location pass traffic to Global Accelerator Global Accelerator pass the traffic to the Endpoint Group Without Global Accelerator user have to go through a lot of ISP Provider to reach the AWS Region Global Accelerator provides the following components Static IP Address Provide 2 static IP Address We can also bring our own Accelerator Direct traffic from the Edge Location to Optimal Endpoint through the AWS Global Network DNS Name DNS Name is provided to the IP Address Network Zone Similar to AZ Each Network Zone provide the Static IP If one Network Zone is blocked/not-available, it use the healthy one Listener Traffic can be distributed using Traffic Dial Endpoint Group Act as a fixed entry point of the using multiple endpoints of multiple region Endpoint","title":"Global Accelerator"},{"location":"Notes/VPC/30%20VPN%20Connection/","text":"VPN Connection Connection between On premise data center AWS Resource Connection is secure and private using IPsec","title":"30 VPN Connection"},{"location":"Notes/VPC/30%20VPN%20Connection/#vpn-connection","text":"Connection between On premise data center AWS Resource Connection is secure and private using IPsec","title":"VPN Connection"},{"location":"Notes/security/01%20Security%20Threat/","text":"NACL Use to block certain IP / IP Range HOST Based Firewall Use to block certain IP / IP Range HOST Based Firewalls are firewall iptables ufw Windows Firewall HOST Based Firewalls do not work when ALB is being used. Here NACL be used in ALB . ( Think Why? ) WAF Web Application Firewall Used for Common Security Threats Check Origin IP address SQL Injection Cross Site Scripting Check Headers When Cloudfront is used, set WAF in front of Cloudfront Rate Based ACL Rules can be used to avoid potential threat AWS Shield Use to prevent the DDoS attack AWS GuardDuty Thread detection service Used to monitor malicious activity and protect from unauthorized activities AWS Firewall Manager Used to manage the AWS WAF and AWS Shield Encryption On Flight (SSL) When a data is sending to server over internet, it is encrypted and only the server knows how to decrypt Use to prevent MITM (Man In The Middle) attack Encryption At Rest Before data is persist in the server, the data is encrypted and before retrival the data will be decrypted Key for encryption and decryption is being managed by another service like KMS The server should have permission to access KMS for the encryption and decryption operation With Encryption at Rest , even the server become vulnarable, the will still be safe Client Side Encryption Clients are responsible for encryption and decryption Server can not / should not decrypt the data This method is utilized by the Envelope Encryption","title":"01 Security Threat"},{"location":"Notes/security/01%20Security%20Threat/#nacl","text":"Use to block certain IP / IP Range","title":"NACL"},{"location":"Notes/security/01%20Security%20Threat/#host-based-firewall","text":"Use to block certain IP / IP Range HOST Based Firewalls are firewall iptables ufw Windows Firewall HOST Based Firewalls do not work when ALB is being used. Here NACL be used in ALB . ( Think Why? )","title":"HOST Based Firewall"},{"location":"Notes/security/01%20Security%20Threat/#waf","text":"Web Application Firewall Used for Common Security Threats Check Origin IP address SQL Injection Cross Site Scripting Check Headers When Cloudfront is used, set WAF in front of Cloudfront Rate Based ACL Rules can be used to avoid potential threat","title":"WAF"},{"location":"Notes/security/01%20Security%20Threat/#aws-shield","text":"Use to prevent the DDoS attack","title":"AWS Shield"},{"location":"Notes/security/01%20Security%20Threat/#aws-guardduty","text":"Thread detection service Used to monitor malicious activity and protect from unauthorized activities","title":"AWS GuardDuty"},{"location":"Notes/security/01%20Security%20Threat/#aws-firewall-manager","text":"Used to manage the AWS WAF and AWS Shield","title":"AWS Firewall Manager"},{"location":"Notes/security/01%20Security%20Threat/#encryption-on-flight-ssl","text":"When a data is sending to server over internet, it is encrypted and only the server knows how to decrypt Use to prevent MITM (Man In The Middle) attack","title":"Encryption On Flight (SSL)"},{"location":"Notes/security/01%20Security%20Threat/#encryption-at-rest","text":"Before data is persist in the server, the data is encrypted and before retrival the data will be decrypted Key for encryption and decryption is being managed by another service like KMS The server should have permission to access KMS for the encryption and decryption operation With Encryption at Rest , even the server become vulnarable, the will still be safe","title":"Encryption At Rest"},{"location":"Notes/security/01%20Security%20Threat/#client-side-encryption","text":"Clients are responsible for encryption and decryption Server can not / should not decrypt the data This method is utilized by the Envelope Encryption","title":"Client Side Encryption"},{"location":"Notes/security/02%20Cloud%20HSM/","text":"Cloud HSM Use key , not password Stands for Hardware Security Module Provide temper resistance environment for managing keys Its a dedicated hardware security module Manage your own keys, hence no access to AWS managed services Once key is lost, no way to retrieve it CloudHSM is Level 3 standard ( FIPS 140-2 Level 3 ) KMS is Level 2 standard AWS managed service Runs within VPC Single tenant, dedicated hardware, multi AZ cluster Use industry standard API Required when Need complete control over keys, including the underlying hardware and manage the lifecycle of keys Strict regulatory compliance is needed Level 3 compliance is needed PKCS#11 Java Cryptography Extensions Microsoft CryptoNG To keep Cloud HSM backup in secure and durable way Use EBK i.e. Ephemeral Backup Key to encrypt data Use PBK i.e. Persistent Backup Key to encrypt EBK Save the data to S3","title":"02 Cloud HSM"},{"location":"Notes/security/02%20Cloud%20HSM/#cloud-hsm","text":"Use key , not password Stands for Hardware Security Module Provide temper resistance environment for managing keys Its a dedicated hardware security module Manage your own keys, hence no access to AWS managed services Once key is lost, no way to retrieve it CloudHSM is Level 3 standard ( FIPS 140-2 Level 3 ) KMS is Level 2 standard AWS managed service Runs within VPC Single tenant, dedicated hardware, multi AZ cluster Use industry standard API Required when Need complete control over keys, including the underlying hardware and manage the lifecycle of keys Strict regulatory compliance is needed Level 3 compliance is needed PKCS#11 Java Cryptography Extensions Microsoft CryptoNG To keep Cloud HSM backup in secure and durable way Use EBK i.e. Ephemeral Backup Key to encrypt data Use PBK i.e. Persistent Backup Key to encrypt EBK Save the data to S3","title":"Cloud HSM"},{"location":"Notes/security/04%20Parameter%20Store/","text":"System Manager Parameter Store (SSM) Parameter Store is a component of AWS System Manager Manage secrets and configurations securely Parameter store is centralized tool to caching and distributing parameters across AWS services Helpful to separate configs and secrets from source control It is Serverless Scalable High Performance Used to store data and secrets Application configuration DB String Password API key Host Name Access Keys lambda functions env variable (when encrypted variables is shared to multiple lambda function) Values can be stored Encrypted by KMS Plaintext Can store parameters in hierarchies (Max 15 levels), like dev/app1/config prod/app1/config Can track version and roll back Can use TTL to expire values like passwords Must use Advanced Tier Allow events through cloudwatch Expiration (Set specific date) NoChangeNotification ExpirationNotification Can use to login to EC2 Instance using Run Command without using RDP or SSH Cloudtrail can be used to audit the api calls made to the parameter store Patch Manager : Used to patch the managed instances to overcome security vulnarebilities","title":"04 Parameter Store"},{"location":"Notes/security/04%20Parameter%20Store/#system-manager-parameter-store-ssm","text":"Parameter Store is a component of AWS System Manager Manage secrets and configurations securely Parameter store is centralized tool to caching and distributing parameters across AWS services Helpful to separate configs and secrets from source control It is Serverless Scalable High Performance Used to store data and secrets Application configuration DB String Password API key Host Name Access Keys lambda functions env variable (when encrypted variables is shared to multiple lambda function) Values can be stored Encrypted by KMS Plaintext Can store parameters in hierarchies (Max 15 levels), like dev/app1/config prod/app1/config Can track version and roll back Can use TTL to expire values like passwords Must use Advanced Tier Allow events through cloudwatch Expiration (Set specific date) NoChangeNotification ExpirationNotification Can use to login to EC2 Instance using Run Command without using RDP or SSH Cloudtrail can be used to audit the api calls made to the parameter store Patch Manager : Used to patch the managed instances to overcome security vulnarebilities","title":"System Manager Parameter Store (SSM)"},{"location":"Notes/security/05%20KMS/","text":"KMS With customer managed keys (CMK), kms Store the CMK Receives data from clients, encrypt the data and send it back When key is managed by in house security team For Encryption Generate data key using Customer managed CMK Encrypt data with data key Delete data key Store encrypted data key and data in S3 For decryption Use CMK to decrypt data key Decrypt data using Decrypted data key KMS Master Key is region specific KMS keys are region bounded By default KMS can encrypt mx 4kb of data. If we need to encrypt more data, we need to make use of Envelope Encryption With CloudTrail , audit can be done to determine, which keys were used to make API call Moving KMS encrypted resources between regions Create a snapshot of the resources While move it between region define new region KMS key Types of CMK Symmetric (AES-256) : Use single key for encryption and decryption Asymmetric (RSA & ECC) : Use key pairs, public key and private key. Public key for encryption and private key for decryption operation. Encryption is being happened from outside of the AWS. Key Policies Default KMS Key Policy : Everyone in the account can access the key Custom KMS Key Policy : Defined user, roles or cross account can access the key Data Key Caching Instead of invoking KMS key every time, we can use key caching This reduce the number of API calls to KMS The drawback is security, using same data key multiple times We can use CMK of AWS managed Default CMK (free) User managed CMK (1$/month) User imported CMK (1$/month) Cross account snapshot of KMS Keys Crate a snapshot with our own key Attach a policy so target account can read our key Share the encrypted snapshot Copy the snapshot Envelope Encryption CMK is used to generate , encrypt and decrypt the data keys Data Keys are used to encrypt and decrypt the data, from outside the AWS Envelope Encryption Local Encryption Usage In local environment, when a data is encrypted using a key, the data is protected. But we also need to encrypt the encryption key . We can encrypt the encryption key using another master key, called Master Key or CMK (Customer Master Key) . This CMK is stored in the KMS and never leave the KMS. To use this CMK we must call the KMS api. To encrypt the local data, First get the data encryption key using GenerateDataKey api This data encryption key can be used to encrypt the data Delete the data encryption key To decrypt local data First decrypt the encrypted data key and get plaintext data key Decrypt local data using the plaintext data key Delete the plaintext data key API for KMS Envelope Encryption Encrypt/Decrypt : Encrypt or decrypt the data up to 4KB. When data is more than 4KB, use Generate Data Key and Generate Data Key without Plaintext Generate Data Key : Returns DEK (Data Encryption Key) and a copy that is encrypted Generate Data Key Without Plaintext : Purpose is not to use immediately. Returns DEK (Data Encryption Key). Use Generate Data Key if the encelope encryption should be done right now. For later encryption, use Generate Data Key Without Plaintext . KMS Limits If the request quota is exceeded, the response shows ThrottlingException Minimize the issue by, Expotential backoff (backoff and retry) can be used to for exceeding the quota Data key caching Increasing the request quota","title":"05 KMS"},{"location":"Notes/security/05%20KMS/#kms","text":"With customer managed keys (CMK), kms Store the CMK Receives data from clients, encrypt the data and send it back When key is managed by in house security team For Encryption Generate data key using Customer managed CMK Encrypt data with data key Delete data key Store encrypted data key and data in S3 For decryption Use CMK to decrypt data key Decrypt data using Decrypted data key KMS Master Key is region specific KMS keys are region bounded By default KMS can encrypt mx 4kb of data. If we need to encrypt more data, we need to make use of Envelope Encryption With CloudTrail , audit can be done to determine, which keys were used to make API call","title":"KMS"},{"location":"Notes/security/05%20KMS/#moving-kms-encrypted-resources-between-regions","text":"Create a snapshot of the resources While move it between region define new region KMS key","title":"Moving KMS encrypted resources between regions"},{"location":"Notes/security/05%20KMS/#types-of-cmk","text":"Symmetric (AES-256) : Use single key for encryption and decryption Asymmetric (RSA & ECC) : Use key pairs, public key and private key. Public key for encryption and private key for decryption operation. Encryption is being happened from outside of the AWS.","title":"Types of CMK"},{"location":"Notes/security/05%20KMS/#key-policies","text":"Default KMS Key Policy : Everyone in the account can access the key Custom KMS Key Policy : Defined user, roles or cross account can access the key","title":"Key Policies"},{"location":"Notes/security/05%20KMS/#data-key-caching","text":"Instead of invoking KMS key every time, we can use key caching This reduce the number of API calls to KMS The drawback is security, using same data key multiple times","title":"Data Key Caching"},{"location":"Notes/security/05%20KMS/#we-can-use-cmk-of","text":"AWS managed Default CMK (free) User managed CMK (1$/month) User imported CMK (1$/month)","title":"We can use CMK of"},{"location":"Notes/security/05%20KMS/#cross-account-snapshot-of-kms-keys","text":"Crate a snapshot with our own key Attach a policy so target account can read our key Share the encrypted snapshot Copy the snapshot","title":"Cross account snapshot of KMS Keys"},{"location":"Notes/security/05%20KMS/#envelope-encryption","text":"CMK is used to generate , encrypt and decrypt the data keys Data Keys are used to encrypt and decrypt the data, from outside the AWS Envelope Encryption Local Encryption Usage In local environment, when a data is encrypted using a key, the data is protected. But we also need to encrypt the encryption key . We can encrypt the encryption key using another master key, called Master Key or CMK (Customer Master Key) . This CMK is stored in the KMS and never leave the KMS. To use this CMK we must call the KMS api. To encrypt the local data, First get the data encryption key using GenerateDataKey api This data encryption key can be used to encrypt the data Delete the data encryption key To decrypt local data First decrypt the encrypted data key and get plaintext data key Decrypt local data using the plaintext data key Delete the plaintext data key","title":"Envelope Encryption"},{"location":"Notes/security/05%20KMS/#api-for-kms-envelope-encryption","text":"Encrypt/Decrypt : Encrypt or decrypt the data up to 4KB. When data is more than 4KB, use Generate Data Key and Generate Data Key without Plaintext Generate Data Key : Returns DEK (Data Encryption Key) and a copy that is encrypted Generate Data Key Without Plaintext : Purpose is not to use immediately. Returns DEK (Data Encryption Key). Use Generate Data Key if the encelope encryption should be done right now. For later encryption, use Generate Data Key Without Plaintext .","title":"API for KMS Envelope Encryption"},{"location":"Notes/security/05%20KMS/#kms-limits","text":"If the request quota is exceeded, the response shows ThrottlingException Minimize the issue by, Expotential backoff (backoff and retry) can be used to for exceeding the quota Data key caching Increasing the request quota","title":"KMS Limits"}]}