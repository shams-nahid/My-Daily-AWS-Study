{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home My Daily AWS Study Notes Getting Started User Guide Features CSAA & CDA Suitable for both associate level certification AWS Certified Solution Architect Associate AWS Certified Developer Associate Notes on Exam Dumps Notes of practice exam from, Wizlab (CSAA, CDA) Tutorial Dojo (CSAA, CDA) Stephane Maarek (CSAA) ACloud Guru (CSAA)","title":"Home"},{"location":"#home","text":"My Daily AWS Study Notes Getting Started User Guide","title":"Home"},{"location":"Notes/Next Study/","text":"More Study Needed KMS Envelope Encryption Cloudformation Masterclass X-Ray trust policy and iam role envelope encryption code commit Cloudwatch period, data point and evaluation period Dynamodb rcu wcu API and Lambda method/integration request/response Cognito properties like cognito streams","title":"Next Study"},{"location":"Notes/Next Study/#more-study-needed","text":"KMS Envelope Encryption Cloudformation Masterclass X-Ray trust policy and iam role envelope encryption code commit Cloudwatch period, data point and evaluation period Dynamodb rcu wcu API and Lambda method/integration request/response Cognito properties like cognito streams","title":"More Study Needed"},{"location":"Notes/AWS Fundamentals/01 Region and AZ/","text":"Region Combination of data centers Data centers are called AZ , i.e. Availability Zone AZ Availability Zone In region, couple of AZ exists AZ are isolated each other, so Disaster in one AZ does not impact on other AZ AZ and Region Some service are AZ scoped Some services are Global scope AZ name can be us-east-1a , us-east-2b Region name be us-east , us-west AZ names are randomized according to the Account us-east-1a in one account could be us-east-1b to another account","title":"01 Region and AZ"},{"location":"Notes/AWS Fundamentals/01 Region and AZ/#region","text":"Combination of data centers Data centers are called AZ , i.e. Availability Zone","title":"Region"},{"location":"Notes/AWS Fundamentals/01 Region and AZ/#az","text":"Availability Zone In region, couple of AZ exists AZ are isolated each other, so Disaster in one AZ does not impact on other AZ","title":"AZ"},{"location":"Notes/AWS Fundamentals/01 Region and AZ/#az-and-region","text":"Some service are AZ scoped Some services are Global scope AZ name can be us-east-1a , us-east-2b Region name be us-east , us-west AZ names are randomized according to the Account us-east-1a in one account could be us-east-1b to another account","title":"AZ and Region"},{"location":"Notes/AWS Fundamentals/02 IAM/","text":"IAM Identity and Access Management AWS Security spans Users (For Physical Person) Groups (Can be admins, dev-ops, developers) Roles (For AWS Resources ) Policy Written in JSON format Determine what Users , Groups and Role has access Verify IAM Policy by dry-run policy can be used to verify if there is available permission IAM Policy Simulator Permission specified in cli with access key and secret overrides the IAM role permissions For any unauthorized encrypt message of the unauthorized access, can be decrypt by decode-authorization-message of STS API Account Alisa By default, sign in url is like, account-id.signin.aws.amazon.com/console By creating the account alias, url become, account-alias.signin.aws.amazon.com/console IAM Certificate Store Can be used to import 3rd party SSL/TLS certificate. Both ACM and IAM Certificate Store can be used to import 3rd party SSL/TLS Certificate. Trust Policy To access a service using cli/api from ec2 instance First need to create policy for the targeted resources Add the ec2 service as the trust policy (So ec2 can use the policy created in the first step) With passRole we can ensure, user does not have more permission than it required. This way, we do not need to store any credentials in the ec2 service Best Practices Delete the root user access keys Create roles and IAM policies with least permissions Use groups for users and assign roles If the new policy does not work, we can revert back to the old policy by selecting the previous version","title":"02 IAM"},{"location":"Notes/AWS Fundamentals/02 IAM/#iam","text":"Identity and Access Management AWS Security spans Users (For Physical Person) Groups (Can be admins, dev-ops, developers) Roles (For AWS Resources ) Policy Written in JSON format Determine what Users , Groups and Role has access Verify IAM Policy by dry-run policy can be used to verify if there is available permission IAM Policy Simulator Permission specified in cli with access key and secret overrides the IAM role permissions For any unauthorized encrypt message of the unauthorized access, can be decrypt by decode-authorization-message of STS API","title":"IAM"},{"location":"Notes/AWS Fundamentals/02 IAM/#account-alisa","text":"By default, sign in url is like, account-id.signin.aws.amazon.com/console By creating the account alias, url become, account-alias.signin.aws.amazon.com/console","title":"Account Alisa"},{"location":"Notes/AWS Fundamentals/02 IAM/#iam-certificate-store","text":"Can be used to import 3rd party SSL/TLS certificate. Both ACM and IAM Certificate Store can be used to import 3rd party SSL/TLS Certificate.","title":"IAM Certificate Store"},{"location":"Notes/AWS Fundamentals/02 IAM/#trust-policy","text":"To access a service using cli/api from ec2 instance First need to create policy for the targeted resources Add the ec2 service as the trust policy (So ec2 can use the policy created in the first step) With passRole we can ensure, user does not have more permission than it required. This way, we do not need to store any credentials in the ec2 service","title":"Trust Policy"},{"location":"Notes/AWS Fundamentals/02 IAM/#best-practices","text":"Delete the root user access keys Create roles and IAM policies with least permissions Use groups for users and assign roles If the new policy does not work, we can revert back to the old policy by selecting the previous version","title":"Best Practices"},{"location":"Notes/AWS Fundamentals/03 EC2 Introduction/","text":"EC2 Elastic Cloud Computer EC2 User Data Scripts that runs when the instance is booted up Use for Installing updates while boot up Installing software while boot up Download common files while boot up These scripts are run as root user like sudo command EC2 Meta Data Information about the instance Launch Type On Demand Pay as you go Reserved Regular Reserved Instance Min 1 to max 3 Years Convertible Reserved Instance Can be convert the types like more cpu optimized , more memory optimized Scheduled Reserved Instance Will be up and running for certain times in regular basis Recommended when the time frame is at least 1 year usage Spot Instance can loose instance very low price Define max spot price if the current spot price goes high of the defined spot price, we loose the instance in 2 minutes Using Spot Block we can extend the termination delay till -6 hours Two type of request One time request Once max price < current price, all instances are removed Persistent Request If spot instances stop and then things are good, these instances launched automatically To stop persistent request First delete the spot request Then removed the spot instances Spot Fleets Set of Spot Instances and On-Demand Instances We can define Possible launch pools Multiple AZ Various type of instance Various OS Automatically stops when meed the capacity It offers lowestPrice diversified (distribute across az and workloads) capacityOptimized Dedicated Instance Does not share hardware Dedicated Host Does not share server, entire placement is booked Dedicated Instance Vs Dedicated Host In Dedicated Instance The billing is is done per instance The other instance of same account, may share hardware No control over placement group In Dedicated Host , billing is on whole Dedicated Host Control over placement group Instance Type R Instance with lots of memory/ RAM Used when in memory caching is required C Instance with good Computation Power Used for DB Server M Middle between RAM and Computation Used for Application Server / General Application Has 10-25 GB/s networking Has ENA enabled Low latency network with NVME ebs I For heave I/O Application Used for DB When good Instance Storage is required G GPU optimized instance Used for video rendering or machine learning T2/T3 Burstble instance Provide good performance according to the capacity There is a criteria, where unlimited burst is provided Terminating a EC2 Instance For production EC2 Instance Need a tag In resource level there should be explicit deny for production tag for not to terminates","title":"03 EC2 Introduction"},{"location":"Notes/AWS Fundamentals/03 EC2 Introduction/#ec2","text":"Elastic Cloud Computer","title":"EC2"},{"location":"Notes/AWS Fundamentals/03 EC2 Introduction/#ec2-user-data","text":"Scripts that runs when the instance is booted up Use for Installing updates while boot up Installing software while boot up Download common files while boot up These scripts are run as root user like sudo command","title":"EC2 User Data"},{"location":"Notes/AWS Fundamentals/03 EC2 Introduction/#ec2-meta-data","text":"Information about the instance","title":"EC2 Meta Data"},{"location":"Notes/AWS Fundamentals/03 EC2 Introduction/#launch-type","text":"On Demand Pay as you go Reserved Regular Reserved Instance Min 1 to max 3 Years Convertible Reserved Instance Can be convert the types like more cpu optimized , more memory optimized Scheduled Reserved Instance Will be up and running for certain times in regular basis Recommended when the time frame is at least 1 year usage Spot Instance can loose instance very low price Define max spot price if the current spot price goes high of the defined spot price, we loose the instance in 2 minutes Using Spot Block we can extend the termination delay till -6 hours Two type of request One time request Once max price < current price, all instances are removed Persistent Request If spot instances stop and then things are good, these instances launched automatically To stop persistent request First delete the spot request Then removed the spot instances Spot Fleets Set of Spot Instances and On-Demand Instances We can define Possible launch pools Multiple AZ Various type of instance Various OS Automatically stops when meed the capacity It offers lowestPrice diversified (distribute across az and workloads) capacityOptimized Dedicated Instance Does not share hardware Dedicated Host Does not share server, entire placement is booked","title":"Launch Type"},{"location":"Notes/AWS Fundamentals/03 EC2 Introduction/#dedicated-instance-vs-dedicated-host","text":"In Dedicated Instance The billing is is done per instance The other instance of same account, may share hardware No control over placement group In Dedicated Host , billing is on whole Dedicated Host Control over placement group","title":"Dedicated Instance Vs Dedicated Host"},{"location":"Notes/AWS Fundamentals/03 EC2 Introduction/#instance-type","text":"R Instance with lots of memory/ RAM Used when in memory caching is required C Instance with good Computation Power Used for DB Server M Middle between RAM and Computation Used for Application Server / General Application Has 10-25 GB/s networking Has ENA enabled Low latency network with NVME ebs I For heave I/O Application Used for DB When good Instance Storage is required G GPU optimized instance Used for video rendering or machine learning T2/T3 Burstble instance Provide good performance according to the capacity There is a criteria, where unlimited burst is provided","title":"Instance Type"},{"location":"Notes/AWS Fundamentals/03 EC2 Introduction/#terminating-a-ec2-instance","text":"For production EC2 Instance Need a tag In resource level there should be explicit deny for production tag for not to terminates","title":"Terminating a EC2 Instance"},{"location":"Notes/AWS Fundamentals/04 Security Group/","text":"Security Group Firewall on EC2 Instance Stateful Effective immediately","title":"04 Security Group"},{"location":"Notes/AWS Fundamentals/04 Security Group/#security-group","text":"Firewall on EC2 Instance Stateful Effective immediately","title":"Security Group"},{"location":"Notes/AWS Fundamentals/06 Public Vs Private vs Elastic IP/","text":"Public IP Any IP other than Private IP Private IP Private IP does not change if EC2 Instance is restarted (stop and then start) Big Network Range 10.0.0.0 - 10.255.255.255 and CIDR ( 10.0.0.0/8 ) AWS Default Range 172.16.0.0 - 172.31.255.255 and CIDR ( 172.16.0.0/12 ) Home Network Range 192.168.0.0 - 192.168.255.255 and CIDR ( 192.168.0.0/16 ) Elastic IP Does not change if EC2 Instance is restarted (stop and then start) or terminated Max 5 Elastic IP can be used in one region We can bring our own IP as Elastic IP Elastic IP is charge, even if it is not being used","title":"06 Public Vs Private vs Elastic IP"},{"location":"Notes/AWS Fundamentals/06 Public Vs Private vs Elastic IP/#public-ip","text":"Any IP other than Private IP","title":"Public IP"},{"location":"Notes/AWS Fundamentals/06 Public Vs Private vs Elastic IP/#private-ip","text":"Private IP does not change if EC2 Instance is restarted (stop and then start) Big Network Range 10.0.0.0 - 10.255.255.255 and CIDR ( 10.0.0.0/8 ) AWS Default Range 172.16.0.0 - 172.31.255.255 and CIDR ( 172.16.0.0/12 ) Home Network Range 192.168.0.0 - 192.168.255.255 and CIDR ( 192.168.0.0/16 )","title":"Private IP"},{"location":"Notes/AWS Fundamentals/06 Public Vs Private vs Elastic IP/#elastic-ip","text":"Does not change if EC2 Instance is restarted (stop and then start) or terminated Max 5 Elastic IP can be used in one region We can bring our own IP as Elastic IP Elastic IP is charge, even if it is not being used","title":"Elastic IP"},{"location":"Notes/AWS Monitoring/01 Cloudwatch/","text":"Cloudwatch Cloudwatch Metrics Cloudwatch provide metrics for every AWS Service Metrics is a variable to monitor, such as CPU Utilization Networking data Metric belong to Namespace Namespace are similar to Group Metric dimensions are Attribute , like Instance ID Environment Name Each Metric can have up to 10 Dimensions Metrics have Timestamps Using metric , the Cloudwatch Dashboard is generated Cloudwatch Detailed Monitoring By default, EC2 have metrics each 5 minutes With Detailed Monitoring Metric generate every 1 Minute Good for ASG Free Tier allows 10 Detail Monitoring For EC2 Memory Usage , there is no default metric. Need to use Custom Metric Cloudwatch Custom Metric Can send custom metrics to Cloudwatch Ability to send Dimension instance.id environment.name Metric Resolution Standard 1 Minute High Resolution, up to 1 Sec To send custom metric use PutMetricData Cloudwatch Dashboard Dashboard are Global Dashboard Graph includes different Region Can setup Auto Refresh Pricing 3 Dashboard (Up to 50 Metrics) free After free tier, 3 dollar/dashboard/per month Cloudwatch Logs Logs can be send to Cloudwatch through SDK Cloudwatch collect log from Elastic Beanstalk ECS AWS Lambda VPC Flow Logs API Gateway Cloudtrail Cloudwatch Log Agent (From EC2 Instance ) Route 53 (DNS Query) Logs go to S3 to store or archive Stream to Elastic Search for analytics Log Storage Architecture Groups: Log is grouped under name Each group has streams of logs Can define expiration period (After the expiration period, the logs will be deleted) KMS can be used to encrypt the logs To send logs, make sure the Permission to write logs are set To follow/tail logs, we can use AWS CLI Possible to filter by expression Helpful to find logs or specific IP Can use to trigger alarm Cloudwatch Logs Insights Can be used to query logs To use, need to install the Unified Cloudwatch Logs Event Cloudwatch Logs Agent Vs Unified Agent Logs Agent Old Version Unified Agent Newer version Get Additional system level metrics Can use SSM Parameter Store to centralized configuration Cloudwatch Alarms Alarms are used to trigger notification for any metric Alarms can go to ASG EC2 Actions SNS Notifications Alarm can raise Sampling value Percentage value Max or Min value Alarm States OK (When everything is alright) INSUFFICIENT_DATA (When not enough data to measure it its OK or ALARM state) ALARM (When metrics reached the Threshold ) Period Time length to evaluate the metric In case of High Resolution Metric , period can be 10 sec Creating Cloudwatch Event While creating a cloudwatch event, we can set Period : Define evaluation time in seconds. Evaluation Period : Known as Data Point . Number of recent Period to consider to generate a alarm state DataPoints To Alarm : Determine to go to ALARM state. We can define how many period can be reached within a evaluation period to go to ALARM Cloudwatch Event Can schedule CRON Jobs Event Pattern Rules on react a service doing something Example: Code Pipeline state change Can trigger Lambda Function SQS SNS Kinesis Cloudwatch Event create a sample document to give information about the change Use case in S3 and Code Pipeline Code can be uploaded to S3 Cloudwatch Event trigger the Code Pipeline Code will be deployed to the Elastic Beanstalk Can be used to change the number of Fargate Cluster should run according to the events Allow monitor jobs in the batch jobs Cloudwatch Agent Collect system info and log files Can track memory, swap and disk space","title":"01 Cloudwatch"},{"location":"Notes/AWS Monitoring/01 Cloudwatch/#cloudwatch","text":"","title":"Cloudwatch"},{"location":"Notes/AWS Monitoring/01 Cloudwatch/#cloudwatch-metrics","text":"Cloudwatch provide metrics for every AWS Service Metrics is a variable to monitor, such as CPU Utilization Networking data Metric belong to Namespace Namespace are similar to Group Metric dimensions are Attribute , like Instance ID Environment Name Each Metric can have up to 10 Dimensions Metrics have Timestamps Using metric , the Cloudwatch Dashboard is generated","title":"Cloudwatch Metrics"},{"location":"Notes/AWS Monitoring/01 Cloudwatch/#cloudwatch-detailed-monitoring","text":"By default, EC2 have metrics each 5 minutes With Detailed Monitoring Metric generate every 1 Minute Good for ASG Free Tier allows 10 Detail Monitoring For EC2 Memory Usage , there is no default metric. Need to use Custom Metric","title":"Cloudwatch Detailed Monitoring"},{"location":"Notes/AWS Monitoring/01 Cloudwatch/#cloudwatch-custom-metric","text":"Can send custom metrics to Cloudwatch Ability to send Dimension instance.id environment.name Metric Resolution Standard 1 Minute High Resolution, up to 1 Sec To send custom metric use PutMetricData","title":"Cloudwatch Custom Metric"},{"location":"Notes/AWS Monitoring/01 Cloudwatch/#cloudwatch-dashboard","text":"Dashboard are Global Dashboard Graph includes different Region Can setup Auto Refresh Pricing 3 Dashboard (Up to 50 Metrics) free After free tier, 3 dollar/dashboard/per month","title":"Cloudwatch Dashboard"},{"location":"Notes/AWS Monitoring/01 Cloudwatch/#cloudwatch-logs","text":"Logs can be send to Cloudwatch through SDK Cloudwatch collect log from Elastic Beanstalk ECS AWS Lambda VPC Flow Logs API Gateway Cloudtrail Cloudwatch Log Agent (From EC2 Instance ) Route 53 (DNS Query) Logs go to S3 to store or archive Stream to Elastic Search for analytics Log Storage Architecture Groups: Log is grouped under name Each group has streams of logs Can define expiration period (After the expiration period, the logs will be deleted) KMS can be used to encrypt the logs To send logs, make sure the Permission to write logs are set To follow/tail logs, we can use AWS CLI Possible to filter by expression Helpful to find logs or specific IP Can use to trigger alarm Cloudwatch Logs Insights Can be used to query logs To use, need to install the Unified Cloudwatch Logs Event","title":"Cloudwatch Logs"},{"location":"Notes/AWS Monitoring/01 Cloudwatch/#cloudwatch-logs-agent-vs-unified-agent","text":"Logs Agent Old Version Unified Agent Newer version Get Additional system level metrics Can use SSM Parameter Store to centralized configuration","title":"Cloudwatch Logs Agent Vs Unified Agent"},{"location":"Notes/AWS Monitoring/01 Cloudwatch/#cloudwatch-alarms","text":"Alarms are used to trigger notification for any metric Alarms can go to ASG EC2 Actions SNS Notifications Alarm can raise Sampling value Percentage value Max or Min value Alarm States OK (When everything is alright) INSUFFICIENT_DATA (When not enough data to measure it its OK or ALARM state) ALARM (When metrics reached the Threshold ) Period Time length to evaluate the metric In case of High Resolution Metric , period can be 10 sec","title":"Cloudwatch Alarms"},{"location":"Notes/AWS Monitoring/01 Cloudwatch/#creating-cloudwatch-event","text":"While creating a cloudwatch event, we can set Period : Define evaluation time in seconds. Evaluation Period : Known as Data Point . Number of recent Period to consider to generate a alarm state DataPoints To Alarm : Determine to go to ALARM state. We can define how many period can be reached within a evaluation period to go to ALARM","title":"Creating Cloudwatch Event"},{"location":"Notes/AWS Monitoring/01 Cloudwatch/#cloudwatch-event","text":"Can schedule CRON Jobs Event Pattern Rules on react a service doing something Example: Code Pipeline state change Can trigger Lambda Function SQS SNS Kinesis Cloudwatch Event create a sample document to give information about the change Use case in S3 and Code Pipeline Code can be uploaded to S3 Cloudwatch Event trigger the Code Pipeline Code will be deployed to the Elastic Beanstalk Can be used to change the number of Fargate Cluster should run according to the events Allow monitor jobs in the batch jobs","title":"Cloudwatch Event"},{"location":"Notes/AWS Monitoring/01 Cloudwatch/#cloudwatch-agent","text":"Collect system info and log files Can track memory, swap and disk space","title":"Cloudwatch Agent"},{"location":"Notes/AWS Monitoring/02 Cloudtrail/","text":"Cloudtrail All the AWS history and events are stored here, including Console SDK CLI AWS Service Cloudtrail is enabled by default Can put the logs to CloudWatch Logs Example, if need to trace, who change the resource, need to go to Cloudtrail The logs are encrypted by default By default Cloudtrail Logs are encrypted by S3 Server Side Encryption Also we can use KMS for encryption Enabling Cloudtrail Log File Integrity ensure Non compliance log Generate public and private key of the logs Put the digest in separate folder Cloudtrail Global Event Logs can only done by AWS CLI , not Console To monitor API calls in the Redshift Cluster , need to use Cloudtrail","title":"02 Cloudtrail"},{"location":"Notes/AWS Monitoring/02 Cloudtrail/#cloudtrail","text":"All the AWS history and events are stored here, including Console SDK CLI AWS Service Cloudtrail is enabled by default Can put the logs to CloudWatch Logs Example, if need to trace, who change the resource, need to go to Cloudtrail The logs are encrypted by default By default Cloudtrail Logs are encrypted by S3 Server Side Encryption Also we can use KMS for encryption Enabling Cloudtrail Log File Integrity ensure Non compliance log Generate public and private key of the logs Put the digest in separate folder Cloudtrail Global Event Logs can only done by AWS CLI , not Console To monitor API calls in the Redshift Cluster , need to use Cloudtrail","title":"Cloudtrail"},{"location":"Notes/AWS Monitoring/03 AWS Config/","text":"AWS Config Helps record configurations and changes over time Can store config data to S3 for further analysis Analysis can be Is there un-restricted SSH access in any SG Do bucket has public access Is ALB Config changes over time Alert for the config can be received by SNS AWS-Config is per-region service, although can be aggregate across regions and accounts Config Rules Can use AWS Managed Rules Can make custom rules using AWS Lambda , like If each EBS disk is type GP2 If each EC2 instance is type t2.micro Rules be triggered or evaluated On config change Regular time intervals Using Cloudwatch Event AWS Config does not prevent actions from happening, it just check the changes Pricing is 2 Dollar/per active rule/per region / per month","title":"03 AWS Config"},{"location":"Notes/AWS Monitoring/03 AWS Config/#aws-config","text":"Helps record configurations and changes over time Can store config data to S3 for further analysis Analysis can be Is there un-restricted SSH access in any SG Do bucket has public access Is ALB Config changes over time Alert for the config can be received by SNS AWS-Config is per-region service, although can be aggregate across regions and accounts","title":"AWS Config"},{"location":"Notes/AWS Monitoring/03 AWS Config/#config-rules","text":"Can use AWS Managed Rules Can make custom rules using AWS Lambda , like If each EBS disk is type GP2 If each EC2 instance is type t2.micro Rules be triggered or evaluated On config change Regular time intervals Using Cloudwatch Event AWS Config does not prevent actions from happening, it just check the changes Pricing is 2 Dollar/per active rule/per region / per month","title":"Config Rules"},{"location":"Notes/AWS Storage/01 Snowball/","text":"Snowball Moving data from on-premise to S3 Features Secure Temper Resistant KMS 256 Encryption Charge per data transfer job Snowball tracking can be done using SNS (Text Message) and AWS Console Process Request a snowball Install snowball clients to server Connect snowball with server Copy files from server to snowball Ship back to the device Snowball Edge Snowball Edge has the computational capability Two types Storage optimized Compute Optimized (Can be with GPU) Supports EC2 AMI so it can perform processing on go Supports Lambda Function Use cases Data Migration Image Collection IoT Capture Machine Learning Snowmobile A truck Use to transfer exabytes of data Limitation Snowball can only transfer data back and forth with S3 To transfer data from/to Glacier , first need to use S3 . Then Life Cycle Policy can be used to transfer to/from Glacier .","title":"01 Snowball"},{"location":"Notes/AWS Storage/01 Snowball/#snowball","text":"Moving data from on-premise to S3 Features Secure Temper Resistant KMS 256 Encryption Charge per data transfer job Snowball tracking can be done using SNS (Text Message) and AWS Console","title":"Snowball"},{"location":"Notes/AWS Storage/01 Snowball/#process","text":"Request a snowball Install snowball clients to server Connect snowball with server Copy files from server to snowball Ship back to the device","title":"Process"},{"location":"Notes/AWS Storage/01 Snowball/#snowball-edge","text":"Snowball Edge has the computational capability Two types Storage optimized Compute Optimized (Can be with GPU) Supports EC2 AMI so it can perform processing on go Supports Lambda Function Use cases Data Migration Image Collection IoT Capture Machine Learning","title":"Snowball Edge"},{"location":"Notes/AWS Storage/01 Snowball/#snowmobile","text":"A truck Use to transfer exabytes of data","title":"Snowmobile"},{"location":"Notes/AWS Storage/01 Snowball/#limitation","text":"Snowball can only transfer data back and forth with S3 To transfer data from/to Glacier , first need to use S3 . Then Life Cycle Policy can be used to transfer to/from Glacier .","title":"Limitation"},{"location":"Notes/AWS Storage/02 Storage Gateway/","text":"Storage Gateway Bridge between AWS S3 and On-Premise data center 3 types of storage gateway File Gateway Volume Gateway Tape Gateway File Gateway Use for File Access Use for NFS and SMB protocol Can be mounted on many servers Bucket access is done by IAM role in the File Gateway Most recent data is being cached Hardware Appliance File gateway needs virtualization Alternative can be Hardware Appliance Use when small data center does not have virtualization capability Volume Gateway Use for Volumes and Block Storage Use for iSCSI protocol Backed by EBS Snapshot Two types of Volume Gateway Cached Volume Low latency for the recent data Stored Volumes All data are in the on-premise Schedule backup from on premise to S3 Tape Gateway Backup process use Physical Tape Backup with iSCSI Protocol Use VTL (Virtual Tape Library) Allow archive data to Glacier directly from the on-premise Misc Notes When we need low latency data access, use Storage Gateway Stored Volumes , it keeps all data in on-premise data center and make backup to cloud time basis In Storage Gateway Cached Volumes , only recent data are cached in the on-premise center","title":"02 Storage Gateway"},{"location":"Notes/AWS Storage/02 Storage Gateway/#storage-gateway","text":"Bridge between AWS S3 and On-Premise data center 3 types of storage gateway File Gateway Volume Gateway Tape Gateway","title":"Storage Gateway"},{"location":"Notes/AWS Storage/02 Storage Gateway/#file-gateway","text":"Use for File Access Use for NFS and SMB protocol Can be mounted on many servers Bucket access is done by IAM role in the File Gateway Most recent data is being cached Hardware Appliance File gateway needs virtualization Alternative can be Hardware Appliance Use when small data center does not have virtualization capability","title":"File Gateway"},{"location":"Notes/AWS Storage/02 Storage Gateway/#volume-gateway","text":"Use for Volumes and Block Storage Use for iSCSI protocol Backed by EBS Snapshot Two types of Volume Gateway Cached Volume Low latency for the recent data Stored Volumes All data are in the on-premise Schedule backup from on premise to S3","title":"Volume Gateway"},{"location":"Notes/AWS Storage/02 Storage Gateway/#tape-gateway","text":"Backup process use Physical Tape Backup with iSCSI Protocol Use VTL (Virtual Tape Library) Allow archive data to Glacier directly from the on-premise","title":"Tape Gateway"},{"location":"Notes/AWS Storage/02 Storage Gateway/#misc-notes","text":"When we need low latency data access, use Storage Gateway Stored Volumes , it keeps all data in on-premise data center and make backup to cloud time basis In Storage Gateway Cached Volumes , only recent data are cached in the on-premise center","title":"Misc Notes"},{"location":"Notes/AWS Storage/03 FSx/","text":"FSx Two types of FSx FSx for Windows FSx for Lustre FSx For Windows Fully managed widows file system Use SMB and NTFS protocol Built on SSD Can be accessed from On-Premise server Can be configured Multi-AZ Data is backed by S3 There is Microsoft Active Directory integration This type of storage can be managed by AWS Managed AD and accessed by other instances FSx for Lustre Parallel distributed file system for linux Stands for FSx for Linux Cluster Use for HPC (High Performance Computing) High performance storage Seamless integration with S3","title":"03 FSx"},{"location":"Notes/AWS Storage/03 FSx/#fsx","text":"Two types of FSx FSx for Windows FSx for Lustre","title":"FSx"},{"location":"Notes/AWS Storage/03 FSx/#fsx-for-windows","text":"Fully managed widows file system Use SMB and NTFS protocol Built on SSD Can be accessed from On-Premise server Can be configured Multi-AZ Data is backed by S3 There is Microsoft Active Directory integration This type of storage can be managed by AWS Managed AD and accessed by other instances","title":"FSx For Windows"},{"location":"Notes/AWS Storage/03 FSx/#fsx-for-lustre","text":"Parallel distributed file system for linux Stands for FSx for Linux Cluster Use for HPC (High Performance Computing) High performance storage Seamless integration with S3","title":"FSx for Lustre"},{"location":"Notes/AWS Storage/04 EFS/","text":"EFS Known as Elastic File System POSIX-compliant file system Network file system Only compatible with linux EFS Mount Helper can be used to encrypt data in transit KMS can be use to encrypt EFS Pay as go Increase as we use Can be used with only one VPC Used when Multiple linux server need to connect to storage simultaneously Linux server can be from multi-az Two types of performance mode General purpose Max I/O Cons are higher latency Can be used with higher level of throughput Can connect thousands of instances Storage Tier Two types of EFS Standard Infrequent Access Lifecycle policy Can be used to move files to different tier after N days","title":"04 EFS"},{"location":"Notes/AWS Storage/04 EFS/#efs","text":"Known as Elastic File System POSIX-compliant file system Network file system Only compatible with linux EFS Mount Helper can be used to encrypt data in transit KMS can be use to encrypt EFS Pay as go Increase as we use Can be used with only one VPC Used when Multiple linux server need to connect to storage simultaneously Linux server can be from multi-az Two types of performance mode General purpose Max I/O Cons are higher latency Can be used with higher level of throughput Can connect thousands of instances","title":"EFS"},{"location":"Notes/AWS Storage/04 EFS/#storage-tier","text":"Two types of EFS Standard Infrequent Access Lifecycle policy Can be used to move files to different tier after N days","title":"Storage Tier"},{"location":"Notes/AWS Storage/05 EBS/","text":"EBS Elastic Block Store 4 Types GP2 General Purpose Handle up to 16000 IO/PS Used for Recommended for most workloads System Boot Low latency interactive apps Dev and Test environment IO1 IO optimized Used for When required more than 16000 IO/PS is required Large Database Critical business operation, require high sustained IO/PS When huge load and performance for the NoSQL database ST1 Throughput optimized Used for Streaming workloads, require consistent and fast throughput at low price Big Data, Data warehouses Apache kafka SC1 Infrequently Used Throughput optimized Used for large amount of data which are infrequently used For sequential I/O operations EBS transfer EBS volumes are AZ Locked First need to take a Snapshot Create the volume from the Snapshot to another AZ EBS Backup Backups are incremental, only backup the changed blocks Can take snapshot of the EBS Volume Theses snapshot can be made available to other Regions Snapshot can be automate by using DLM aka Data Lifecycle Manager While taking backups There's hamper on heavy traffic Recommended to detach the volume To use snapshot, require pre-warm Snapshots are taking place in S3 Using lifecycle policy, can automate the Snapshot Using the retention policy, can be delete the old Snapshot Reference For EBS Types Encryption Encryption is handled by AWS Use KMS (AES-256) When a EBS is encrypted All data inside the volume is encrypted All moving data between instance and volume is encrypted All snapshots created from them is encrypted All volumes created from these snapshots are encrypted EBS volumes can be used while making a Snapshots , no problem. To encrypt an un encrypted EBS Take a snapshot Encrypt the snapshot using copy Create volume from the encrypted volume Delete volume and un encrypted snapshot for security leakage EBS vs Instance Store Instance Store Physically attached to the instance Good I/O When instance is terminated, the instance store along with the data lost Although it is block storage, size can not be increased over time EBS Network drive Data is persisted even the instance is terminated EBS RAID Two types of RAID RAID 0 Improve performance Example Lets say we have 2 EBS with 10000 IOps We logically merge and use 20000 IOps RAID 1 Use for fault tolerance Mirror the EBS volume Example If we have 2 EBS volume We write on each of volume So even if one EBS volume fails, data is still exist in another one Attaching a New EBS Volume When we attach a new EBS to EC2 instance, it is considered as block device. To make it usable, need to Format to a file system (AWS does not pre configure any file system to EBS) Mount it to the instance Detaching Existing EBS Volume For root volume Stop the instance Detach the volume For non-root volume For running instance, un-mount and detach For stopped instance, detach, no need to un-mount","title":"05 EBS"},{"location":"Notes/AWS Storage/05 EBS/#ebs","text":"Elastic Block Store 4 Types GP2 General Purpose Handle up to 16000 IO/PS Used for Recommended for most workloads System Boot Low latency interactive apps Dev and Test environment IO1 IO optimized Used for When required more than 16000 IO/PS is required Large Database Critical business operation, require high sustained IO/PS When huge load and performance for the NoSQL database ST1 Throughput optimized Used for Streaming workloads, require consistent and fast throughput at low price Big Data, Data warehouses Apache kafka SC1 Infrequently Used Throughput optimized Used for large amount of data which are infrequently used For sequential I/O operations EBS transfer EBS volumes are AZ Locked First need to take a Snapshot Create the volume from the Snapshot to another AZ EBS Backup Backups are incremental, only backup the changed blocks Can take snapshot of the EBS Volume Theses snapshot can be made available to other Regions Snapshot can be automate by using DLM aka Data Lifecycle Manager While taking backups There's hamper on heavy traffic Recommended to detach the volume To use snapshot, require pre-warm Snapshots are taking place in S3 Using lifecycle policy, can automate the Snapshot Using the retention policy, can be delete the old Snapshot Reference For EBS Types","title":"EBS"},{"location":"Notes/AWS Storage/05 EBS/#encryption","text":"Encryption is handled by AWS Use KMS (AES-256) When a EBS is encrypted All data inside the volume is encrypted All moving data between instance and volume is encrypted All snapshots created from them is encrypted All volumes created from these snapshots are encrypted EBS volumes can be used while making a Snapshots , no problem. To encrypt an un encrypted EBS Take a snapshot Encrypt the snapshot using copy Create volume from the encrypted volume Delete volume and un encrypted snapshot for security leakage","title":"Encryption"},{"location":"Notes/AWS Storage/05 EBS/#ebs-vs-instance-store","text":"Instance Store Physically attached to the instance Good I/O When instance is terminated, the instance store along with the data lost Although it is block storage, size can not be increased over time EBS Network drive Data is persisted even the instance is terminated","title":"EBS vs Instance Store"},{"location":"Notes/AWS Storage/05 EBS/#ebs-raid","text":"Two types of RAID RAID 0 Improve performance Example Lets say we have 2 EBS with 10000 IOps We logically merge and use 20000 IOps RAID 1 Use for fault tolerance Mirror the EBS volume Example If we have 2 EBS volume We write on each of volume So even if one EBS volume fails, data is still exist in another one","title":"EBS RAID"},{"location":"Notes/AWS Storage/05 EBS/#attaching-a-new-ebs-volume","text":"When we attach a new EBS to EC2 instance, it is considered as block device. To make it usable, need to Format to a file system (AWS does not pre configure any file system to EBS) Mount it to the instance","title":"Attaching a New EBS Volume"},{"location":"Notes/AWS Storage/05 EBS/#detaching-existing-ebs-volume","text":"For root volume Stop the instance Detach the volume For non-root volume For running instance, un-mount and detach For stopped instance, detach, no need to un-mount","title":"Detaching Existing EBS Volume"},{"location":"Notes/Cloudfront And Global Accerator/01 Cloudfront/","text":"Cloudfront Overview CDN (Content Delivery Network) Content is cached in the Edge Location There are more than 200 Edge Location all over the world This is a global service Only support web distribution Improve Read Performance Can expose any Internal HTTP endpoints External HTTP endpoints (Even on-premise server) Security DDoS Protection AWS Firewall Shield Integration If the Cache Control header has max-age is 0 , all the request will go to the Origin Cloudfront Origin As S3 Bucket origin Use as Ingress i.e. upload file Distribute files Caching files at the Edge location OAI Origin Access Identity S3 Bucket only be accessed through Cloudfront As Custom Origin HTTP Application Load Balancer EC2 instance can be private SG of ALB must allow the Cloudfront Public IP EC2 Instance EC2 Instance must be public SG of the EC2 Instance must allow the Cloudfront Public IP S3 Static Website Any HTTP Backend (AWS Internal / on-premise) On Premise server can be used as a origin of Cloudfront Cloudfront Geo Restriction Two types of filtering Whitelist IP from Certain countries can access the content Blacklist IP from Certain countries can not access the content Determining the IP origin country is determined by using a 3rd party database Cloudfront vs S3 Cross Replication Cloudfront Use for static content When content must be available on almost all region Caching is for certain times (TTL) Use for both READ (Caching) WRITE (ingress) S3 Cross Replication Use for dynamic content When content must be available in certain region in very low latency Content is always available Use for READ Only Cloudfront Query String The delimiter character should be $ Parameter's name and values are case sensitive Cloudfront Origin Group Origin failover can be handled with using two origin Has two origin One is primary origin Other can be treated as secondary origin Cloudfront switch to secondary origin from primary origin if Primary origin fails Primary origin sends HTTP Fail Status Code Policies Cloudfront can ensure, From client object and request will be encrypted and use https When it comes to send response, if the object is not available in cache, cloudfront will fetch it from origin also in https format To enable both client -> cloudfront -> origin in https, we will need Viewer Protocol Policy (When HTTPS, there is ssl certificate installed in cloudfront) Origin Protocol Policy Cache Invalidation When new object is uploaded, to invalidate previous cache We can invalidate instantly We can wait for the existing cache to be invalidated Use versioned name TTL To make a object for a certain time, Configure origin to add a Cache-Control or Expires-Header Specify minimum TTL to the Cloudfront Cache Behevior Default value of TTL is 24 hours","title":"01 Cloudfront"},{"location":"Notes/Cloudfront And Global Accerator/01 Cloudfront/#cloudfront-overview","text":"CDN (Content Delivery Network) Content is cached in the Edge Location There are more than 200 Edge Location all over the world This is a global service Only support web distribution Improve Read Performance Can expose any Internal HTTP endpoints External HTTP endpoints (Even on-premise server) Security DDoS Protection AWS Firewall Shield Integration If the Cache Control header has max-age is 0 , all the request will go to the Origin","title":"Cloudfront Overview"},{"location":"Notes/Cloudfront And Global Accerator/01 Cloudfront/#cloudfront-origin","text":"As S3 Bucket origin Use as Ingress i.e. upload file Distribute files Caching files at the Edge location OAI Origin Access Identity S3 Bucket only be accessed through Cloudfront As Custom Origin HTTP Application Load Balancer EC2 instance can be private SG of ALB must allow the Cloudfront Public IP EC2 Instance EC2 Instance must be public SG of the EC2 Instance must allow the Cloudfront Public IP S3 Static Website Any HTTP Backend (AWS Internal / on-premise) On Premise server can be used as a origin of Cloudfront","title":"Cloudfront Origin"},{"location":"Notes/Cloudfront And Global Accerator/01 Cloudfront/#cloudfront-geo-restriction","text":"Two types of filtering Whitelist IP from Certain countries can access the content Blacklist IP from Certain countries can not access the content Determining the IP origin country is determined by using a 3rd party database","title":"Cloudfront Geo Restriction"},{"location":"Notes/Cloudfront And Global Accerator/01 Cloudfront/#cloudfront-vs-s3-cross-replication","text":"","title":"Cloudfront vs S3 Cross Replication"},{"location":"Notes/Cloudfront And Global Accerator/01 Cloudfront/#cloudfront","text":"Use for static content When content must be available on almost all region Caching is for certain times (TTL) Use for both READ (Caching) WRITE (ingress)","title":"Cloudfront"},{"location":"Notes/Cloudfront And Global Accerator/01 Cloudfront/#s3-cross-replication","text":"Use for dynamic content When content must be available in certain region in very low latency Content is always available Use for READ Only","title":"S3 Cross Replication"},{"location":"Notes/Cloudfront And Global Accerator/01 Cloudfront/#cloudfront-query-string","text":"The delimiter character should be $ Parameter's name and values are case sensitive","title":"Cloudfront Query String"},{"location":"Notes/Cloudfront And Global Accerator/01 Cloudfront/#cloudfront-origin-group","text":"Origin failover can be handled with using two origin Has two origin One is primary origin Other can be treated as secondary origin Cloudfront switch to secondary origin from primary origin if Primary origin fails Primary origin sends HTTP Fail Status Code","title":"Cloudfront Origin Group"},{"location":"Notes/Cloudfront And Global Accerator/01 Cloudfront/#policies","text":"Cloudfront can ensure, From client object and request will be encrypted and use https When it comes to send response, if the object is not available in cache, cloudfront will fetch it from origin also in https format To enable both client -> cloudfront -> origin in https, we will need Viewer Protocol Policy (When HTTPS, there is ssl certificate installed in cloudfront) Origin Protocol Policy","title":"Policies"},{"location":"Notes/Cloudfront And Global Accerator/01 Cloudfront/#cache-invalidation","text":"When new object is uploaded, to invalidate previous cache We can invalidate instantly We can wait for the existing cache to be invalidated Use versioned name","title":"Cache Invalidation"},{"location":"Notes/Cloudfront And Global Accerator/01 Cloudfront/#ttl","text":"To make a object for a certain time, Configure origin to add a Cache-Control or Expires-Header Specify minimum TTL to the Cloudfront Cache Behevior Default value of TTL is 24 hours","title":"TTL"},{"location":"Notes/Cloudfront And Global Accerator/02 Cloudfront Signed URL or Cloudfront Signed Cookies/","text":"Cloudfront Signed URL or Cloudfront Signed Cookies Signed URL To distribute premium contents Should have a policy With URL expiration Define IP Ranges that can access the content Trusted Signers (Who can create signed URLS) Signed URL vs Signed Cookies Signed URL for single content Signed Cookies for multiple content Cloudfront Signed URL vs S3-presigned URL Cloudfront Signed URL Allow a path, no matter the origin Account wide Key-pair, only root can manage it Filter by IP Date Expiration Leverage caching features S3 Pre-Signed URL Only allow S3 as origin Use the IAM key of the signer, (Pre-sign URL has the same principle as the signer) Filter by Expiration No caching available","title":"02 Cloudfront Signed URL or Cloudfront Signed Cookies"},{"location":"Notes/Cloudfront And Global Accerator/02 Cloudfront Signed URL or Cloudfront Signed Cookies/#cloudfront-signed-url-or-cloudfront-signed-cookies","text":"","title":"Cloudfront Signed URL or Cloudfront Signed Cookies"},{"location":"Notes/Cloudfront And Global Accerator/02 Cloudfront Signed URL or Cloudfront Signed Cookies/#signed-url","text":"To distribute premium contents Should have a policy With URL expiration Define IP Ranges that can access the content Trusted Signers (Who can create signed URLS)","title":"Signed URL"},{"location":"Notes/Cloudfront And Global Accerator/02 Cloudfront Signed URL or Cloudfront Signed Cookies/#signed-url-vs-signed-cookies","text":"Signed URL for single content Signed Cookies for multiple content","title":"Signed URL vs Signed Cookies"},{"location":"Notes/Cloudfront And Global Accerator/02 Cloudfront Signed URL or Cloudfront Signed Cookies/#cloudfront-signed-url-vs-s3-presigned-url","text":"","title":"Cloudfront Signed URL vs S3-presigned URL"},{"location":"Notes/Cloudfront And Global Accerator/02 Cloudfront Signed URL or Cloudfront Signed Cookies/#cloudfront-signed-url","text":"Allow a path, no matter the origin Account wide Key-pair, only root can manage it Filter by IP Date Expiration Leverage caching features","title":"Cloudfront Signed URL"},{"location":"Notes/Cloudfront And Global Accerator/02 Cloudfront Signed URL or Cloudfront Signed Cookies/#s3-pre-signed-url","text":"Only allow S3 as origin Use the IAM key of the signer, (Pre-sign URL has the same principle as the signer) Filter by Expiration No caching available","title":"S3 Pre-Signed URL"},{"location":"Notes/Cloudfront And Global Accerator/03 Global Accelerator/","text":"Global Accelerator Discussed in the VPC section Problem When we have an Application in another region, we have to reach that application through lots of ISP Provider When EC2 has a Public IP and some regions having difficulties with this Public IP access Since AWS Accelerator is using Any Cast , this problem can be resolved UniCast and Any Cast IP Uni Cast IP means each server has one IP Any Cast Ip means Multiple server has same IP Traffic routed to nearest server AWS Global Accelerator uses the Any Cast IP concept Overview Use 2 Any Cast IP Leverage AWS Internal Network Any Cast IP send traffic to the nearest AWS Edge Location From the Edge Location , traffic goes to the server using AWS Internal Network Works with Elastic IP Public/Private EC2 Instance Public/Private ALB Public/Private NLB Performance User intelligent routing to ensure lowest latency Use Internal AWS Network Has health check If issue with an application, has automatic failover feature So good for disaster recovery Security Only 2 IP needs to be white listed DDoS Protection by AWS Shield Global Accelerator Vs Cloudfront Both use AWS Global Network i.e. Edge Locations Both use Shield for DDoS protection Cloudfront Cache Content Global Accelerator Improve performance for TCL and UDP Has Failover","title":"03 Global Accelerator"},{"location":"Notes/Cloudfront And Global Accerator/03 Global Accelerator/#global-accelerator","text":"Discussed in the VPC section","title":"Global Accelerator"},{"location":"Notes/Cloudfront And Global Accerator/03 Global Accelerator/#problem","text":"When we have an Application in another region, we have to reach that application through lots of ISP Provider When EC2 has a Public IP and some regions having difficulties with this Public IP access Since AWS Accelerator is using Any Cast , this problem can be resolved","title":"Problem"},{"location":"Notes/Cloudfront And Global Accerator/03 Global Accelerator/#unicast-and-any-cast-ip","text":"Uni Cast IP means each server has one IP Any Cast Ip means Multiple server has same IP Traffic routed to nearest server AWS Global Accelerator uses the Any Cast IP concept","title":"UniCast and Any Cast IP"},{"location":"Notes/Cloudfront And Global Accerator/03 Global Accelerator/#overview","text":"Use 2 Any Cast IP Leverage AWS Internal Network Any Cast IP send traffic to the nearest AWS Edge Location From the Edge Location , traffic goes to the server using AWS Internal Network Works with Elastic IP Public/Private EC2 Instance Public/Private ALB Public/Private NLB Performance User intelligent routing to ensure lowest latency Use Internal AWS Network Has health check If issue with an application, has automatic failover feature So good for disaster recovery Security Only 2 IP needs to be white listed DDoS Protection by AWS Shield","title":"Overview"},{"location":"Notes/Cloudfront And Global Accerator/03 Global Accelerator/#global-accelerator-vs-cloudfront","text":"Both use AWS Global Network i.e. Edge Locations Both use Shield for DDoS protection","title":"Global Accelerator Vs Cloudfront"},{"location":"Notes/Cloudfront And Global Accerator/03 Global Accelerator/#cloudfront","text":"Cache Content","title":"Cloudfront"},{"location":"Notes/Cloudfront And Global Accerator/03 Global Accelerator/#global-accelerator_1","text":"Improve performance for TCL and UDP Has Failover","title":"Global Accelerator"},{"location":"Notes/DB/01 Redshift/","text":"Redshift Its an OLAP i.e. Online Analytics Processing based on PostgreSQL Columnar Storage MPP i.e. Massively Parallel Query Pay as instance provisioned For long term consider using Reserved Instance Has SQL interface to perform query BI tools are integrated with it AWS Quicksight Tableau Data can be loaded from S3 DynamoDB DMS It can have nodes number from 1 to 128 Each node can contain 160GB data Two types of node Leader Node , do the planning and results aggregation Compute Node , perform queries and send results to the leader Using VPC Enhanced Routing , Redshift Clusters can be access through the AWS Private Network Snapshot and DR Snapshots are Point in time backup of a cluster Snapshots are stored in S3 Snapshots are incremental, on changes are saved Snapshots can be restored to a new cluster Snapshots can be copied to another AWS Region Manual snapshot does not delete automatically Automatic snapshot has an automatic retention period (35 days) Monitor performance of Redshift Cluster by Cloudwatch and AWS Trusted Advisor Can be enabled cross-region snapshots for Redshift Cluster Redshift Spectrum Direct query to S3 without loading Must have a Cluster available to start the query Query is submitted to thousands of Redshift Spectrum Nodes Best Practices To load data from S3 use the COPY command","title":"01 Redshift"},{"location":"Notes/DB/01 Redshift/#redshift","text":"Its an OLAP i.e. Online Analytics Processing based on PostgreSQL Columnar Storage MPP i.e. Massively Parallel Query Pay as instance provisioned For long term consider using Reserved Instance Has SQL interface to perform query BI tools are integrated with it AWS Quicksight Tableau Data can be loaded from S3 DynamoDB DMS It can have nodes number from 1 to 128 Each node can contain 160GB data Two types of node Leader Node , do the planning and results aggregation Compute Node , perform queries and send results to the leader Using VPC Enhanced Routing , Redshift Clusters can be access through the AWS Private Network","title":"Redshift"},{"location":"Notes/DB/01 Redshift/#snapshot-and-dr","text":"Snapshots are Point in time backup of a cluster Snapshots are stored in S3 Snapshots are incremental, on changes are saved Snapshots can be restored to a new cluster Snapshots can be copied to another AWS Region Manual snapshot does not delete automatically Automatic snapshot has an automatic retention period (35 days) Monitor performance of Redshift Cluster by Cloudwatch and AWS Trusted Advisor Can be enabled cross-region snapshots for Redshift Cluster","title":"Snapshot and DR"},{"location":"Notes/DB/01 Redshift/#redshift-spectrum","text":"Direct query to S3 without loading Must have a Cluster available to start the query Query is submitted to thousands of Redshift Spectrum Nodes","title":"Redshift Spectrum"},{"location":"Notes/DB/01 Redshift/#best-practices","text":"To load data from S3 use the COPY command","title":"Best Practices"},{"location":"Notes/DB/02 RDS/","text":"RDS OLTP i.e. Online Transaction Processing Relational Database -Managed PostgreSQL MySQL Oracle MSSQL Must provision EC2 Instance EBS Volume Support Read Replica for performance Multi AZ for digester recovery and availability Have Backup Snapshot Point in time restore Managed and scheduled maintenance Monitoring through Cloudwatch IAM authentication can be used as a feature in PostgreSQL MySQL To increase the number of db connection Create a parameter group Attach parameter group to DB Instance Change the parameter group settings When the db instance CPU is 100% and stopped working, we can Use read replica Use elastic cache in the application layer Shard data among multiple RDS DB instance For async database copy, use READ Replica While using READ Replica , in these READ Replica , there should be some replication lag When primary instance failed the CNAME of the DB Instance switch to Standby Instance Non supported oracle feature RMAN RAC In multi-AZ deployment, Standby Instance can not be used for read and write operations Automated Backup Take snapshot every 24 hours RDS take Snapshot of the whole database instance It captures the transaction logs of every 5 minutes A new DB Instance can be created from the from the DB Snapshot Encryption Un-encrypted database can not be encrypted on the fly (This is a limitation) Un-encrypted database read replica can not be encrypted To encrypted the un-encrypted database Create a DB snapshot Copy the snapshot Encrypt the copied snapshot Restore database from the Encrypted Snapshot To encrypt data while write and decrypt during read, there is Transparent Data Encryption or TDE TDE is for data encryption whereas the regular RDS Encryption is for encrypt the ec2 instance and ebs volume TDE is only applicable for the microsoft sql server Security IAM DB Authentication can be used for MySQL and PostgreSQL IAM DB Authentication has following feature SSL Encryption of Network Traffic Application runs in EC2 Instance can connect with database without password To enable security between Web Server and DB server Force ssl by rds.force_ssl Download RDS Root CA Certificate Monitoring Default Monitoring CPU Utilization Database Connection Free able Memory Enhanced Monitoring RDS Process OS Process Get logs of Audit Log Error Log General Log Slow query log","title":"02 RDS"},{"location":"Notes/DB/02 RDS/#rds","text":"OLTP i.e. Online Transaction Processing Relational Database -Managed PostgreSQL MySQL Oracle MSSQL Must provision EC2 Instance EBS Volume Support Read Replica for performance Multi AZ for digester recovery and availability Have Backup Snapshot Point in time restore Managed and scheduled maintenance Monitoring through Cloudwatch IAM authentication can be used as a feature in PostgreSQL MySQL To increase the number of db connection Create a parameter group Attach parameter group to DB Instance Change the parameter group settings When the db instance CPU is 100% and stopped working, we can Use read replica Use elastic cache in the application layer Shard data among multiple RDS DB instance For async database copy, use READ Replica While using READ Replica , in these READ Replica , there should be some replication lag When primary instance failed the CNAME of the DB Instance switch to Standby Instance Non supported oracle feature RMAN RAC In multi-AZ deployment, Standby Instance can not be used for read and write operations","title":"RDS"},{"location":"Notes/DB/02 RDS/#automated-backup","text":"Take snapshot every 24 hours RDS take Snapshot of the whole database instance It captures the transaction logs of every 5 minutes A new DB Instance can be created from the from the DB Snapshot","title":"Automated Backup"},{"location":"Notes/DB/02 RDS/#encryption","text":"Un-encrypted database can not be encrypted on the fly (This is a limitation) Un-encrypted database read replica can not be encrypted To encrypted the un-encrypted database Create a DB snapshot Copy the snapshot Encrypt the copied snapshot Restore database from the Encrypted Snapshot To encrypt data while write and decrypt during read, there is Transparent Data Encryption or TDE TDE is for data encryption whereas the regular RDS Encryption is for encrypt the ec2 instance and ebs volume TDE is only applicable for the microsoft sql server","title":"Encryption"},{"location":"Notes/DB/02 RDS/#security","text":"IAM DB Authentication can be used for MySQL and PostgreSQL IAM DB Authentication has following feature SSL Encryption of Network Traffic Application runs in EC2 Instance can connect with database without password To enable security between Web Server and DB server Force ssl by rds.force_ssl Download RDS Root CA Certificate","title":"Security"},{"location":"Notes/DB/02 RDS/#monitoring","text":"Default Monitoring CPU Utilization Database Connection Free able Memory Enhanced Monitoring RDS Process OS Process Get logs of Audit Log Error Log General Log Slow query log","title":"Monitoring"},{"location":"Notes/DB/03 Aurora/","text":"Aurora Proprietary database by AWS Data is replicated to 6 Replicas in 3 AZ Auto healing capability Multi AZ Auto scaling read-replica Storage from 10GB to 64TB Aurora Global database for DR and Latency Uses Need to provision EC2 Instance and EBS Volume Using Aurora Serverless does not require instance and volume 5x faster than regular RDMS Support MAX 15 Read Replicas We can create custom endpoint for Production Database Reporting queries Built in reader-endpoint can be used for distribute the traffic between read-replicas If DB Primary Instance fails, it create a new DB Instance in the same AZ as the Original Instance and done by Best Effort basis Read Replicas can be used to avoid un necessary downtime","title":"03 Aurora"},{"location":"Notes/DB/03 Aurora/#aurora","text":"Proprietary database by AWS Data is replicated to 6 Replicas in 3 AZ Auto healing capability Multi AZ Auto scaling read-replica Storage from 10GB to 64TB Aurora Global database for DR and Latency Uses Need to provision EC2 Instance and EBS Volume Using Aurora Serverless does not require instance and volume 5x faster than regular RDMS Support MAX 15 Read Replicas We can create custom endpoint for Production Database Reporting queries Built in reader-endpoint can be used for distribute the traffic between read-replicas If DB Primary Instance fails, it create a new DB Instance in the same AZ as the Original Instance and done by Best Effort basis Read Replicas can be used to avoid un necessary downtime","title":"Aurora"},{"location":"Notes/DB/04 Elastic Cache/","text":"Elastic Cache Key-value store In memory database Sub millisecond performance 2 types Memcached Redis Support Clustering (Redis) Multi AZ Read Replicas (sharding) Have Backup Snapshot Point in time recovery Scheduled Maintenance Monitoring through Cloudwatch Redis has authentication feature Redis Auth To secure the redis cluster access Use redis auth in transit encryption, enabled for clusters In most cases, it's better to use Redis. But should choice Memcached when, Required the simplest model possible Multi-thread or multi core supports Redis is not primarily designed for using the multi-cpu. Caching Strategies Lazy Loading : Loads data only when it is required Russian doll : Have nested records. Top level keys are the cahce keys for child resources. Write Througth : Adds item to cache when a item is added or updated. may cause cachec churn ie. lots of cache is not being used or read.","title":"04 Elastic Cache"},{"location":"Notes/DB/04 Elastic Cache/#elastic-cache","text":"Key-value store In memory database Sub millisecond performance 2 types Memcached Redis Support Clustering (Redis) Multi AZ Read Replicas (sharding) Have Backup Snapshot Point in time recovery Scheduled Maintenance Monitoring through Cloudwatch Redis has authentication feature Redis Auth To secure the redis cluster access Use redis auth in transit encryption, enabled for clusters In most cases, it's better to use Redis. But should choice Memcached when, Required the simplest model possible Multi-thread or multi core supports Redis is not primarily designed for using the multi-cpu.","title":"Elastic Cache"},{"location":"Notes/DB/04 Elastic Cache/#caching-strategies","text":"Lazy Loading : Loads data only when it is required Russian doll : Have nested records. Top level keys are the cahce keys for child resources. Write Througth : Adds item to cache when a item is added or updated. may cause cachec churn ie. lots of cache is not being used or read.","title":"Caching Strategies"},{"location":"Notes/DB/05 DynamoDB/","text":"Check the serverless section","title":"05 DynamoDB"},{"location":"Notes/DB/06 S3/","text":"Check the S3 section","title":"06 S3"},{"location":"Notes/DB/07 Athena/","text":"Check the S3 section","title":"07 Athena"},{"location":"Notes/DB/08 Neptune/","text":"Neptune Fully managed Graph Database Use for Highly Relational Data Available across 3AZ Can have 15 Read Replicas Has Point in time recovery Continuous Backup","title":"08 Neptune"},{"location":"Notes/DB/08 Neptune/#neptune","text":"Fully managed Graph Database Use for Highly Relational Data Available across 3AZ Can have 15 Read Replicas Has Point in time recovery Continuous Backup","title":"Neptune"},{"location":"Notes/DB/09 Elastic Search/","text":"Elastic Search Use for Searching and Indexing Open source Use to search any field or partially search It's common to use Elastic Search as a compliment of other databases Used in big data application Need to provision a cluster of instances Built in integrations of Amazon Kinesis Data Firehose AWS IoT Cloudwatch Cloudwatch logs ELK stack stands for Elastic Search Logstash (Log Ingestion) Kibana (Visualization)","title":"09 Elastic Search"},{"location":"Notes/DB/09 Elastic Search/#elastic-search","text":"Use for Searching and Indexing Open source Use to search any field or partially search It's common to use Elastic Search as a compliment of other databases Used in big data application Need to provision a cluster of instances Built in integrations of Amazon Kinesis Data Firehose AWS IoT Cloudwatch Cloudwatch logs ELK stack stands for Elastic Search Logstash (Log Ingestion) Kibana (Visualization)","title":"Elastic Search"},{"location":"Notes/Decoupling Application/01 SQS/","text":"SQS Overview Fully manage message broker system Unlimited throughput Unlimited number of messages Retention period Default 4 days Max 14 days Max message size 256 KB Can have duplicate message At least once delivery Can have out of order Use best effort ordering Applicable for SQS Standard FIFO SQS maintain order but limited throughput Message can be produced by the SQS SDK Message can be consumed by AWS Lambda , EC2 Instance Consumer can pull max 10 messages at a time After process the message, the consumer has to delete the message, otherwise the message will be appeared in the queue again Can be use to handle extended number of db writ operations We can ignore default message size from 256 KB to up to 2GB by making use of S3 and SQS Extended library for Java Security IAM Policies can be used to regulate the SQS API SQS Access Policy Use for Cross Account Access Allowing other service like S3 , SNS Encryption In flight encryption using the HTTPS At rest encryption using KMS Client side encryption Encryption be done by Message Producers Decryption can be by the Message Consumers Message Visibility Timeout After message is polled by a consumer, the message become invisible for other consumer Default visibility timeout is 30 Seconds Is message is not proceed by the consumer within the visibility timeout, it will appear again in the queue and might possibility to proceed again by another consumer If message require more time than Visibility Timeout , the consumer can call the api ChangeMessageVisibility to increase Message Visibility Timeout Best practices If the Message Visibility Time too high and consumer crashes, it took long time to re appear the message and proceed If Message Visibility Time is too low, it is possible to duplicate processing Better to set minimum Message Visibility Time and increase by calling ChangeMessageVisibility api if necessary Dead Letter Queue If consumer fails to process a message couple of times it can be passed to the DLQ We can set the threshold, how many fails attempts to allow Best practice to set retention period of 14 days for dead letter queue Delay Queue Using delay queue the message will be appeared in the queue after certain times Time Default is 0 Sec Could be max 15 Min FIFO Queue Maintain Orders (First In First Out) Limited throughput 300 msg/s without batching 3000 msg/s with batching De duplication ensure multiple message with same ID does not appear When message has identical bodies, use unique de duplication id When message has unique message bodies, use content-based de duplication ID Message group Id ensure the order of message is being proceed De duplication id prevent duplication, group id ensure message is being proceed A regular queue can not be converted to a FIFO queue. I should be created from scratch SQS With ASG ASG can be implemented to scale the Consumers Using Cloudwatch ApproximateNumberOfMessages can be determined number of messages in the queue Using Cloudwatch ApproximateNumberOfMessages can be scaled up/down the EC2 Instances Need two alarm to scale up/down the instance Step Scaling is being used here Migration, Queue to Fifo Queue Need Message de-duplication Id (As token while sending the message, ensure no message duplication happen) Message group ID (As tag, to make message group, ensure order of message been proceed) Performance Use long poling Use batch processing","title":"01 SQS"},{"location":"Notes/Decoupling Application/01 SQS/#sqs","text":"","title":"SQS"},{"location":"Notes/Decoupling Application/01 SQS/#overview","text":"Fully manage message broker system Unlimited throughput Unlimited number of messages Retention period Default 4 days Max 14 days Max message size 256 KB Can have duplicate message At least once delivery Can have out of order Use best effort ordering Applicable for SQS Standard FIFO SQS maintain order but limited throughput Message can be produced by the SQS SDK Message can be consumed by AWS Lambda , EC2 Instance Consumer can pull max 10 messages at a time After process the message, the consumer has to delete the message, otherwise the message will be appeared in the queue again Can be use to handle extended number of db writ operations We can ignore default message size from 256 KB to up to 2GB by making use of S3 and SQS Extended library for Java","title":"Overview"},{"location":"Notes/Decoupling Application/01 SQS/#security","text":"IAM Policies can be used to regulate the SQS API SQS Access Policy Use for Cross Account Access Allowing other service like S3 , SNS Encryption In flight encryption using the HTTPS At rest encryption using KMS Client side encryption Encryption be done by Message Producers Decryption can be by the Message Consumers","title":"Security"},{"location":"Notes/Decoupling Application/01 SQS/#message-visibility-timeout","text":"After message is polled by a consumer, the message become invisible for other consumer Default visibility timeout is 30 Seconds Is message is not proceed by the consumer within the visibility timeout, it will appear again in the queue and might possibility to proceed again by another consumer If message require more time than Visibility Timeout , the consumer can call the api ChangeMessageVisibility to increase Message Visibility Timeout Best practices If the Message Visibility Time too high and consumer crashes, it took long time to re appear the message and proceed If Message Visibility Time is too low, it is possible to duplicate processing Better to set minimum Message Visibility Time and increase by calling ChangeMessageVisibility api if necessary","title":"Message Visibility Timeout"},{"location":"Notes/Decoupling Application/01 SQS/#dead-letter-queue","text":"If consumer fails to process a message couple of times it can be passed to the DLQ We can set the threshold, how many fails attempts to allow Best practice to set retention period of 14 days for dead letter queue","title":"Dead Letter Queue"},{"location":"Notes/Decoupling Application/01 SQS/#delay-queue","text":"Using delay queue the message will be appeared in the queue after certain times Time Default is 0 Sec Could be max 15 Min","title":"Delay Queue"},{"location":"Notes/Decoupling Application/01 SQS/#fifo-queue","text":"Maintain Orders (First In First Out) Limited throughput 300 msg/s without batching 3000 msg/s with batching De duplication ensure multiple message with same ID does not appear When message has identical bodies, use unique de duplication id When message has unique message bodies, use content-based de duplication ID Message group Id ensure the order of message is being proceed De duplication id prevent duplication, group id ensure message is being proceed A regular queue can not be converted to a FIFO queue. I should be created from scratch","title":"FIFO Queue"},{"location":"Notes/Decoupling Application/01 SQS/#sqs-with-asg","text":"ASG can be implemented to scale the Consumers Using Cloudwatch ApproximateNumberOfMessages can be determined number of messages in the queue Using Cloudwatch ApproximateNumberOfMessages can be scaled up/down the EC2 Instances Need two alarm to scale up/down the instance Step Scaling is being used here","title":"SQS With ASG"},{"location":"Notes/Decoupling Application/01 SQS/#migration-queue-to-fifo-queue","text":"Need Message de-duplication Id (As token while sending the message, ensure no message duplication happen) Message group ID (As tag, to make message group, ensure order of message been proceed)","title":"Migration, Queue to Fifo Queue"},{"location":"Notes/Decoupling Application/01 SQS/#performance","text":"Use long poling Use batch processing","title":"Performance"},{"location":"Notes/Decoupling Application/02 SNS/","text":"SNS Simple Notification Service Pub/Sub Model Event Producer send the message to the SNS Event Receiver receive notification from the SNS Subscriber can be SQS HTTP/HTTPS Endpoint Lambda Function Email SMS Mobile Notification Filtering: Using filter in policy to filter message before publish Use Case Cloudwatch alarm ASG Notification S3 bucket Events Cloudformation State Change Publish Topic Publish Create a topic Create subscription Publish Topic Direct Publish (In Platform) Create Platform Create Platform Endpoint Publish to Platform Endpoint Works with Google GCM Apple APNS Amazon ADM Security Encryption In flight encryption by HTTPS endpoint At rest encryption by KMS Client side encryption and decryption Publisher is responsible to encrypt the message Subscriber is responsible to decrypt the message Access Controls by IAM Policy SNS Access Policy Sharing cross-account SNS Allow other service to allow Publish Topic","title":"02 SNS"},{"location":"Notes/Decoupling Application/02 SNS/#sns","text":"Simple Notification Service Pub/Sub Model Event Producer send the message to the SNS Event Receiver receive notification from the SNS Subscriber can be SQS HTTP/HTTPS Endpoint Lambda Function Email SMS Mobile Notification Filtering: Using filter in policy to filter message before publish Use Case Cloudwatch alarm ASG Notification S3 bucket Events Cloudformation State Change Publish Topic Publish Create a topic Create subscription Publish Topic Direct Publish (In Platform) Create Platform Create Platform Endpoint Publish to Platform Endpoint Works with Google GCM Apple APNS Amazon ADM Security Encryption In flight encryption by HTTPS endpoint At rest encryption by KMS Client side encryption and decryption Publisher is responsible to encrypt the message Subscriber is responsible to decrypt the message Access Controls by IAM Policy SNS Access Policy Sharing cross-account SNS Allow other service to allow Publish Topic","title":"SNS"},{"location":"Notes/Decoupling Application/03 Fan Out/","text":"Fan Out SNS + SQS Push event in one SNS and received by many SQS SQS access policy should allow SNS to publish message FIFO SQS can not be a subscriber to a SNS Used when one topic should be published to multiple SQS","title":"03 Fan Out"},{"location":"Notes/Decoupling Application/03 Fan Out/#fan-out","text":"SNS + SQS Push event in one SNS and received by many SQS SQS access policy should allow SNS to publish message FIFO SQS can not be a subscriber to a SNS Used when one topic should be published to multiple SQS","title":"Fan Out"},{"location":"Notes/Decoupling Application/04 Kinesis Data Stream/","text":"Kinesis Data Stream Managed Apache Kafka Used for Real Time Big Data Cover streaming processing framework like Spark NiFi Data is replication across 3 AZ 3 tools included Kinesis Stream (Low latency streaming at scale) Kinesis Analytics (Real time analytics using SQL) Kinesis Firehose (Load data to S3 , RedShift , Elastic Search ) Security Control Access by IAM Policy Encryption In flight by HTTPS Install SSL certificate kinesis Send data through SSL At rest by KMS Encryption is enabled at rest Ensure streams are transferring data from producers Client side encryption VPC Endpoints are available to access Kinesis through AWS Private Network Stream Shards One stream is combination of multiple shard Each shard throughput Read 2MB/s Write 1MB/s or 1000 message/s Batching is available to reduce the cost and increase throughput Number of shards can be merged or re-shards over time Records are ordered per shard Multiple shard can not ensure the ordering Merging shards to process less data Splitting shards to process more data Kinesis API (Put Records) Same key always go to same partition Partition keys should be highly distributed, otherwise it cause Hot Partition Problem If partition key is userID, it is highly distributed If partition key is country code and 90% users are from the same country then 90% data will go to the same shard ProvisionThroughputExceed happens when we go over limit Hot Sharding Happen Solution of ProvisionThroughputExceed be Retry Increase shard Ensure partition key is a good one As consumer we can use CLI SDK Kinesis Client Library i.e. KCL available for almost all major languages 01 Kinesis Stream Streams are divided into shards Data retention period Default 1 Day Max 7 Days Data can be proceed multiple time (In SQS we can process message only one time) Multiple consumer can consume the data Once data is inserted in the shards it can not be deleted 02 Kinesis Firehose Managed Service No Administration Auto Scaling Serverless Load data to S3 Elastic Search Redshift Splunk Near real time 60 Sec Or Minimum 32Mb Can get data from Kinesis Stream KCL (Kinesis Client Library) 03 Kinesis Data Analytics Perform real time analytics using SQL Features Auto Scaling Managed Real time Firehose Vs Streams Streams Custom code for producer and consumer Real time Must manage scaling (Re shards and merging) Data store for 1-7 days Multi consumers Replay capability Firehose Fully managed, serverless Near real time Automated scaling No data storage Re-Sharding Enables increase or decrease of the number of shards in the stream. We can increase instance size and shards number to handle more data. Kinesis Client Library (KCL) In instances, used to process data from data stream. There should be same number of kcl as well as the ec2 instances as the number of open shards. To process data, if we make use of the lambda functions, to get optimal performance, need to have same number of concurrent lambda function same as the shard number. Enhanced Fan Out Enabled shards get data 2MB/s per shard with 70s propagation delay to all consumers.","title":"04 Kinesis Data Stream"},{"location":"Notes/Decoupling Application/04 Kinesis Data Stream/#kinesis-data-stream","text":"Managed Apache Kafka Used for Real Time Big Data Cover streaming processing framework like Spark NiFi Data is replication across 3 AZ 3 tools included Kinesis Stream (Low latency streaming at scale) Kinesis Analytics (Real time analytics using SQL) Kinesis Firehose (Load data to S3 , RedShift , Elastic Search )","title":"Kinesis Data Stream"},{"location":"Notes/Decoupling Application/04 Kinesis Data Stream/#security","text":"Control Access by IAM Policy Encryption In flight by HTTPS Install SSL certificate kinesis Send data through SSL At rest by KMS Encryption is enabled at rest Ensure streams are transferring data from producers Client side encryption VPC Endpoints are available to access Kinesis through AWS Private Network","title":"Security"},{"location":"Notes/Decoupling Application/04 Kinesis Data Stream/#stream-shards","text":"One stream is combination of multiple shard Each shard throughput Read 2MB/s Write 1MB/s or 1000 message/s Batching is available to reduce the cost and increase throughput Number of shards can be merged or re-shards over time Records are ordered per shard Multiple shard can not ensure the ordering Merging shards to process less data Splitting shards to process more data","title":"Stream Shards"},{"location":"Notes/Decoupling Application/04 Kinesis Data Stream/#kinesis-api-put-records","text":"Same key always go to same partition Partition keys should be highly distributed, otherwise it cause Hot Partition Problem If partition key is userID, it is highly distributed If partition key is country code and 90% users are from the same country then 90% data will go to the same shard ProvisionThroughputExceed happens when we go over limit Hot Sharding Happen Solution of ProvisionThroughputExceed be Retry Increase shard Ensure partition key is a good one As consumer we can use CLI SDK Kinesis Client Library i.e. KCL available for almost all major languages","title":"Kinesis API (Put Records)"},{"location":"Notes/Decoupling Application/04 Kinesis Data Stream/#01-kinesis-stream","text":"Streams are divided into shards Data retention period Default 1 Day Max 7 Days Data can be proceed multiple time (In SQS we can process message only one time) Multiple consumer can consume the data Once data is inserted in the shards it can not be deleted","title":"01 Kinesis Stream"},{"location":"Notes/Decoupling Application/04 Kinesis Data Stream/#02-kinesis-firehose","text":"Managed Service No Administration Auto Scaling Serverless Load data to S3 Elastic Search Redshift Splunk Near real time 60 Sec Or Minimum 32Mb Can get data from Kinesis Stream KCL (Kinesis Client Library)","title":"02 Kinesis Firehose"},{"location":"Notes/Decoupling Application/04 Kinesis Data Stream/#03-kinesis-data-analytics","text":"Perform real time analytics using SQL Features Auto Scaling Managed Real time","title":"03 Kinesis Data Analytics"},{"location":"Notes/Decoupling Application/04 Kinesis Data Stream/#firehose-vs-streams","text":"Streams Custom code for producer and consumer Real time Must manage scaling (Re shards and merging) Data store for 1-7 days Multi consumers Replay capability Firehose Fully managed, serverless Near real time Automated scaling No data storage","title":"Firehose Vs Streams"},{"location":"Notes/Decoupling Application/04 Kinesis Data Stream/#re-sharding","text":"Enables increase or decrease of the number of shards in the stream. We can increase instance size and shards number to handle more data.","title":"Re-Sharding"},{"location":"Notes/Decoupling Application/04 Kinesis Data Stream/#kinesis-client-library-kcl","text":"In instances, used to process data from data stream. There should be same number of kcl as well as the ec2 instances as the number of open shards. To process data, if we make use of the lambda functions, to get optimal performance, need to have same number of concurrent lambda function same as the shard number.","title":"Kinesis Client Library (KCL)"},{"location":"Notes/Decoupling Application/04 Kinesis Data Stream/#enhanced-fan-out","text":"Enabled shards get data 2MB/s per shard with 70s propagation delay to all consumers.","title":"Enhanced Fan Out"},{"location":"Notes/Decoupling Application/05 Amazon MQ/","text":"Amazon MQ Managed Apache ActiveMQ Amazon MQ has both Queue feature Topic feature Compatible with MQTT AMQP STOMP OPEN-WIRE WSS SQS and SNS are proprietary tech If application use existing open-source message broker system, then need to use Amazon MQ Amazon MQ does not scale as SQS and SNS Amazon MQ run on dedicated machine, can be used as HA Failover","title":"05 Amazon MQ"},{"location":"Notes/Decoupling Application/05 Amazon MQ/#amazon-mq","text":"Managed Apache ActiveMQ Amazon MQ has both Queue feature Topic feature Compatible with MQTT AMQP STOMP OPEN-WIRE WSS SQS and SNS are proprietary tech If application use existing open-source message broker system, then need to use Amazon MQ Amazon MQ does not scale as SQS and SNS Amazon MQ run on dedicated machine, can be used as HA Failover","title":"Amazon MQ"},{"location":"Notes/Disaster Recovery & Migrations/01 Disaster Recovery/","text":"Disaster Recovery DR is all about preparing and recovering from disaster DR can be On-premise to On-premise is an expensive one On-premise to AWS Cloud is hybrid one AWS Cloud to AWS Cloud Could be multi-AZ Could be cross-region AWS Service for DR Backup EBS Snapshots (copy to destination region) AMI Snapshots (Copy to destination region) RDS auto backup/snapshot S3 replication, glacier, lifecycle policy, cross-region-replication snowball, snowmobile, storage gateway HA cross-region multi-az site-to-site vpn, direct connect Replication RDS, Aurora Global Automation Cloudformation Elastic Beanstalk AWS lambda Chaos simian army","title":"01 Disaster Recovery"},{"location":"Notes/Disaster Recovery & Migrations/01 Disaster Recovery/#disaster-recovery","text":"DR is all about preparing and recovering from disaster DR can be On-premise to On-premise is an expensive one On-premise to AWS Cloud is hybrid one AWS Cloud to AWS Cloud Could be multi-AZ Could be cross-region AWS Service for DR Backup EBS Snapshots (copy to destination region) AMI Snapshots (Copy to destination region) RDS auto backup/snapshot S3 replication, glacier, lifecycle policy, cross-region-replication snowball, snowmobile, storage gateway HA cross-region multi-az site-to-site vpn, direct connect Replication RDS, Aurora Global Automation Cloudformation Elastic Beanstalk AWS lambda Chaos simian army","title":"Disaster Recovery"},{"location":"Notes/Disaster Recovery & Migrations/02 RPO & RTO/","text":"RPO & RTO RPO stands for Recovery Point Objective How often backup being taken When disaster happen, time between RPO and Disaster is a Data Loss If we take backups more frequently Data Loss will be minimized In this case, it become expensive also RTO stands for Recovery Time Objective RTO is the time to recover from the Disaster When Disaster happen, time between Disaster and Time to Make Application Online is the Downtime The Lower Time the RPO and RTO , the better the application, also become expensive","title":"02 RPO & RTO"},{"location":"Notes/Disaster Recovery & Migrations/02 RPO & RTO/#rpo-rto","text":"RPO stands for Recovery Point Objective How often backup being taken When disaster happen, time between RPO and Disaster is a Data Loss If we take backups more frequently Data Loss will be minimized In this case, it become expensive also RTO stands for Recovery Time Objective RTO is the time to recover from the Disaster When Disaster happen, time between Disaster and Time to Make Application Online is the Downtime The Lower Time the RPO and RTO , the better the application, also become expensive","title":"RPO &amp; RTO"},{"location":"Notes/Disaster Recovery & Migrations/03 Disaster Recovery Strategies/","text":"Disaster Recovery Strategies 4 types of Disaster Recovery Strategies Backup and Restore Pilot Light Warm Standby Hot Site / Multi Site Approach Backup And Restore Data and applications backup is taken time to time When Disaster happen, restore these data and spin the application Has very high RPO and RTO and comparatively cheaper Pilot Light Application most critical part is always standby in other places In case of Disaster , add the non-critical part Not whole system is standby, only critical part is standby Warm Standby Minimum version of the whole system is standby Hot Site / Multi Site Approach Whole system is standby in another az or region or from op-premise to aws-cloud Most expensive one","title":"03 Disaster Recovery Strategies"},{"location":"Notes/Disaster Recovery & Migrations/03 Disaster Recovery Strategies/#disaster-recovery-strategies","text":"4 types of Disaster Recovery Strategies Backup and Restore Pilot Light Warm Standby Hot Site / Multi Site Approach","title":"Disaster Recovery Strategies"},{"location":"Notes/Disaster Recovery & Migrations/03 Disaster Recovery Strategies/#backup-and-restore","text":"Data and applications backup is taken time to time When Disaster happen, restore these data and spin the application Has very high RPO and RTO and comparatively cheaper","title":"Backup And Restore"},{"location":"Notes/Disaster Recovery & Migrations/03 Disaster Recovery Strategies/#pilot-light","text":"Application most critical part is always standby in other places In case of Disaster , add the non-critical part Not whole system is standby, only critical part is standby","title":"Pilot Light"},{"location":"Notes/Disaster Recovery & Migrations/03 Disaster Recovery Strategies/#warm-standby","text":"Minimum version of the whole system is standby","title":"Warm Standby"},{"location":"Notes/Disaster Recovery & Migrations/03 Disaster Recovery Strategies/#hot-site-multi-site-approach","text":"Whole system is standby in another az or region or from op-premise to aws-cloud Most expensive one","title":"Hot Site / Multi Site Approach"},{"location":"Notes/Disaster Recovery & Migrations/04 DMS/","text":"DMS DMS stands for Database Migration Service Use to migrate database from on-premise to RDS Supports Homogeneous Migration Source and destination DB has same data base engine Heterogeneous Migration Source and destination DB has different engine Require SCT i.e. Schema Conversion Tool While use SCT , first put the converted data to S3 Then put all the data to the Database It is CDC i.e. Continuous Data Migration If there is a change in source database, it is replicated to the destination database as well DMS needs to run in EC2 instance In order to allow DMS , the instance should have suitable AWS Identity and IAM Policy ( important ) This enable migrating production database without any interruption","title":"04 DMS"},{"location":"Notes/Disaster Recovery & Migrations/04 DMS/#dms","text":"DMS stands for Database Migration Service Use to migrate database from on-premise to RDS Supports Homogeneous Migration Source and destination DB has same data base engine Heterogeneous Migration Source and destination DB has different engine Require SCT i.e. Schema Conversion Tool While use SCT , first put the converted data to S3 Then put all the data to the Database It is CDC i.e. Continuous Data Migration If there is a change in source database, it is replicated to the destination database as well DMS needs to run in EC2 instance In order to allow DMS , the instance should have suitable AWS Identity and IAM Policy ( important ) This enable migrating production database without any interruption","title":"DMS"},{"location":"Notes/Disaster Recovery & Migrations/05 On Premise Strategy/","text":"On Premise Strategy Amazon Linux 2 can be downloaded to on-premise server. VM Import / Export Standby an AMI in VM and put it in AWS Cloud in case of Disaster Application Discovery Service Gather info about on-premise servers and plan migration AWS Server Migration Service , i.e. SMS Incremental replication of live server to AWS Automate, schedule, track incremental of live volumes Can be used for Hyper V machines","title":"05 On Premise Strategy"},{"location":"Notes/Disaster Recovery & Migrations/05 On Premise Strategy/#on-premise-strategy","text":"Amazon Linux 2 can be downloaded to on-premise server. VM Import / Export Standby an AMI in VM and put it in AWS Cloud in case of Disaster Application Discovery Service Gather info about on-premise servers and plan migration AWS Server Migration Service , i.e. SMS Incremental replication of live server to AWS Automate, schedule, track incremental of live volumes Can be used for Hyper V machines","title":"On Premise Strategy"},{"location":"Notes/Disaster Recovery & Migrations/06 Data Sync/","text":"Data Sync Use to transfer Large Data Sets Work with SMB and NFS protocol Data can be transferred from on-premise to S3 EFS FSx (for windows) Data can be transferred from EFS to EFS A Data Sync Client need to implement in the client It looks for the changes Transfer data time to time Hourly Daily Weekly While transfer data from on-premise to EFS , to ensure all data is copied, we can do the following for faster transfer Disable verification during initial file transfer Enable it post last data transfer","title":"06 Data Sync"},{"location":"Notes/Disaster Recovery & Migrations/06 Data Sync/#data-sync","text":"Use to transfer Large Data Sets Work with SMB and NFS protocol Data can be transferred from on-premise to S3 EFS FSx (for windows) Data can be transferred from EFS to EFS A Data Sync Client need to implement in the client It looks for the changes Transfer data time to time Hourly Daily Weekly While transfer data from on-premise to EFS , to ensure all data is copied, we can do the following for faster transfer Disable verification during initial file transfer Enable it post last data transfer","title":"Data Sync"},{"location":"Notes/Disaster Recovery & Migrations/07 Transferring Large Data Sets/","text":"Transferring Large Data Sets Sending 200TB data with 100Mbps through public Internet ( Site to Site VPN ) take 185 days Sending 200TB data with 1Gbps connection through Aws Direct Connect take 18 days Also consider 30 days to set up AWS Direct Connect Using snowball might take 7 days consider it will come to facility in 2-3 days","title":"07 Transferring Large Data Sets"},{"location":"Notes/Disaster Recovery & Migrations/07 Transferring Large Data Sets/#transferring-large-data-sets","text":"Sending 200TB data with 100Mbps through public Internet ( Site to Site VPN ) take 185 days Sending 200TB data with 1Gbps connection through Aws Direct Connect take 18 days Also consider 30 days to set up AWS Direct Connect Using snowball might take 7 days consider it will come to facility in 2-3 days","title":"Transferring Large Data Sets"},{"location":"Notes/EC2/01 EC2 Instance Recovery/","text":"EC2 Instance Recovery Using Cloudwatch Alarm we can recover EC2 Instance It recover previous Public IP Private IP Elastic IP (if attached) Metadata Placement Group It does not recover Instance Store Set up Check EC2 Instance Check EC2 Instance Underlying Hardware SNS Topic can be used to notify this incident","title":"01 EC2 Instance Recovery"},{"location":"Notes/EC2/01 EC2 Instance Recovery/#ec2-instance-recovery","text":"Using Cloudwatch Alarm we can recover EC2 Instance It recover previous Public IP Private IP Elastic IP (if attached) Metadata Placement Group It does not recover Instance Store Set up Check EC2 Instance Check EC2 Instance Underlying Hardware SNS Topic can be used to notify this incident","title":"EC2 Instance Recovery"},{"location":"Notes/EC2/02 EC2 Fundamentals/","text":"EC2 Fundamentals While restart an EC2 instance Public IP changes Both IP6 and private IP does not change It is possible to change the underlying hardware Pre warm for minimum launching time Launch EC2 Instance with EBS Volume Enable Hibernate To save cost Use reserved instance (Save up to 54%) Use EC2 Instance Saving Plan (For same instance family, save up to 72%) Nitro based instance Regular instance can handle only 32000 IO/ps Nitro based instance can handle more SSH protocol use TCP connection with PORT 22 EC2 instance billing When in running state When preparing to hibernate from the stopping state When reserved instance is in terminate state For High Performance EC2 Instance Use EFA Use Dedicated Instance When a reserved instance is no longer required Stop the instance, so it wont billed after expiration Sell the instance to the Reserved Instance Marketplace EC2 Classic instance can be launched outside of the VPC EC2 Enhanced Networking allows Higher Packet Per Second (PPS) Consistently lower inter instance latencies If Cluster Placement Group through insufficient Capacity Error , restart the instance, there's no such capacity limitation Warm Attach means attaching an ENI when it is Stopped Placement Group Cluster Tightly coupled used for HPC ( High Performance Computing ) Low latency network performance Partition Logical partition group Does not share underlying hardware Used for distributed and replicated workloads Hadoop Cassandra kafka Spread Distinct underlying hardware Logging aws ec3 describe-instances for get logs including the recently terminated instance Limitation There are limitation for launching vCPU in each region Default is 20 Submitting a request to increase the limit can work There is no cost for EIP , if there is only one EIP and it is associated with running EC2","title":"02 EC2 Fundamentals"},{"location":"Notes/EC2/02 EC2 Fundamentals/#ec2-fundamentals","text":"While restart an EC2 instance Public IP changes Both IP6 and private IP does not change It is possible to change the underlying hardware Pre warm for minimum launching time Launch EC2 Instance with EBS Volume Enable Hibernate To save cost Use reserved instance (Save up to 54%) Use EC2 Instance Saving Plan (For same instance family, save up to 72%) Nitro based instance Regular instance can handle only 32000 IO/ps Nitro based instance can handle more SSH protocol use TCP connection with PORT 22 EC2 instance billing When in running state When preparing to hibernate from the stopping state When reserved instance is in terminate state For High Performance EC2 Instance Use EFA Use Dedicated Instance When a reserved instance is no longer required Stop the instance, so it wont billed after expiration Sell the instance to the Reserved Instance Marketplace EC2 Classic instance can be launched outside of the VPC EC2 Enhanced Networking allows Higher Packet Per Second (PPS) Consistently lower inter instance latencies If Cluster Placement Group through insufficient Capacity Error , restart the instance, there's no such capacity limitation Warm Attach means attaching an ENI when it is Stopped Placement Group Cluster Tightly coupled used for HPC ( High Performance Computing ) Low latency network performance Partition Logical partition group Does not share underlying hardware Used for distributed and replicated workloads Hadoop Cassandra kafka Spread Distinct underlying hardware Logging aws ec3 describe-instances for get logs including the recently terminated instance Limitation There are limitation for launching vCPU in each region Default is 20 Submitting a request to increase the limit can work There is no cost for EIP , if there is only one EIP and it is associated with running EC2","title":"EC2 Fundamentals"},{"location":"Notes/ELB And ASG/01 ASG Overview/","text":"ASG Overview Terms Desired capacity or actual size means, number of instance while first time running Minimum size means number of minimum instance while application load is minimum Maximum size means number of maximum instance while application load is maximum Scale Out mean adding instance Scale In means removing instance When new AMI is required, Keep the existing Target Group Create new Launch Configuration ASG terminate the instance in the following manner Select instance with oldest launch config Select instance with closest billing hour The more instance in a AZ got terminated first When we need to manually add or remove instances, update the launch configuration with desired capacity A launch configuration can not be updated after being created Cool Down Period Ensure the ASG does not add or terminate instance without previous activity being completed Default cool-down period is 300 sec Cool Down Period ensures it will not terminate or launch before the last scaling takes place To reduce the changing of EC2 Instance Use high cool-down period Use high threshold value in the Cloudwatch Alarm Metric Applicable for Simple Scaling Policy Type of Scaling Target Tracking Scaling Scaling is done based on specific metric value Example be, want all the CPU average speed 40%. If goes more usage, add instance Step Scaling oe Simple Scaling Scaling is done based on set of scaling adjustments Cloudwatch alarm is involved Example If CPU Usage is over 30% add 2 instance If CPU usage is over 70% add 5 instance Schedule Scaling Based on usage pattern Example be, use 10 instance on each friday 10AM - 5PM, other time 1 instance When a new AMI is required to launch to ASG , need to update the ASG Launch Config Termination Policy Find the AZ with most instances and terminate from there Find the instance with oldest config Find instance with closest billing period Lifecycle Policy While launch instance, there is pending state In pending state, we can go to Pending:wait Pending:proceed Then it goes to in service While terminate instance, there is terminating state In terminating state we can go to Terminating:wait terminating:proceed Then it goes to terminate We can use following state to do additional task Pending:wait Pending:proceed Terminating:wait Terminating:proceed Launch Configuration vs Launch Template Both Use AMI Instance Type Key pair SG Launch Configuration Must be created every time Launch Template Can have multiple version Use both on-demand and spot instances Can use t2 burst feature Recommended by AWS","title":"01 ASG Overview"},{"location":"Notes/ELB And ASG/01 ASG Overview/#asg-overview","text":"Terms Desired capacity or actual size means, number of instance while first time running Minimum size means number of minimum instance while application load is minimum Maximum size means number of maximum instance while application load is maximum Scale Out mean adding instance Scale In means removing instance When new AMI is required, Keep the existing Target Group Create new Launch Configuration ASG terminate the instance in the following manner Select instance with oldest launch config Select instance with closest billing hour The more instance in a AZ got terminated first When we need to manually add or remove instances, update the launch configuration with desired capacity A launch configuration can not be updated after being created","title":"ASG Overview"},{"location":"Notes/ELB And ASG/01 ASG Overview/#cool-down-period","text":"Ensure the ASG does not add or terminate instance without previous activity being completed Default cool-down period is 300 sec Cool Down Period ensures it will not terminate or launch before the last scaling takes place To reduce the changing of EC2 Instance Use high cool-down period Use high threshold value in the Cloudwatch Alarm Metric Applicable for Simple Scaling Policy","title":"Cool Down Period"},{"location":"Notes/ELB And ASG/01 ASG Overview/#type-of-scaling","text":"Target Tracking Scaling Scaling is done based on specific metric value Example be, want all the CPU average speed 40%. If goes more usage, add instance Step Scaling oe Simple Scaling Scaling is done based on set of scaling adjustments Cloudwatch alarm is involved Example If CPU Usage is over 30% add 2 instance If CPU usage is over 70% add 5 instance Schedule Scaling Based on usage pattern Example be, use 10 instance on each friday 10AM - 5PM, other time 1 instance When a new AMI is required to launch to ASG , need to update the ASG Launch Config","title":"Type of Scaling"},{"location":"Notes/ELB And ASG/01 ASG Overview/#termination-policy","text":"Find the AZ with most instances and terminate from there Find the instance with oldest config Find instance with closest billing period","title":"Termination Policy"},{"location":"Notes/ELB And ASG/01 ASG Overview/#lifecycle-policy","text":"While launch instance, there is pending state In pending state, we can go to Pending:wait Pending:proceed Then it goes to in service While terminate instance, there is terminating state In terminating state we can go to Terminating:wait terminating:proceed Then it goes to terminate We can use following state to do additional task Pending:wait Pending:proceed Terminating:wait Terminating:proceed","title":"Lifecycle Policy"},{"location":"Notes/ELB And ASG/01 ASG Overview/#launch-configuration-vs-launch-template","text":"Both Use AMI Instance Type Key pair SG Launch Configuration Must be created every time Launch Template Can have multiple version Use both on-demand and spot instances Can use t2 burst feature Recommended by AWS","title":"Launch Configuration vs Launch Template"},{"location":"Notes/ELB And ASG/02 ELB Overview/","text":"ELB Overview Use to forward traffic to multiple servers Expose single point of access DNS to the application Do regular health checks Provide SSL for the site Can be used stickiness with cookies User with same cookie will go to the same server/instance Allow cross zone availability 3 types of load balancers Classic Load Balancer Handle HTTP , HTTPS and TCP traffic Application Load Balancer Handle HTTP , HTTPS and Websocket Great fit for micro-services and container based applications Has port mapping feature to redirect a dynamic port in ECS The application can get the IP from header x-forwarded-for The application can get the protocol from header x-forwarded-proto The application can get the port from header x-forwarded-port Network Load Balancer Handle TCP , TLS aka Secure TCP and UDP Supports one EIP for each AZ, that is helpful for whitelisting the IP Use for Extreme performance TCP or UDP protocol ALB and CLB expose DNS, on the other hand NLB expose static IP Load balancer can be Public and Private For huge scale out, need to use warm-up . Need to contact AWS for this purpose Troubleshooting 4xx for client induced error 5xx for application induced error 503 for at capacity or no registered target Monitoring ALB access logs can provide details of API Calls Cloudwatch for aggregate statistics ELB does the health check by HTTP HTTPS A security feature is Perfect forward secrecy offer SSl/TLS to Cloudfront and ELB In ALB using path condition we can forward request to different Target Groups based on api path, like abc.com/a abc.com/b using host condition we can forward request to different Target Groups based on host name in the header, like abc.site.com def.site.com Stickiness Ensure the user goes to the same instance Supported by Application Load Balancer Need to update the target group Classic Load Balancer Can set the time of stickiness Cross Zone Load Balancing Load balancer can distribute traffic evenly among all the AZ For NLB there is charge for Inter AZ load balancing In CLB and NLB this Cross Zone Load Balancing is turned of by default SSL and TLS Certificate SNI stands for server name identification SNI can be use for multiple endpoint with multiple certificate Connection Draining It is the time of In Flight Request while the instance is de registering or unhealthy For CLB it is called Connection Draining For ALB and NLB it is called De registration Delay Happen in Target Group Connection draining time can be set from 0 (disabled) to 3600 sec Lambda Function Integration With ALB and target groups, We can send multi value headers Enable health check multi value query enables us sending query as name=['foo', 'bar'] instead of ?name='foo'&name='bar'","title":"02 ELB Overview"},{"location":"Notes/ELB And ASG/02 ELB Overview/#elb-overview","text":"Use to forward traffic to multiple servers Expose single point of access DNS to the application Do regular health checks Provide SSL for the site Can be used stickiness with cookies User with same cookie will go to the same server/instance Allow cross zone availability 3 types of load balancers Classic Load Balancer Handle HTTP , HTTPS and TCP traffic Application Load Balancer Handle HTTP , HTTPS and Websocket Great fit for micro-services and container based applications Has port mapping feature to redirect a dynamic port in ECS The application can get the IP from header x-forwarded-for The application can get the protocol from header x-forwarded-proto The application can get the port from header x-forwarded-port Network Load Balancer Handle TCP , TLS aka Secure TCP and UDP Supports one EIP for each AZ, that is helpful for whitelisting the IP Use for Extreme performance TCP or UDP protocol ALB and CLB expose DNS, on the other hand NLB expose static IP Load balancer can be Public and Private For huge scale out, need to use warm-up . Need to contact AWS for this purpose Troubleshooting 4xx for client induced error 5xx for application induced error 503 for at capacity or no registered target Monitoring ALB access logs can provide details of API Calls Cloudwatch for aggregate statistics ELB does the health check by HTTP HTTPS A security feature is Perfect forward secrecy offer SSl/TLS to Cloudfront and ELB In ALB using path condition we can forward request to different Target Groups based on api path, like abc.com/a abc.com/b using host condition we can forward request to different Target Groups based on host name in the header, like abc.site.com def.site.com","title":"ELB Overview"},{"location":"Notes/ELB And ASG/02 ELB Overview/#stickiness","text":"Ensure the user goes to the same instance Supported by Application Load Balancer Need to update the target group Classic Load Balancer Can set the time of stickiness","title":"Stickiness"},{"location":"Notes/ELB And ASG/02 ELB Overview/#cross-zone-load-balancing","text":"Load balancer can distribute traffic evenly among all the AZ For NLB there is charge for Inter AZ load balancing In CLB and NLB this Cross Zone Load Balancing is turned of by default","title":"Cross Zone Load Balancing"},{"location":"Notes/ELB And ASG/02 ELB Overview/#ssl-and-tls-certificate","text":"SNI stands for server name identification SNI can be use for multiple endpoint with multiple certificate","title":"SSL and TLS Certificate"},{"location":"Notes/ELB And ASG/02 ELB Overview/#connection-draining","text":"It is the time of In Flight Request while the instance is de registering or unhealthy For CLB it is called Connection Draining For ALB and NLB it is called De registration Delay Happen in Target Group Connection draining time can be set from 0 (disabled) to 3600 sec","title":"Connection Draining"},{"location":"Notes/ELB And ASG/02 ELB Overview/#lambda-function-integration","text":"With ALB and target groups, We can send multi value headers Enable health check multi value query enables us sending query as name=['foo', 'bar'] instead of ?name='foo'&name='bar'","title":"Lambda Function Integration"},{"location":"Notes/ELB And ASG/03 Scalability & High Availability/","text":"Scalability & High Availability Scalability Two types of scalability Vertical Scalability Upgrade the Underlying Hardware There is hardware limitation for Vertical Scalability Horizontal Scalability Increase number of instances/system High Availability Run the server at least 2 AZ or data center Goal is survive the a data center loss or AZ down","title":"03 Scalability & High Availability"},{"location":"Notes/ELB And ASG/03 Scalability & High Availability/#scalability-high-availability","text":"","title":"Scalability &amp; High Availability"},{"location":"Notes/ELB And ASG/03 Scalability & High Availability/#scalability","text":"Two types of scalability Vertical Scalability Upgrade the Underlying Hardware There is hardware limitation for Vertical Scalability Horizontal Scalability Increase number of instances/system","title":"Scalability"},{"location":"Notes/ELB And ASG/03 Scalability & High Availability/#high-availability","text":"Run the server at least 2 AZ or data center Goal is survive the a data center loss or AZ down","title":"High Availability"},{"location":"Notes/IAM/01 STS/","text":"STS Security Token Service Limited validity time (15 mins to 1 hour) AssumeRole Use to create temporary role within the account and share resource 4 types of AssumeRole Plain AssumeRole AssumeRoleWithSAMl , this return the credentials to the users who is logged in with SAML Federation AssumeRoleWithWebIdentity , this return the credentials to the users who is logged in with LDB , like google, facebook etc. Recommended to use Cognito instead GetSessionToken , for MFA Using AssumeRole Define IAM role within your account or cross-account Define principal to access resource in the IAM Role Use STS to retrieve credentials and impersonate the IAM role using AssumeRole API This credentials valid 15 mins to 1 hour","title":"01 STS"},{"location":"Notes/IAM/01 STS/#sts","text":"Security Token Service Limited validity time (15 mins to 1 hour) AssumeRole Use to create temporary role within the account and share resource 4 types of AssumeRole Plain AssumeRole AssumeRoleWithSAMl , this return the credentials to the users who is logged in with SAML Federation AssumeRoleWithWebIdentity , this return the credentials to the users who is logged in with LDB , like google, facebook etc. Recommended to use Cognito instead GetSessionToken , for MFA","title":"STS"},{"location":"Notes/IAM/01 STS/#using-assumerole","text":"Define IAM role within your account or cross-account Define principal to access resource in the IAM Role Use STS to retrieve credentials and impersonate the IAM role using AssumeRole API This credentials valid 15 mins to 1 hour","title":"Using AssumeRole"},{"location":"Notes/IAM/02 Identity Fedaration/","text":"Identity Federation User management outside of AWS No need to create AWS Users Allows users, who are outside of AWS can access AWS Resource 6 Types of Identity Federation SAML 2.0 Custom Identity Broker Web Identity Federation without Cognito Web Identity Federation with Cognito Single Sign On Non SAML, like compatible like Microsoft AD Custom Identity Broker Use when the Identity Broker is not compatible with SAML Web Identity Federation allow to connect Single Sign In using OpenID-connect","title":"02 Identity Fedaration"},{"location":"Notes/IAM/02 Identity Fedaration/#identity-federation","text":"User management outside of AWS No need to create AWS Users Allows users, who are outside of AWS can access AWS Resource 6 Types of Identity Federation SAML 2.0 Custom Identity Broker Web Identity Federation without Cognito Web Identity Federation with Cognito Single Sign On Non SAML, like compatible like Microsoft AD Custom Identity Broker Use when the Identity Broker is not compatible with SAML Web Identity Federation allow to connect Single Sign In using OpenID-connect","title":"Identity Federation"},{"location":"Notes/IAM/03 Directory Service/","text":"Directory Service 3 types of Directory service AWS Managed Microsoft AD Hybrid solution Users can be from AWS Directory and On premise Directory Both AWS Directory and On premise Directory are connected through a TRUST connection This TRUST connection can be set up using VPN or Direct Connect AD Connector Directory Gateway (Proxy) Users managed by only On premise AD Redirect to the On premise AD Simple AD AD compatible service by AWS Can not join with On premise AD Microsoft Active Directory Available in any windows server with AD Domain Service Combination of Objects, like User Accounts Computers Printers File Shares Security Groups Objects are organized in Trees A group of Trees are Forest AD has feature Centralized Security Management Create Account Assigning Permission","title":"03 Directory Service"},{"location":"Notes/IAM/03 Directory Service/#directory-service","text":"3 types of Directory service AWS Managed Microsoft AD Hybrid solution Users can be from AWS Directory and On premise Directory Both AWS Directory and On premise Directory are connected through a TRUST connection This TRUST connection can be set up using VPN or Direct Connect AD Connector Directory Gateway (Proxy) Users managed by only On premise AD Redirect to the On premise AD Simple AD AD compatible service by AWS Can not join with On premise AD","title":"Directory Service"},{"location":"Notes/IAM/03 Directory Service/#microsoft-active-directory","text":"Available in any windows server with AD Domain Service Combination of Objects, like User Accounts Computers Printers File Shares Security Groups Objects are organized in Trees A group of Trees are Forest AD has feature Centralized Security Management Create Account Assigning Permission","title":"Microsoft Active Directory"},{"location":"Notes/IAM/04 Organizations/","text":"Organizations Global Service Allow to manage multiple AWS Account Main Account is Master Account Master Account can not be changed Other account is Member Account Each member can attach only one Organization Consolidated Billing For all Organization Account Pricing benefits are calculated by Consolidated Billing API is available to automate creating the AWS Account Resource sharing can be done by individual account even though the resource sharing is not enabled by SCP SCP Service Control Policies Use for White List and Black List the IAM Action Applied to Organization Unit or Account Level Does not apply to Master Account SCP By default deny everything Need explicit allow to for any action Effect of Service Linked Roles Service Linked Roles enable AWS Service to AWS Organization SCP can not affect Service Linked Roles Transfer An Account Between AWS Organization Member Account Transfer Leave the current organization Get invitation from the other organization Accept the invitation Master Account Transfer Remove all the member account Delete old organization","title":"04 Organizations"},{"location":"Notes/IAM/04 Organizations/#organizations","text":"Global Service Allow to manage multiple AWS Account Main Account is Master Account Master Account can not be changed Other account is Member Account Each member can attach only one Organization Consolidated Billing For all Organization Account Pricing benefits are calculated by Consolidated Billing API is available to automate creating the AWS Account Resource sharing can be done by individual account even though the resource sharing is not enabled by SCP","title":"Organizations"},{"location":"Notes/IAM/04 Organizations/#scp","text":"Service Control Policies Use for White List and Black List the IAM Action Applied to Organization Unit or Account Level Does not apply to Master Account SCP By default deny everything Need explicit allow to for any action Effect of Service Linked Roles Service Linked Roles enable AWS Service to AWS Organization SCP can not affect Service Linked Roles","title":"SCP"},{"location":"Notes/IAM/04 Organizations/#transfer-an-account-between-aws-organization","text":"Member Account Transfer Leave the current organization Get invitation from the other organization Accept the invitation Master Account Transfer Remove all the member account Delete old organization","title":"Transfer An Account Between AWS Organization"},{"location":"Notes/IAM/05 IAM Conditions/","text":"IAM Conditions Allow IP to API call to the AWS from certain IP Any request to the AWS should come from 192.0.2.0/24 Allow taking action to certain resources only from certain region, like EC2 Instance can be start or stop if the request is from eu-east-1 Restriction can be based on Tags , like We can start a instance if the instance has certain tags We can stop a instance if the instance has certain tags For certain actions we can force using MFA , like Stop EC2 Instance Terminate EC2 Instance","title":"05 IAM Conditions"},{"location":"Notes/IAM/05 IAM Conditions/#iam-conditions","text":"Allow IP to API call to the AWS from certain IP Any request to the AWS should come from 192.0.2.0/24 Allow taking action to certain resources only from certain region, like EC2 Instance can be start or stop if the request is from eu-east-1 Restriction can be based on Tags , like We can start a instance if the instance has certain tags We can stop a instance if the instance has certain tags For certain actions we can force using MFA , like Stop EC2 Instance Terminate EC2 Instance","title":"IAM Conditions"},{"location":"Notes/IAM/06 IAM for S3/","text":"IAM for S3 When arn is arn:aws:s3:::test , then the rules applied to Bucket Level when arn is arn:aws:s3:::test/* the the rules applied to Object Level","title":"06 IAM for S3"},{"location":"Notes/IAM/06 IAM for S3/#iam-for-s3","text":"When arn is arn:aws:s3:::test , then the rules applied to Bucket Level when arn is arn:aws:s3:::test/* the the rules applied to Object Level","title":"IAM for S3"},{"location":"Notes/IAM/07 IAM vs Resource Policies/","text":"IAM vs Resource Policies Using AssumeRole , original permissions have to give up Lets say, I have permission to access DynamoDB` Using AssumeRole I got access of S3 When I am using this AssumeRole , I can only access S3 , but can not access DynamoDB But using Resource Policies we can use both S3 and DynamoDB Resource Based Policies are provided by S3 SNS SQS","title":"07 IAM vs Resource Policies"},{"location":"Notes/IAM/07 IAM vs Resource Policies/#iam-vs-resource-policies","text":"Using AssumeRole , original permissions have to give up Lets say, I have permission to access DynamoDB` Using AssumeRole I got access of S3 When I am using this AssumeRole , I can only access S3 , but can not access DynamoDB But using Resource Policies we can use both S3 and DynamoDB Resource Based Policies are provided by S3 SNS SQS","title":"IAM vs Resource Policies"},{"location":"Notes/IAM/08 IAM Policy Evaluation/","text":"IAM Policy Evaluation IAM Permission Boundaries Supported for users and roles (not groups) Even a user has Administrator Access , these access can be restricted using the IAM Permission Boundaries This can be used as a combination of Organization SCP Evaluation Logic Couple of roles and policies we can define Deny Evaluation Organization SCP Resource Based Policy IAM Permission Boundaries Session Policy Identity Based Policy If there is any Explicit Deny the other Allow will be discarded IAM Policy Evaluation Order Command Line Options : Override any other config. Used like --region , --output , --profile etc Environment variable CLI Credentials File : Created by aws configure and store in ~/.aws/credentials CLI Config File : TODO: find diff between CLI Credentials File and CLI Config File Container Credentials : Temporary credentials in the ECS Task container Instance Profile Credentials : IAM role attached to the instance","title":"08 IAM Policy Evaluation"},{"location":"Notes/IAM/08 IAM Policy Evaluation/#iam-policy-evaluation","text":"","title":"IAM Policy Evaluation"},{"location":"Notes/IAM/08 IAM Policy Evaluation/#iam-permission-boundaries","text":"Supported for users and roles (not groups) Even a user has Administrator Access , these access can be restricted using the IAM Permission Boundaries This can be used as a combination of Organization SCP","title":"IAM Permission Boundaries"},{"location":"Notes/IAM/08 IAM Policy Evaluation/#evaluation-logic","text":"Couple of roles and policies we can define Deny Evaluation Organization SCP Resource Based Policy IAM Permission Boundaries Session Policy Identity Based Policy If there is any Explicit Deny the other Allow will be discarded","title":"Evaluation Logic"},{"location":"Notes/IAM/08 IAM Policy Evaluation/#iam-policy-evaluation-order","text":"Command Line Options : Override any other config. Used like --region , --output , --profile etc Environment variable CLI Credentials File : Created by aws configure and store in ~/.aws/credentials CLI Config File : TODO: find diff between CLI Credentials File and CLI Config File Container Credentials : Temporary credentials in the ECS Task container Instance Profile Credentials : IAM role attached to the instance","title":"IAM Policy Evaluation Order"},{"location":"Notes/IAM/09 Resource Access Manager/","text":"Resource Access Manager Known as RAM Share AWS Resource with other AWS Account Avoid resource duplication Example of RAM on VPC Subnet Allow to have all the resource launched in the same subnets For VPC Subnet , other account have to be same Organization Can not share SG or Default VPC Participants can manage there own resource there Participant can not view/modify other participants resource Also we can use RAM in AWS Transit Gateway Route53 Resolver License Manager Configurations","title":"09 Resource Access Manager"},{"location":"Notes/IAM/09 Resource Access Manager/#resource-access-manager","text":"Known as RAM Share AWS Resource with other AWS Account Avoid resource duplication Example of RAM on VPC Subnet Allow to have all the resource launched in the same subnets For VPC Subnet , other account have to be same Organization Can not share SG or Default VPC Participants can manage there own resource there Participant can not view/modify other participants resource Also we can use RAM in AWS Transit Gateway Route53 Resolver License Manager Configurations","title":"Resource Access Manager"},{"location":"Notes/IAM/10 Single Sign On/","text":"Single Sign On Single sion on for Multiple accounts 3rd party Applications Integrated with AWS Organization Supports SAML 2.0 Markup On Premise AD Feature Centralized Permission Centralized Auditing Cloudtrail","title":"10 Single Sign On"},{"location":"Notes/IAM/10 Single Sign On/#single-sign-on","text":"Single sion on for Multiple accounts 3rd party Applications Integrated with AWS Organization Supports SAML 2.0 Markup On Premise AD Feature Centralized Permission Centralized Auditing Cloudtrail","title":"Single Sign On"},{"location":"Notes/IAM/11 SSL/TLS Certificate/","text":"SSL/TLS Certificate Can use AWS Certificate Manager (ACM) IAM Certificate Store For 3rd party certificate, we can use one of the following Import it to ACM Upload it to IAM","title":"TLS Certificate"},{"location":"Notes/IAM/11 SSL/TLS Certificate/#ssltls-certificate","text":"Can use AWS Certificate Manager (ACM) IAM Certificate Store For 3rd party certificate, we can use one of the following Import it to ACM Upload it to IAM","title":"SSL/TLS Certificate"},{"location":"Notes/Other Services/01 CI-CD_deprecated/","text":"CI/CD Flow Code -> Build -> Test -> Deploy -> Provision AWS Codepipeline AWS Codecommit Like Github Responsible for Code portion AWS Codebuild Like Jenkins Responsible for Build and Test portion AWS Code Deploy Responsible for Deploy portion Ues AWS Beanstalk or AWS Cloudformation to provision the code Can be use for deploying code to EC2 Instance On premise sever Lambda Function Use for Rapid release of new feature Updating Lambda Function Avoid downtime during Application deployment Type of code deploy Linear (This deployment send traffic incrementally) All at once (All traffic shifts to new deployment) Canary (Can be define, how many traffic will flow new and updated deployments) While deploying a code base Code commit can be a source stage Beanstalk can be a deploy stage Blue/Green Deployment Isolation between blue and green Roll incoming traffic during deployments Minimum downtime","title":"01 CI CD deprecated"},{"location":"Notes/Other Services/01 CI-CD_deprecated/#cicd","text":"Flow Code -> Build -> Test -> Deploy -> Provision AWS Codepipeline AWS Codecommit Like Github Responsible for Code portion AWS Codebuild Like Jenkins Responsible for Build and Test portion AWS Code Deploy Responsible for Deploy portion Ues AWS Beanstalk or AWS Cloudformation to provision the code Can be use for deploying code to EC2 Instance On premise sever Lambda Function Use for Rapid release of new feature Updating Lambda Function Avoid downtime during Application deployment Type of code deploy Linear (This deployment send traffic incrementally) All at once (All traffic shifts to new deployment) Canary (Can be define, how many traffic will flow new and updated deployments) While deploying a code base Code commit can be a source stage Beanstalk can be a deploy stage Blue/Green Deployment Isolation between blue and green Roll incoming traffic during deployments Minimum downtime","title":"CI/CD"},{"location":"Notes/Other Services/02 Cloudformation/","text":"Cloudformation Infrastructure as Code Easy to CRUD the infrastructure as Code Savings Strategy Automate the creation and deletion of the infrastructure at for certain times StackSet : Extends the functionalities of the stack, administrator create the stack template and other accounts can extends the stacks functionality Allow Create , Update , Delete stacks across multiple accounts and regions Change Sets : Allow to show the changes preview. Stack Instances : Reference to a original stack in another account Artifacts : Used to conjunction with the code pipeline Properties cfn-init : Used to retrieve metadata, install packages (like nginx in ec2), run services cfn-signal : Send signal for create or wait, use to synchronize the resources cfn-get-metadata : Use to retrieve metadata for a service or resources cfn-hup : Upon checking the metadata, execute custom hooks when changes are detected","title":"02 Cloudformation"},{"location":"Notes/Other Services/02 Cloudformation/#cloudformation","text":"Infrastructure as Code Easy to CRUD the infrastructure as Code Savings Strategy Automate the creation and deletion of the infrastructure at for certain times StackSet : Extends the functionalities of the stack, administrator create the stack template and other accounts can extends the stacks functionality Allow Create , Update , Delete stacks across multiple accounts and regions Change Sets : Allow to show the changes preview. Stack Instances : Reference to a original stack in another account Artifacts : Used to conjunction with the code pipeline","title":"Cloudformation"},{"location":"Notes/Other Services/02 Cloudformation/#properties","text":"cfn-init : Used to retrieve metadata, install packages (like nginx in ec2), run services cfn-signal : Send signal for create or wait, use to synchronize the resources cfn-get-metadata : Use to retrieve metadata for a service or resources cfn-hup : Upon checking the metadata, execute custom hooks when changes are detected","title":"Properties"},{"location":"Notes/Other Services/03 ECS/","text":"ECS Elastic Container Service Run docker containers in EC2 machines Components ECS Core provision EC2 instance to run docker container Fargate run ECS task to AWS Provide Compute , more serverless than ECS Core EKS is Elastic Kubernetes Service by AWS ECR is Elastic Container Registry by AWS IAM Security and Roles are in ECS Task level ECS can enable dynamic port mapping with ALB NLB ECS setup For regular EC2 Instance install the ECS Agent and edit config file For ECS ready Linux AMI , no need to install ECS Agent , only edit the config file Editing the config file Location /etc/ecs/ecs.config There are 35 Configuration to edit ECS CLUSTER ECS_ENGINE_AUTH_DATA ECS_AVAILABLE_LOGGING_DRIVER ECS_ENABLE_TASK_IAM_ROLE If the associated ec2 instances are in stopped state and we terminate ecs, it will not be de-registered automatically When ecs client is stopped, the instance remain active with agent connection status false ECS Components Task Definition : Allow port mapping. Port Mapping allows the container to send and receive traffic through the host machine Service Schedular : Allow to run tasks manually Container Instance : On which the container runs on, typically the ec2 instance or ecs ready ec2 instance Container Agent : Allows the containers to connect with the cluster Task Placement Strategy Determine how the tasks will be placed between instances. ECS supports 3 types of Task Placement Strategies binpack : Placed task by using least amount of CPU or memory. This minimize the number of instances are being used random : Place tasks randomly. Make sure tasks are scheduled in instances with enough resources. spread : Placed tasks based on specified value like instance id, host, az etc. For example, if the field is instanceId the task will be distributed evenly in different instances. Another example, if the field is az , the tasks will be evenly distributed among availability zones.","title":"03 ECS"},{"location":"Notes/Other Services/03 ECS/#ecs","text":"Elastic Container Service Run docker containers in EC2 machines Components ECS Core provision EC2 instance to run docker container Fargate run ECS task to AWS Provide Compute , more serverless than ECS Core EKS is Elastic Kubernetes Service by AWS ECR is Elastic Container Registry by AWS IAM Security and Roles are in ECS Task level ECS can enable dynamic port mapping with ALB NLB ECS setup For regular EC2 Instance install the ECS Agent and edit config file For ECS ready Linux AMI , no need to install ECS Agent , only edit the config file Editing the config file Location /etc/ecs/ecs.config There are 35 Configuration to edit ECS CLUSTER ECS_ENGINE_AUTH_DATA ECS_AVAILABLE_LOGGING_DRIVER ECS_ENABLE_TASK_IAM_ROLE If the associated ec2 instances are in stopped state and we terminate ecs, it will not be de-registered automatically When ecs client is stopped, the instance remain active with agent connection status false","title":"ECS"},{"location":"Notes/Other Services/03 ECS/#ecs-components","text":"Task Definition : Allow port mapping. Port Mapping allows the container to send and receive traffic through the host machine Service Schedular : Allow to run tasks manually Container Instance : On which the container runs on, typically the ec2 instance or ecs ready ec2 instance Container Agent : Allows the containers to connect with the cluster","title":"ECS Components"},{"location":"Notes/Other Services/03 ECS/#task-placement-strategy","text":"Determine how the tasks will be placed between instances. ECS supports 3 types of Task Placement Strategies binpack : Placed task by using least amount of CPU or memory. This minimize the number of instances are being used random : Place tasks randomly. Make sure tasks are scheduled in instances with enough resources. spread : Placed tasks based on specified value like instance id, host, az etc. For example, if the field is instanceId the task will be distributed evenly in different instances. Another example, if the field is az , the tasks will be evenly distributed among availability zones.","title":"Task Placement Strategy"},{"location":"Notes/Other Services/04 EKS/","text":"EKS EKS stands for Elastic Kuberneties Service Managed kubernetes Cluster by AWS Similar to ECS but it's open source Easy when migrate kubernetes from other provider","title":"04 EKS"},{"location":"Notes/Other Services/04 EKS/#eks","text":"EKS stands for Elastic Kuberneties Service Managed kubernetes Cluster by AWS Similar to ECS but it's open source Easy when migrate kubernetes from other provider","title":"EKS"},{"location":"Notes/Other Services/05 Step Function vs SWF/","text":"SWF Simple Work Flow Run on EC2 , not Serverless Concept of Activity Step Decision Step Has built in Human Intervention Feature Used when External Signal is is required to intervene When child process return value to parent process Can be used to de couple an application Properties Tags : Allow filtering the list of executions Makers : Records history of executions Timers : Allow notify the decider after a certain amount of defined time Signals : Enable to inject information to the execution Step Function vs SWF Step Function Serverless No human intervention SWF No Serverless Build in human intervention","title":"05 Step Function vs SWF"},{"location":"Notes/Other Services/05 Step Function vs SWF/#swf","text":"Simple Work Flow Run on EC2 , not Serverless Concept of Activity Step Decision Step Has built in Human Intervention Feature Used when External Signal is is required to intervene When child process return value to parent process Can be used to de couple an application Properties Tags : Allow filtering the list of executions Makers : Records history of executions Timers : Allow notify the decider after a certain amount of defined time Signals : Enable to inject information to the execution","title":"SWF"},{"location":"Notes/Other Services/05 Step Function vs SWF/#step-function-vs-swf","text":"Step Function Serverless No human intervention SWF No Serverless Build in human intervention","title":"Step Function vs SWF"},{"location":"Notes/Other Services/06 EMR/","text":"EMR Elastic Map Reduce Use for Big data processing Hadoop Clusters Apache Spark Collect and process EC2 log files Collect and process ALB log files Supports Apache Spark HBase Presto Flink To save cost, the EC2 instances are used to run in SPot Instances When it comes to process lot of log files, EMR could be a go to move","title":"06 EMR"},{"location":"Notes/Other Services/06 EMR/#emr","text":"Elastic Map Reduce Use for Big data processing Hadoop Clusters Apache Spark Collect and process EC2 log files Collect and process ALB log files Supports Apache Spark HBase Presto Flink To save cost, the EC2 instances are used to run in SPot Instances When it comes to process lot of log files, EMR could be a go to move","title":"EMR"},{"location":"Notes/Other Services/07 Glue/","text":"Glue Does ETL i.e. Extract Transform Load Move data from source to destination and process on the fly Keeps track of proceed data using Job Bookmark Using Job Bookmark we can only search the changes Prevent processing the whole data again","title":"07 Glue"},{"location":"Notes/Other Services/07 Glue/#glue","text":"Does ETL i.e. Extract Transform Load Move data from source to destination and process on the fly Keeps track of proceed data using Job Bookmark Using Job Bookmark we can only search the changes Prevent processing the whole data again","title":"Glue"},{"location":"Notes/Other Services/08 Opsworks/","text":"Opsworks Combination of Chef and Puppet Alternative of AWS SSM Opsworks Stacks are collection of instance, serving a common task Opsworks Stacks allows to Manage stack in different layers, like Load Balancing Database Application server Allow to manage app and server in AWS Cloud On premise","title":"08 Opsworks"},{"location":"Notes/Other Services/08 Opsworks/#opsworks","text":"Combination of Chef and Puppet Alternative of AWS SSM Opsworks Stacks are collection of instance, serving a common task Opsworks Stacks allows to Manage stack in different layers, like Load Balancing Database Application server Allow to manage app and server in AWS Cloud On premise","title":"Opsworks"},{"location":"Notes/Other Services/09 Elastic Transcoder/","text":"Elastic Transcoder Convert media files frm S3 To various size various formats So, compatible with Phone Tablet TV","title":"09 Elastic Transcoder"},{"location":"Notes/Other Services/09 Elastic Transcoder/#elastic-transcoder","text":"Convert media files frm S3 To various size various formats So, compatible with Phone Tablet TV","title":"Elastic Transcoder"},{"location":"Notes/Other Services/10 Workspace/","text":"AWS Workspace Managed, secure, cloud Desktop Alternative of VDI i.e. Virtual Desktop Infrastructure","title":"10 Workspace"},{"location":"Notes/Other Services/10 Workspace/#aws-workspace","text":"Managed, secure, cloud Desktop Alternative of VDI i.e. Virtual Desktop Infrastructure","title":"AWS Workspace"},{"location":"Notes/Other Services/11 AppSync/","text":"AppSync Managed GraphQL","title":"11 AppSync"},{"location":"Notes/Other Services/11 AppSync/#appsync","text":"Managed GraphQL","title":"AppSync"},{"location":"Notes/Other Services/12 AWS Polly/","text":"AWS Polly Use to read text Region specific SSML control generated speech SSML converts comma into period Strong tag control the speech speed Lexicons are region specific For a text, which appear multiple times Create alias using multiple lexicons","title":"12 AWS Polly"},{"location":"Notes/Other Services/12 AWS Polly/#aws-polly","text":"Use to read text Region specific SSML control generated speech SSML converts comma into period Strong tag control the speech speed Lexicons are region specific For a text, which appear multiple times Create alias using multiple lexicons","title":"AWS Polly"},{"location":"Notes/Other Services/13 Managed Blockchain/","text":"Managed Blockchain In managed blockchain, when a new member is added, an unique id is assigned to these members For any transaction, each member should use the following format ResourceID.MemberID.NetworkID.managedblockchain.amazonaws.com:PortNumber","title":"13 Managed Blockchain"},{"location":"Notes/Other Services/13 Managed Blockchain/#managed-blockchain","text":"In managed blockchain, when a new member is added, an unique id is assigned to these members For any transaction, each member should use the following format ResourceID.MemberID.NetworkID.managedblockchain.amazonaws.com:PortNumber","title":"Managed Blockchain"},{"location":"Notes/Other Services/14 AWS Quicksight/","text":"AWS Quicksight Cloud powered business intelligence tool Use for visualizations Ad hoc analysis ML insights","title":"14 AWS Quicksight"},{"location":"Notes/Other Services/14 AWS Quicksight/#aws-quicksight","text":"Cloud powered business intelligence tool Use for visualizations Ad hoc analysis ML insights","title":"AWS Quicksight"},{"location":"Notes/Other Services/15 AWS Cloud Search/","text":"AWS Cloud Search Managed Search Solution Has feature highlighting Autocomplete Geo spatial Search Scales seamlessly Use for Add rich search capabilities in website","title":"15 AWS Cloud Search"},{"location":"Notes/Other Services/15 AWS Cloud Search/#aws-cloud-search","text":"Managed Search Solution Has feature highlighting Autocomplete Geo spatial Search Scales seamlessly Use for Add rich search capabilities in website","title":"AWS Cloud Search"},{"location":"Notes/Other Services/16 Multiple Account/","text":"Multiple Account When to manage multiple AWS Account Business require administrative isolation on workloads Business require limited visibility and discoverability of workloads Business require isolation to minimize the blast radius Business require strong isolation of recovery and auditing data","title":"16 Multiple Account"},{"location":"Notes/Other Services/16 Multiple Account/#multiple-account","text":"When to manage multiple AWS Account Business require administrative isolation on workloads Business require limited visibility and discoverability of workloads Business require isolation to minimize the blast radius Business require strong isolation of recovery and auditing data","title":"Multiple Account"},{"location":"Notes/Other Services/17 Amazon X-Ray/","text":"Amazon X-Ray Allow developer debugging the production and distributed application and micro services For cost effective performance, the sampling rate should be low, so Get significant number of request statistically Optimum traces Properties Segments : Used to break down the data in sub segments. Sub Segments : Provides, timings, other aws service/resource calls in downstream forms. Inferred Segment : Allow display timings and calls to all services, even the services who does not support x-ray or tracing annotations and metadata do not trace the calls of other aws services and resources. Instead these annotations and metadata are included in the segments and sub-segments . sub-segments fields, namespace : Can be aws or remote aws for AWS SDK calls remote for other downstream calls http : Outgoing HTTP call annotations : Key-value for index search metadata : Additional data for debugging annotations can be used to filter the traced data from console. Also can trace by groups. metadata can include some additional key value data, that can be helpful during the debugging and tracing. Segment Document Add custom attributes as annotation to use as filter expression Add custom attributes as metadata to store custom data during trace Add custom attributes as segment field to use ::::TODO Sampling Rule to create a representative sampling of tracing Wit default sampling one request per sec and 5% additional requests per host X Ray Daemon X-Ray sdk does not send data directly to X-Ray. Instead it sends data for multiple request in the daemon and then sends these to the X-Ray service. X-Ray Daemon listens to traffic to UDP 2000 port, Elastic Beanstalk Integration X-Ray is already installed in the elastic beanstalk. To enable the x-ray, update/create a config under the beanstalk extensions, .ebextensions/x-ray-daemon.config . This can also be done using by management console. To get trace data, code must instrumented withe x-ray sdk ECS Integration ECS can integrate x-ray by x-ray daemon and side-car pattern As Daemon Pattern : To use x-ray in ecs, we need to use the x-ray daemon container for each of the ecs instances. In this case, the x-ray daemon acts as the x-ray agents. As Side Car Pattern : In this case, each of the container in the ecs will have x-ray daemon inside it. When we need to use x-ray in the fargate, we have to use the side-car pattern. Because, we do not have control over the ec2 instances in fargate. To set up Run x-ray daemon according to the suited pattern Make port mapping to allows UDP 2000 for x-ray daemon Lambda Integration To implement the X-ray tracing, we need to implement the followings, Enable ACtive Tracing from the lambda function configuration. This will run the x-ray daemon for the function The attached policy should have write access to the x-ray daemon Following environment policies will require to communicate with x-ray _X_AMZN_TRACE_ID : tracing header AWS_XRAY_DAEMON_ADDRESS : IP_ADDRESS:PORT Others are, AWS_XRAY_CONTEXT_MISSING : default: LOG_ERROR AWS_XRAY_TRACING_NAME : set service name AUTO_INSTRUMENT : applicable for django framework EC2 Integration To use x-ray we need the x-ray daemon in the ec2 instance. We can install this manually or update the user script to install the daemon. Also we have to make sure the sg allow UDP of port 2000 for the x-ray daemon API After X-Ray collects the data, it use to combine abd summarize the trace data. GetTraceSummaries : Can get the trace summaries, ids and annotations BatchGetTraces : Get the full traces GetGroup : A group resources GetServiceGraph : Return info of which service handle the incoming request To make a custom debug tool, First get all trace ids by GetTraceSummaries From each id, get full traces by BatchGetTraces","title":"17 Amazon X Ray"},{"location":"Notes/Other Services/17 Amazon X-Ray/#amazon-x-ray","text":"Allow developer debugging the production and distributed application and micro services For cost effective performance, the sampling rate should be low, so Get significant number of request statistically Optimum traces","title":"Amazon X-Ray"},{"location":"Notes/Other Services/17 Amazon X-Ray/#properties","text":"Segments : Used to break down the data in sub segments. Sub Segments : Provides, timings, other aws service/resource calls in downstream forms. Inferred Segment : Allow display timings and calls to all services, even the services who does not support x-ray or tracing annotations and metadata do not trace the calls of other aws services and resources. Instead these annotations and metadata are included in the segments and sub-segments . sub-segments fields, namespace : Can be aws or remote aws for AWS SDK calls remote for other downstream calls http : Outgoing HTTP call annotations : Key-value for index search metadata : Additional data for debugging annotations can be used to filter the traced data from console. Also can trace by groups. metadata can include some additional key value data, that can be helpful during the debugging and tracing. Segment Document Add custom attributes as annotation to use as filter expression Add custom attributes as metadata to store custom data during trace Add custom attributes as segment field to use ::::TODO Sampling Rule to create a representative sampling of tracing Wit default sampling one request per sec and 5% additional requests per host","title":"Properties"},{"location":"Notes/Other Services/17 Amazon X-Ray/#x-ray-daemon","text":"X-Ray sdk does not send data directly to X-Ray. Instead it sends data for multiple request in the daemon and then sends these to the X-Ray service. X-Ray Daemon listens to traffic to UDP 2000 port,","title":"X Ray Daemon"},{"location":"Notes/Other Services/17 Amazon X-Ray/#elastic-beanstalk-integration","text":"X-Ray is already installed in the elastic beanstalk. To enable the x-ray, update/create a config under the beanstalk extensions, .ebextensions/x-ray-daemon.config . This can also be done using by management console. To get trace data, code must instrumented withe x-ray sdk","title":"Elastic Beanstalk Integration"},{"location":"Notes/Other Services/17 Amazon X-Ray/#ecs-integration","text":"ECS can integrate x-ray by x-ray daemon and side-car pattern As Daemon Pattern : To use x-ray in ecs, we need to use the x-ray daemon container for each of the ecs instances. In this case, the x-ray daemon acts as the x-ray agents. As Side Car Pattern : In this case, each of the container in the ecs will have x-ray daemon inside it. When we need to use x-ray in the fargate, we have to use the side-car pattern. Because, we do not have control over the ec2 instances in fargate. To set up Run x-ray daemon according to the suited pattern Make port mapping to allows UDP 2000 for x-ray daemon","title":"ECS Integration"},{"location":"Notes/Other Services/17 Amazon X-Ray/#lambda-integration","text":"To implement the X-ray tracing, we need to implement the followings, Enable ACtive Tracing from the lambda function configuration. This will run the x-ray daemon for the function The attached policy should have write access to the x-ray daemon Following environment policies will require to communicate with x-ray _X_AMZN_TRACE_ID : tracing header AWS_XRAY_DAEMON_ADDRESS : IP_ADDRESS:PORT Others are, AWS_XRAY_CONTEXT_MISSING : default: LOG_ERROR AWS_XRAY_TRACING_NAME : set service name AUTO_INSTRUMENT : applicable for django framework","title":"Lambda Integration"},{"location":"Notes/Other Services/17 Amazon X-Ray/#ec2-integration","text":"To use x-ray we need the x-ray daemon in the ec2 instance. We can install this manually or update the user script to install the daemon. Also we have to make sure the sg allow UDP of port 2000 for the x-ray daemon","title":"EC2 Integration"},{"location":"Notes/Other Services/17 Amazon X-Ray/#api","text":"After X-Ray collects the data, it use to combine abd summarize the trace data. GetTraceSummaries : Can get the trace summaries, ids and annotations BatchGetTraces : Get the full traces GetGroup : A group resources GetServiceGraph : Return info of which service handle the incoming request To make a custom debug tool, First get all trace ids by GetTraceSummaries From each id, get full traces by BatchGetTraces","title":"API"},{"location":"Notes/Other Services/18 AWS Workdocs/","text":"AWS Workdocs Similar to dropbox or One drive For enhanced security Only power users can invite new external users Only power users can share public links","title":"18 AWS Workdocs"},{"location":"Notes/Other Services/18 AWS Workdocs/#aws-workdocs","text":"Similar to dropbox or One drive For enhanced security Only power users can invite new external users Only power users can share public links","title":"AWS Workdocs"},{"location":"Notes/Other Services/19 AWS Batch/","text":"AWS Batch Use to run thousands of batch computing jobs Dynamically provision Quantity Compute Resource (CPU, Memory) Multiple job Queue can be created high priority jobs are in on-demand instance Low priority jobs are in spot instance AWS Log Driver can be used to get logs in the Cloudwatch for further investigation","title":"19 AWS Batch"},{"location":"Notes/Other Services/19 AWS Batch/#aws-batch","text":"Use to run thousands of batch computing jobs Dynamically provision Quantity Compute Resource (CPU, Memory) Multiple job Queue can be created high priority jobs are in on-demand instance Low priority jobs are in spot instance AWS Log Driver can be used to get logs in the Cloudwatch for further investigation","title":"AWS Batch"},{"location":"Notes/Other Services/20 AWS Trusted Advisor/","text":"AWS Trusted Advisor Provide real time guidance to help AWS best practices It ensure Cost Optimization Security Fault Tolerance Performance Service Limits Can monitor S3 (Check versioning) Redshift Cluster (Check cluster configuration) Exposed Access Keys (Check if the access keys become public) EC2 Reserved Instance (Check if the instance will be expired in 30 days) Service limits (IAM service limit check)","title":"20 AWS Trusted Advisor"},{"location":"Notes/Other Services/20 AWS Trusted Advisor/#aws-trusted-advisor","text":"Provide real time guidance to help AWS best practices It ensure Cost Optimization Security Fault Tolerance Performance Service Limits Can monitor S3 (Check versioning) Redshift Cluster (Check cluster configuration) Exposed Access Keys (Check if the access keys become public) EC2 Reserved Instance (Check if the instance will be expired in 30 days) Service limits (IAM service limit check)","title":"AWS Trusted Advisor"},{"location":"Notes/Other Services/21 AWS GreenGrass/","text":"AWS GreenGrass Used to run lambda in connected devices Execute prediction based machine learning models Communicate between devices without internet Keep device date in sync","title":"21 AWS GreenGrass"},{"location":"Notes/Other Services/21 AWS GreenGrass/#aws-greengrass","text":"Used to run lambda in connected devices Execute prediction based machine learning models Communicate between devices without internet Keep device date in sync","title":"AWS GreenGrass"},{"location":"Notes/Other Services/22 AWS Migration Hub/","text":"AWS Migration Hub Use to Choose migration tool Track the progress of migration in AWS Single place to monitor migration Can be helpful while using DMS","title":"22 AWS Migration Hub"},{"location":"Notes/Other Services/22 AWS Migration Hub/#aws-migration-hub","text":"Use to Choose migration tool Track the progress of migration in AWS Single place to monitor migration Can be helpful while using DMS","title":"AWS Migration Hub"},{"location":"Notes/Other Services/23 AWS IoT Core/","text":"AWS IoT Core Lets connected device securely and easily interact with cloud applications Allow secure communication and data processing across different applications","title":"23 AWS IoT Core"},{"location":"Notes/Other Services/23 AWS IoT Core/#aws-iot-core","text":"Lets connected device securely and easily interact with cloud applications Allow secure communication and data processing across different applications","title":"AWS IoT Core"},{"location":"Notes/Other Services/24 AWS Secret Manager/","text":"AWS Secret Manager Manage secrets for applications Can encrypt the keys and credentials Can be enabled auto rotation Used for Database credentials API keys Other app secrets","title":"24 AWS Secret Manager"},{"location":"Notes/Other Services/24 AWS Secret Manager/#aws-secret-manager","text":"Manage secrets for applications Can encrypt the keys and credentials Can be enabled auto rotation Used for Database credentials API keys Other app secrets","title":"AWS Secret Manager"},{"location":"Notes/Other Services/25 AWS Cost Explorer/","text":"AWS Cost Explorer Enable to View usage Analyze cost Visualize over time Chart Graph Can be used to generate usage and cost periodically (Like everyday) over time and put it in s3 as CSV file","title":"25 AWS Cost Explorer"},{"location":"Notes/Other Services/25 AWS Cost Explorer/#aws-cost-explorer","text":"Enable to View usage Analyze cost Visualize over time Chart Graph Can be used to generate usage and cost periodically (Like everyday) over time and put it in s3 as CSV file","title":"AWS Cost Explorer"},{"location":"Notes/Other Services/26 AWS Budget/","text":"AWS Budget Use for setting custom budget alert","title":"26 AWS Budget"},{"location":"Notes/Other Services/26 AWS Budget/#aws-budget","text":"Use for setting custom budget alert","title":"AWS Budget"},{"location":"Notes/Other Services/27 AWS Inspector/","text":"AWS Inspector Automated security assessment service Improve security and application compliance","title":"27 AWS Inspector"},{"location":"Notes/Other Services/27 AWS Inspector/#aws-inspector","text":"Automated security assessment service Improve security and application compliance","title":"AWS Inspector"},{"location":"Notes/Other Services/28 AWS Policy Simulator/","text":"AWS Policy Simulator Can evaluate the policy, permissions and actions Does not make the actual changes in the resources Since it does not make any actual request, it only response if the action is allowed or denied Any changes in the policy emulator does not change the actual policy In AWS Organizations, the SCP can be simulated To use in cli Need the context keys Use iam simulate-custom-policy command","title":"28 AWS Policy Simulator"},{"location":"Notes/Other Services/28 AWS Policy Simulator/#aws-policy-simulator","text":"Can evaluate the policy, permissions and actions Does not make the actual changes in the resources Since it does not make any actual request, it only response if the action is allowed or denied Any changes in the policy emulator does not change the actual policy In AWS Organizations, the SCP can be simulated To use in cli Need the context keys Use iam simulate-custom-policy command","title":"AWS Policy Simulator"},{"location":"Notes/Other Services/29 AWS CodeStar/","text":"AWS CodeStar Quickly develop, build and deploy app within AWS Can use template of ec2, beanstalk or lambda From dashboard we can see day to day activity and dev tasks","title":"29 AWS CodeStar"},{"location":"Notes/Other Services/29 AWS CodeStar/#aws-codestar","text":"Quickly develop, build and deploy app within AWS Can use template of ec2, beanstalk or lambda From dashboard we can see day to day activity and dev tasks","title":"AWS CodeStar"},{"location":"Notes/Other Services/30 Step Function/","text":"Step Function JSON state machine Orchestrate virtual workflow by lambda functions Can resolve the timeout issue of the long running lambda function Also work with ECS EC2 API Gateway On Premise Server Has feature to implement Human Approval Feature , but not native Use to automate the recurring task States Task State : Used to do work, task or run processes Choice State : Make choices between branches of execution Failed or Succeed State : Stop the execution with fail or success state Pass State : Do not run any process, simply take input and pass it to output. If needs any fix, do the fixing and send to the output Wait State : Make a delay before next time. The delayed time can be specified Parallel State : Start parallel branch execution Map State : Iterate the states. ItemPath used here To run process, Task State or Parallel State can be used. Handle Error Both the Task State and Parallel state has Catch and Retry field. To handle error and do a retry we can make use of these state machine features. Handle Error Retrier For retry, we can define different policy for the error. With different error, we can go through different error policy. ErrorEquals : Array of error names. When error occur, it goes to the corresponding retry policy. IntervalSeconds : Interval time before retry. Default is 1 second. MaxAttempts : Maximum number of retry, default is 3. BackoffRate : Additional delay with each new retry, default is 2 seconds Catcher ErrorEquals : Array of error names. When error occurs, it goes to the exactly same named retry policy. Next : Next state machine name ResultPath : Determine next state machine input path Best Practices Specify the timeout for the state machine For larger payload between functions, use S3","title":"30 Step Function"},{"location":"Notes/Other Services/30 Step Function/#step-function","text":"JSON state machine Orchestrate virtual workflow by lambda functions Can resolve the timeout issue of the long running lambda function Also work with ECS EC2 API Gateway On Premise Server Has feature to implement Human Approval Feature , but not native Use to automate the recurring task States Task State : Used to do work, task or run processes Choice State : Make choices between branches of execution Failed or Succeed State : Stop the execution with fail or success state Pass State : Do not run any process, simply take input and pass it to output. If needs any fix, do the fixing and send to the output Wait State : Make a delay before next time. The delayed time can be specified Parallel State : Start parallel branch execution Map State : Iterate the states. ItemPath used here To run process, Task State or Parallel State can be used. Handle Error Both the Task State and Parallel state has Catch and Retry field. To handle error and do a retry we can make use of these state machine features.","title":"Step Function"},{"location":"Notes/Other Services/30 Step Function/#handle-error","text":"Retrier For retry, we can define different policy for the error. With different error, we can go through different error policy. ErrorEquals : Array of error names. When error occur, it goes to the corresponding retry policy. IntervalSeconds : Interval time before retry. Default is 1 second. MaxAttempts : Maximum number of retry, default is 3. BackoffRate : Additional delay with each new retry, default is 2 seconds Catcher ErrorEquals : Array of error names. When error occurs, it goes to the exactly same named retry policy. Next : Next state machine name ResultPath : Determine next state machine input path Best Practices Specify the timeout for the state machine For larger payload between functions, use S3","title":"Handle Error"},{"location":"Notes/Other Services/31 AWS CDK/","text":"AWS CDK Cloud Development Kit Use to model and provision the resources using the languages like TypeScript, JavaScript, Python, Java, C# etc Ultimately, codes in the CDK compiles to Cloudformation","title":"31 AWS CDK"},{"location":"Notes/Other Services/31 AWS CDK/#aws-cdk","text":"Cloud Development Kit Use to model and provision the resources using the languages like TypeScript, JavaScript, Python, Java, C# etc Ultimately, codes in the CDK compiles to Cloudformation","title":"AWS CDK"},{"location":"Notes/Other Services/32 AWS Security Center/","text":"AWS Security Center Receive the security update Can report security concerns","title":"32 AWS Security Center"},{"location":"Notes/Other Services/32 AWS Security Center/#aws-security-center","text":"Receive the security update Can report security concerns","title":"AWS Security Center"},{"location":"Notes/Other Services/33 AWS Data Pipeline/","text":"AWS Data Pipeline Web service Use to move data Example ec2 to s3 Use Data Nodes to determine the source and the destination","title":"33 AWS Data Pipeline"},{"location":"Notes/Other Services/33 AWS Data Pipeline/#aws-data-pipeline","text":"Web service Use to move data Example ec2 to s3 Use Data Nodes to determine the source and the destination","title":"AWS Data Pipeline"},{"location":"Notes/Other Services/01 CI-CD_.md/01 Overview/","text":"CI/CD Flow Code -> Build -> Test -> Deploy -> Provision AWS Codepipeline AWS Codecommit Like Github Responsible for Code portion AWS Codebuild Like Jenkins Responsible for Build and Test portion AWS Code Deploy Responsible for Deploy portion Ues AWS Beanstalk or AWS Cloudformation to provision the code Can be use for deploying code to EC2 Instance On premise sever Lambda Function Use for Rapid release of new feature Updating Lambda Function Avoid downtime during Application deployment Type of code deploy Linear (This deployment send traffic incrementally) All at once (All traffic shifts to new deployment) Canary (Can be define, how many traffic will flow new and updated deployments) While deploying a code base Code commit can be a source stage Beanstalk can be a deploy stage Blue/Green Deployment Isolation between blue and green Roll incoming traffic during deployments Minimum downtime","title":"01 Overview"},{"location":"Notes/Other Services/01 CI-CD_.md/01 Overview/#cicd","text":"Flow Code -> Build -> Test -> Deploy -> Provision AWS Codepipeline AWS Codecommit Like Github Responsible for Code portion AWS Codebuild Like Jenkins Responsible for Build and Test portion AWS Code Deploy Responsible for Deploy portion Ues AWS Beanstalk or AWS Cloudformation to provision the code Can be use for deploying code to EC2 Instance On premise sever Lambda Function Use for Rapid release of new feature Updating Lambda Function Avoid downtime during Application deployment Type of code deploy Linear (This deployment send traffic incrementally) All at once (All traffic shifts to new deployment) Canary (Can be define, how many traffic will flow new and updated deployments) While deploying a code base Code commit can be a source stage Beanstalk can be a deploy stage Blue/Green Deployment Isolation between blue and green Roll incoming traffic during deployments Minimum downtime","title":"CI/CD"},{"location":"Notes/Other Services/01 CI-CD_.md/02 CodeCommit/","text":"CodeCommit AWS service similar to github. Permissions of codecommit, codecommit:* : Allow all the actions codecommit:CreateBranch : Allow to create branches codecommit:DeleteBranch : Allow to delete branches codecommit:CreateRepository : Allow to create repository codecommit:DeleteRepository : Allow to delete repository To give access of codecommit to other developers, we can dod any of the followings Create git credentials with the AWS Credential profile (to take the access key credentials) Generate a new SSH keys and associate SSH kys to the IAM user Generate HTTPS git credentials and specify them in git credential manager For developer with another account, Create cross account role Give repository permission to that role Provide the role arn to the developer","title":"02 CodeCommit"},{"location":"Notes/Other Services/01 CI-CD_.md/02 CodeCommit/#codecommit","text":"AWS service similar to github. Permissions of codecommit, codecommit:* : Allow all the actions codecommit:CreateBranch : Allow to create branches codecommit:DeleteBranch : Allow to delete branches codecommit:CreateRepository : Allow to create repository codecommit:DeleteRepository : Allow to delete repository To give access of codecommit to other developers, we can dod any of the followings Create git credentials with the AWS Credential profile (to take the access key credentials) Generate a new SSH keys and associate SSH kys to the IAM user Generate HTTPS git credentials and specify them in git credential manager For developer with another account, Create cross account role Give repository permission to that role Provide the role arn to the developer","title":"CodeCommit"},{"location":"Notes/Other Services/01 CI-CD_.md/03 CodeDeploy/","text":"Codedeploy CodeDeploy can be used to deploy code to EC2 instance of on-premise server. Usage EC2 Instances On premise servers Serverless Lambda functions ECS Services Code Origins S3 Buckets Github Repositories Bitbucket Repositories CodeCommit Setup Need the code-deploy agent in the server A config file must be included in the root of the source code, like appspec.yml Process On commit, codeDeploy will pull the code Deploy according to the config files CodeDeploy agent will report the success/failure CodeDeploy only do the deployment, do not provision resource Blue/green deployments are only available for the EC2 Instances not the on-premise servers When need to deploy in multiple environments, need to create multiple codeDeploy group AppSpec With appspec.yml , we define how we get the codebase and deploy it. In the File Section we define the source like s3 or github. Then, have a sequence of following hooks and we can define our actions in these hook, Application Stop : Stopping the existing version of app DownloadBundle : Get/download the new codebase BeforeInstall : Task to do before installing the new app AfterInstall : Task to do after installing the app ApplicationStart : Task to start the new app ValidateService : A health check to determine if the app is running properly All hooks are, ApplicationStop -> DownloadBundle -> BeforeInstall -> Install -> AfterInstall -> ApplicationStart -> ValidateService -> BeforeAllowTraffic -> AllowTraffic -> AfterAllowTraffic Types Of Deployment In Place Deployment : Also known as Half at a time . First half of the instance get deployed and then the other half of the application deployed. Blue Green Deployment : Initially it keeps the previous instances and application. A new set of instance will be created and load balancer send traffic on both of these. If everything goes fine, all the traffic will go to the new instances. Blue green deployment is not supported by on premise servers Get Secure Parameters In CodeDeploy from Parameter Store Create IAM role to access the Parameter Store Use ssm get-parameters option Rollback When Missing Files During rollback, if the existing files are removed or no permissions, Put these files in the instances Create a new application instance","title":"03 CodeDeploy"},{"location":"Notes/Other Services/01 CI-CD_.md/03 CodeDeploy/#codedeploy","text":"CodeDeploy can be used to deploy code to EC2 instance of on-premise server.","title":"Codedeploy"},{"location":"Notes/Other Services/01 CI-CD_.md/03 CodeDeploy/#usage","text":"EC2 Instances On premise servers Serverless Lambda functions ECS Services","title":"Usage"},{"location":"Notes/Other Services/01 CI-CD_.md/03 CodeDeploy/#code-origins","text":"S3 Buckets Github Repositories Bitbucket Repositories CodeCommit Setup Need the code-deploy agent in the server A config file must be included in the root of the source code, like appspec.yml Process On commit, codeDeploy will pull the code Deploy according to the config files CodeDeploy agent will report the success/failure CodeDeploy only do the deployment, do not provision resource Blue/green deployments are only available for the EC2 Instances not the on-premise servers When need to deploy in multiple environments, need to create multiple codeDeploy group AppSpec With appspec.yml , we define how we get the codebase and deploy it. In the File Section we define the source like s3 or github. Then, have a sequence of following hooks and we can define our actions in these hook, Application Stop : Stopping the existing version of app DownloadBundle : Get/download the new codebase BeforeInstall : Task to do before installing the new app AfterInstall : Task to do after installing the app ApplicationStart : Task to start the new app ValidateService : A health check to determine if the app is running properly All hooks are, ApplicationStop -> DownloadBundle -> BeforeInstall -> Install -> AfterInstall -> ApplicationStart -> ValidateService -> BeforeAllowTraffic -> AllowTraffic -> AfterAllowTraffic Types Of Deployment In Place Deployment : Also known as Half at a time . First half of the instance get deployed and then the other half of the application deployed. Blue Green Deployment : Initially it keeps the previous instances and application. A new set of instance will be created and load balancer send traffic on both of these. If everything goes fine, all the traffic will go to the new instances. Blue green deployment is not supported by on premise servers Get Secure Parameters In CodeDeploy from Parameter Store Create IAM role to access the Parameter Store Use ssm get-parameters option Rollback When Missing Files During rollback, if the existing files are removed or no permissions, Put these files in the instances Create a new application instance","title":"Code Origins"},{"location":"Notes/Other Services/01 CI-CD_.md/04 CodeBuild/","text":"CodeBuild Run On Proxy Server To run the codeBuild in the proxy server, Configure ssl-bump Update server security policy for ssl-bump Specify the proxy element in the buildspec.yml When the developer does not have the access of the code and can not run edit buildspec.yml , he can use cli to run the code build using the parameter buildspecOverride Access VPC Resources During Testing By default the codeBuild can not access the vpc resource. To give access, we have to provide vpc specific config like vpc id, subnet id, sg id etc.","title":"04 CodeBuild"},{"location":"Notes/Other Services/01 CI-CD_.md/04 CodeBuild/#codebuild","text":"Run On Proxy Server To run the codeBuild in the proxy server, Configure ssl-bump Update server security policy for ssl-bump Specify the proxy element in the buildspec.yml When the developer does not have the access of the code and can not run edit buildspec.yml , he can use cli to run the code build using the parameter buildspecOverride Access VPC Resources During Testing By default the codeBuild can not access the vpc resource. To give access, we have to provide vpc specific config like vpc id, subnet id, sg id etc.","title":"CodeBuild"},{"location":"Notes/Other Services/02 Elastic Beanstalk/01 Elastic beanstalk Overview/","text":"Elastic beanstalk Allow deploy app from docker container Handles Deployment Capacity provisioning Load balancing Auto scaling Health monitoring We can take full control of the underlying resources An automation example with Beanstalk Create LAMP stack Download latest PHP from S3 Set up ELB Store the Application Files to S3 Store the Server Logs Files to S3 or Cloudwatch , optionally Can use the worker process environment for the logn running tasks and also fo decoupling the application Environment files Dockerrun.aws.json used to to configure multi-container docker environments env.yaml used to configure environment name, solution stack and environment links cron.yaml used to define scheduled worker tasks Instance Profile Used to ensure the interaction with other aws services Environment Manifest Used to define environment, stack name, point to launch config etc. If beanstalk does not support the environment by default, can be make use of custom environment named packer Platform Update Elastic beanstalk regularly update their platform time to time with new versions. Once our application is running in a legacy version and want to update the underlying version, there are two methods, Update Environments Platform Versions : Recommended approach to go to latest version. Perform a Blue/Green Deployment : Recommended approach to go to a specif version. Best Practices To preserve database on environment deletion In production, create database separately In dev/test, use database retention as Create Snapshot","title":"01 Elastic beanstalk Overview"},{"location":"Notes/Other Services/02 Elastic Beanstalk/01 Elastic beanstalk Overview/#elastic-beanstalk","text":"Allow deploy app from docker container Handles Deployment Capacity provisioning Load balancing Auto scaling Health monitoring We can take full control of the underlying resources An automation example with Beanstalk Create LAMP stack Download latest PHP from S3 Set up ELB Store the Application Files to S3 Store the Server Logs Files to S3 or Cloudwatch , optionally Can use the worker process environment for the logn running tasks and also fo decoupling the application Environment files Dockerrun.aws.json used to to configure multi-container docker environments env.yaml used to configure environment name, solution stack and environment links cron.yaml used to define scheduled worker tasks Instance Profile Used to ensure the interaction with other aws services Environment Manifest Used to define environment, stack name, point to launch config etc. If beanstalk does not support the environment by default, can be make use of custom environment named packer Platform Update Elastic beanstalk regularly update their platform time to time with new versions. Once our application is running in a legacy version and want to update the underlying version, there are two methods, Update Environments Platform Versions : Recommended approach to go to latest version. Perform a Blue/Green Deployment : Recommended approach to go to a specif version. Best Practices To preserve database on environment deletion In production, create database separately In dev/test, use database retention as Create Snapshot","title":"Elastic beanstalk"},{"location":"Notes/Other Services/02 Elastic Beanstalk/02 Elastic beanstalk Deployment/","text":"Deployment Manual Deploy We can deploy code to elastic beanstalk by zip file or war file. We can define the format( zip / war ) by a config file from .elasticbeanstalk/config.yml To deploy we can simply use the eb deploy . Deployment Modes Types of deployment modes: All at once All instances will be updated at a time There's a downtime during this types of deployment Rolling Update couple of instances When first cluster of instances are healthy, move to the next instances Rolling with additional batches Similar to rolling, but spins up a new set of instance So the previous instances are there until new instances are healthy Immutable Spins up new instances in new ASG When new instances are healthy, move all previous instances Blue/Green A new environment will be deployed A partial percentage of the traffic will route first If the new environment goes right, all traffic will go to new env and previous instances will be removed Traffic Splitting (Canary) New instances will be instantiated in temp ASG A small percentage of traffic goes there If health is good in new instances, route all traffic there and remove the existing one Traffic Splitting (Canary) vs Blue/Green Canary use ASG to split traffic, Blue/Green use Route53 to split traffic Canary is automated wherase the Blue/Green is lot of manual processing","title":"02 Elastic beanstalk Deployment"},{"location":"Notes/Other Services/02 Elastic Beanstalk/02 Elastic beanstalk Deployment/#deployment","text":"","title":"Deployment"},{"location":"Notes/Other Services/02 Elastic Beanstalk/02 Elastic beanstalk Deployment/#manual-deploy","text":"We can deploy code to elastic beanstalk by zip file or war file. We can define the format( zip / war ) by a config file from .elasticbeanstalk/config.yml To deploy we can simply use the eb deploy .","title":"Manual Deploy"},{"location":"Notes/Other Services/02 Elastic Beanstalk/02 Elastic beanstalk Deployment/#deployment-modes","text":"Types of deployment modes: All at once All instances will be updated at a time There's a downtime during this types of deployment Rolling Update couple of instances When first cluster of instances are healthy, move to the next instances Rolling with additional batches Similar to rolling, but spins up a new set of instance So the previous instances are there until new instances are healthy Immutable Spins up new instances in new ASG When new instances are healthy, move all previous instances Blue/Green A new environment will be deployed A partial percentage of the traffic will route first If the new environment goes right, all traffic will go to new env and previous instances will be removed Traffic Splitting (Canary) New instances will be instantiated in temp ASG A small percentage of traffic goes there If health is good in new instances, route all traffic there and remove the existing one Traffic Splitting (Canary) vs Blue/Green Canary use ASG to split traffic, Blue/Green use Route53 to split traffic Canary is automated wherase the Blue/Green is lot of manual processing","title":"Deployment Modes"},{"location":"Notes/Other Services/02 Elastic Beanstalk/03 Elastic Beanstalk Lifecycle/","text":"Elastic Beanstalk Lifecycle Each time we deploy new code to elastic beanstalk, a new version is created. There is a default version quota and we can not deploy any more when we reach the limit. To configure, we can make use of the lifecycle policy. By lifecycle policy, Define how many versions will be kept Define how many older days, we will kept the versions of the application Where do we kept the legacy versions (like s3) after being expired by number of versions or days","title":"03 Elastic Beanstalk Lifecycle"},{"location":"Notes/Other Services/02 Elastic Beanstalk/03 Elastic Beanstalk Lifecycle/#elastic-beanstalk-lifecycle","text":"Each time we deploy new code to elastic beanstalk, a new version is created. There is a default version quota and we can not deploy any more when we reach the limit. To configure, we can make use of the lifecycle policy. By lifecycle policy, Define how many versions will be kept Define how many older days, we will kept the versions of the application Where do we kept the legacy versions (like s3) after being expired by number of versions or days","title":"Elastic Beanstalk Lifecycle"},{"location":"Notes/Route 53/01 Fundamentals/","text":"Route 53 Fundamentals Managed DNS Also work as a Domain Registerer Records A hostname to IPv4 AAAA hostname to IPv6 CNAME hostname to hostname Alias hostname to AWS resource Health Check Active-Active Failover All resources should be available all the time All records should be Same name Same Records Same Routing Policy Active-Passive Failover Primary Resource available all the time In case of Failover , Secondary Resource always in standby Support A (Address Record) AAAA (IP V6 address record) CNAME (Canonical name record) CAA (Certification Authority Authorization) MX (Mail Exchange Record) NAPTR (name Authority Pointer Record) NS (Name Server record) SoA (Start of authority record) SPF (Sender Policy Framework) SRV (Service locator) TXT (Text Record) Not Supported DNSSEC ### TTL Stands for Time To Live Default value is 300 sec During this time the browser does not make the request to DNS server for IP When TTL is High The server make less query to the DNS server DNS server has low traffic To change IP, need to wait long time When TTL is low DNS server make DNS query more frequently Easy to change the IP CNAME vs Alias CNAME Points hostname to hostname Only works for non-root domain Alias Points to AWS Resource Works for both root and non-root domain Alias records is a AWS feature for its resources Routing Policy Simple Routing Policy Send traffic randomly to the resources Weighted Routing Policy Send traffic multiple resource with specified percentage Latency Based Routing Policy Send traffic to the resource that has lowest latency Failover Routing Policy Active passive failover (Check the upper section) Geo location Routing Policy Send traffic based on users location Geo Proximity Routing Policy Send traffic based on resource location Multi-value Routing Policy Simple routing policy with health check 3rd Party Domains To import 3rd party domain to Route 53 Create a hosted zone in Route53 Update NS record on 3rd party website to use Route53","title":"01 Fundamentals"},{"location":"Notes/Route 53/01 Fundamentals/#route-53-fundamentals","text":"Managed DNS Also work as a Domain Registerer Records A hostname to IPv4 AAAA hostname to IPv6 CNAME hostname to hostname Alias hostname to AWS resource Health Check Active-Active Failover All resources should be available all the time All records should be Same name Same Records Same Routing Policy Active-Passive Failover Primary Resource available all the time In case of Failover , Secondary Resource always in standby Support A (Address Record) AAAA (IP V6 address record) CNAME (Canonical name record) CAA (Certification Authority Authorization) MX (Mail Exchange Record) NAPTR (name Authority Pointer Record) NS (Name Server record) SoA (Start of authority record) SPF (Sender Policy Framework) SRV (Service locator) TXT (Text Record) Not Supported DNSSEC ### TTL Stands for Time To Live Default value is 300 sec During this time the browser does not make the request to DNS server for IP When TTL is High The server make less query to the DNS server DNS server has low traffic To change IP, need to wait long time When TTL is low DNS server make DNS query more frequently Easy to change the IP","title":"Route 53 Fundamentals"},{"location":"Notes/Route 53/01 Fundamentals/#cname-vs-alias","text":"CNAME Points hostname to hostname Only works for non-root domain Alias Points to AWS Resource Works for both root and non-root domain Alias records is a AWS feature for its resources","title":"CNAME vs Alias"},{"location":"Notes/Route 53/01 Fundamentals/#routing-policy","text":"Simple Routing Policy Send traffic randomly to the resources Weighted Routing Policy Send traffic multiple resource with specified percentage Latency Based Routing Policy Send traffic to the resource that has lowest latency Failover Routing Policy Active passive failover (Check the upper section) Geo location Routing Policy Send traffic based on users location Geo Proximity Routing Policy Send traffic based on resource location Multi-value Routing Policy Simple routing policy with health check","title":"Routing Policy"},{"location":"Notes/Route 53/01 Fundamentals/#3rd-party-domains","text":"To import 3rd party domain to Route 53 Create a hosted zone in Route53 Update NS record on 3rd party website to use Route53","title":"3rd Party Domains"},{"location":"Notes/S3/01 S3 Overview /","text":"S3 S3 is Evenly Consistent S3 AlowMethod has 5 methods support GET PUT POST DELETE HEAD To prevent accidental delete Enable MFA Delete Enable Versioning Put appropriate IAM Role In a bucket, various objects can have various storage class To ensure the data is successfully inserted in the S3 , use HTTP 200 status MD5 Checksum","title":"01 S3 Overview "},{"location":"Notes/S3/01 S3 Overview /#s3","text":"S3 is Evenly Consistent S3 AlowMethod has 5 methods support GET PUT POST DELETE HEAD To prevent accidental delete Enable MFA Delete Enable Versioning Put appropriate IAM Role In a bucket, various objects can have various storage class To ensure the data is successfully inserted in the S3 , use HTTP 200 status MD5 Checksum","title":"S3"},{"location":"Notes/S3/02 Data Retrival/","text":"Data Retrieval AWS allows two types of archive in s3 for long time data persisting. AWS Glacier AWS Glacier Deep Archive AWS Glacier 4 Types of Data Retrieval Vault Lock Used for long term record retention AWS Glacier allows to lock the storage with various compliance control Expedited Retrieval Data can be retrieved withing 1-5 minutes Comparatively quick retrieval for urgent need Bulk Retrieval Retrieve data withing 5-12 hours Lowest cost Used to retrieved large amount of data with lowest cost Standard Retrieval Data is retrieved within few hours 3-5 hours Minimum storage charge 90 days duration To transfer object object from S3 Glacier to S3 Regular Class First need to restore the objects Then copy to the S3 Regular Class S3 Regular Class means not the Glacier / Deep Archive Provisioned Capacity allows to use Expedited Retrieval whenever necessary AWS Glacier Deep Archive Standard - 12 hours Bulk - 48 hours Minimum storage charge 180 days duration","title":"02 Data Retrival"},{"location":"Notes/S3/02 Data Retrival/#data-retrieval","text":"AWS allows two types of archive in s3 for long time data persisting. AWS Glacier AWS Glacier Deep Archive","title":"Data Retrieval"},{"location":"Notes/S3/02 Data Retrival/#aws-glacier","text":"4 Types of Data Retrieval Vault Lock Used for long term record retention AWS Glacier allows to lock the storage with various compliance control Expedited Retrieval Data can be retrieved withing 1-5 minutes Comparatively quick retrieval for urgent need Bulk Retrieval Retrieve data withing 5-12 hours Lowest cost Used to retrieved large amount of data with lowest cost Standard Retrieval Data is retrieved within few hours 3-5 hours Minimum storage charge 90 days duration To transfer object object from S3 Glacier to S3 Regular Class First need to restore the objects Then copy to the S3 Regular Class S3 Regular Class means not the Glacier / Deep Archive Provisioned Capacity allows to use Expedited Retrieval whenever necessary","title":"AWS Glacier"},{"location":"Notes/S3/02 Data Retrival/#aws-glacier-deep-archive","text":"Standard - 12 hours Bulk - 48 hours Minimum storage charge 180 days duration","title":"AWS Glacier Deep Archive"},{"location":"Notes/S3/03 S3 Buckets And Objects/","text":"Buckets And Objects Buckets Store objects (aka files ) in Buckets (aka directories ) Should be globally unique name Although S3 is Global , buckets are Region Specific Naming convention No Uppercase No Underscore 3 to 63 characters No IP Start with Lowercase Number Objects Objects has Bucket Name , Prefix , and File Name For a object When Object name is s3://bucket_name/key Here bucket_name is the Bucket Name No Prefix here key is the file name When Object name is s3://bucket_name/folder_1/folder_2/key Here bucket_name is the Bucket Name folder_1 and folder_2 is the Nested Folder , where the File placed key is the file name There is no Folder concept in the S3 , UI tricks to think that way Object can be at most 5TB size Can not upload a Object more than 5GB without multipart Object Metadata contains User Metadata System Metadata Tag can be added Max 10 Tags supported Use for Security Lifecycle Policy When Versioning is enabled , then the Object has Version ID","title":"03 S3 Buckets And Objects"},{"location":"Notes/S3/03 S3 Buckets And Objects/#buckets-and-objects","text":"","title":"Buckets And Objects"},{"location":"Notes/S3/03 S3 Buckets And Objects/#buckets","text":"Store objects (aka files ) in Buckets (aka directories ) Should be globally unique name Although S3 is Global , buckets are Region Specific Naming convention No Uppercase No Underscore 3 to 63 characters No IP Start with Lowercase Number","title":"Buckets"},{"location":"Notes/S3/03 S3 Buckets And Objects/#objects","text":"Objects has Bucket Name , Prefix , and File Name For a object When Object name is s3://bucket_name/key Here bucket_name is the Bucket Name No Prefix here key is the file name When Object name is s3://bucket_name/folder_1/folder_2/key Here bucket_name is the Bucket Name folder_1 and folder_2 is the Nested Folder , where the File placed key is the file name There is no Folder concept in the S3 , UI tricks to think that way Object can be at most 5TB size Can not upload a Object more than 5GB without multipart Object Metadata contains User Metadata System Metadata Tag can be added Max 10 Tags supported Use for Security Lifecycle Policy When Versioning is enabled , then the Object has Version ID","title":"Objects"},{"location":"Notes/S3/04 S3 Versioning/","text":"S3 Versioning Enable Version of S3 files Should be enabled in Bucket Level Useful To protect file against un-intended deletes Roll back to any version If Versioning is enable to a Non-versioned bucket, the existing objects get a version of NULL If Versioning is disabled to a Bucket , existing Versioned Object exist When Versioning is enabled Latest version has a flag Latest Version Deleted File has a flag Deleted Marker","title":"04 S3 Versioning"},{"location":"Notes/S3/04 S3 Versioning/#s3-versioning","text":"Enable Version of S3 files Should be enabled in Bucket Level Useful To protect file against un-intended deletes Roll back to any version If Versioning is enable to a Non-versioned bucket, the existing objects get a version of NULL If Versioning is disabled to a Bucket , existing Versioned Object exist When Versioning is enabled Latest version has a flag Latest Version Deleted File has a flag Deleted Marker","title":"S3 Versioning"},{"location":"Notes/S3/05 S3 Encryption/","text":"Encryption 4 types of Encryption is available SSE-S3 Server side encryption handled by AWS ( S3 ) Data key is managed by S3 Use AES-256 algorithm Header should be x-amz-server-side-encryption: AES-256 Completely free SSE-KMS Server side encryption Key is managed by KMS Header should be x-amz-server-side-encryption: aws:kms Useful because Using KMS , we can determine who has access keys Audit Trail SSE-C Server side encryption Key is provided by us S3 does not store the key To use Must be use HTTPS Every time we pass the Encryption Key using HTTP Header as z-amz-server-side-encryption-customer-key Every time we pass the Algorithm Name using HTTP Header as z-amz-server-side-encryption-customer-algorithm Every time we pass the MD5 Key using HTTP Header as z-amz-server-side-encryption-customer-key-md5 HMAC (Hash based Message Authentication Code) is a salted version of encryption keys AWS generate and store the HMAC to validate the encryption and decryption key HMAC is only for validate the original key, can not be used to encrypt or decrypt the object Since in SSE-C , the key is managed by the client, if the key is lost, all the data will also be lost Client Side Encryption We encrypt the object before uploading We decrypt object after retrieving from the S3 To encrypt and decrypt object in client we can use S3 Encryption Client Or other tools Encryption in Transit Also known as SSL/TLS S3 exposes both HTTP HTTPS Default Encryption We can use Bucket Level Default Encryption So any object uploaded to the bucket will be automatically encrypted, even though the we do not pass appropriate header Default encryption only accepts SSE-S3 SSE-KMS It does not accept SSE-C and Client Side Encryption as Default Encryption Bucket Policy evaluated before the Default Encryption Bucket Policy is the old way of Default Encryption Example of in house key management for S3 Create customer managed CMK Encrypt data with the CMK Store encrypted data and data key in S3 Delete the data keys For decrypt, use CMK to decrypt data key Now delete the data using the Decrypted data key","title":"05 S3 Encryption"},{"location":"Notes/S3/05 S3 Encryption/#encryption","text":"4 types of Encryption is available SSE-S3 Server side encryption handled by AWS ( S3 ) Data key is managed by S3 Use AES-256 algorithm Header should be x-amz-server-side-encryption: AES-256 Completely free SSE-KMS Server side encryption Key is managed by KMS Header should be x-amz-server-side-encryption: aws:kms Useful because Using KMS , we can determine who has access keys Audit Trail SSE-C Server side encryption Key is provided by us S3 does not store the key To use Must be use HTTPS Every time we pass the Encryption Key using HTTP Header as z-amz-server-side-encryption-customer-key Every time we pass the Algorithm Name using HTTP Header as z-amz-server-side-encryption-customer-algorithm Every time we pass the MD5 Key using HTTP Header as z-amz-server-side-encryption-customer-key-md5 HMAC (Hash based Message Authentication Code) is a salted version of encryption keys AWS generate and store the HMAC to validate the encryption and decryption key HMAC is only for validate the original key, can not be used to encrypt or decrypt the object Since in SSE-C , the key is managed by the client, if the key is lost, all the data will also be lost Client Side Encryption We encrypt the object before uploading We decrypt object after retrieving from the S3 To encrypt and decrypt object in client we can use S3 Encryption Client Or other tools Encryption in Transit Also known as SSL/TLS S3 exposes both HTTP HTTPS Default Encryption We can use Bucket Level Default Encryption So any object uploaded to the bucket will be automatically encrypted, even though the we do not pass appropriate header Default encryption only accepts SSE-S3 SSE-KMS It does not accept SSE-C and Client Side Encryption as Default Encryption Bucket Policy evaluated before the Default Encryption Bucket Policy is the old way of Default Encryption Example of in house key management for S3 Create customer managed CMK Encrypt data with the CMK Store encrypted data and data key in S3 Delete the data keys For decrypt, use CMK to decrypt data key Now delete the data using the Decrypted data key","title":"Encryption"},{"location":"Notes/S3/06 S3 Security And Bucket Policy/","text":"Security User Based IAM policy determine, which API calls are allowed from console Resource Based Allow Cross Account Object Access Control List (Object ACL) is finer grain Bucket Access Control List (Bucket ACL) is less common A IAM principal allow access S3 if Any IAM permission allow Any Resource Policy allow No explicit deny in anywhere Supports VPC Endpoint , do it is possible to interact with S3 without public internet API Calls can be logged by Cloud Trail S3 Access Logs can be stored in another S3 Bucket User can use MFA to prevent unintended deletion Presigned URL can be used for premium content Presigned URL is generated by user credential token It is valid for limited time Bucket Policy JSON Based Policy Keys in the Policy Effect means is the action Allowed or Denied Principal means who is trying to access/taking-actions It can be User Resource Action means what type of action will be happen Can be READ Object GET Object (single/multiple) UPDATE Object DELETE Object Resource means on which Bucket or Object is targeted Hands On Object can not be uploaded if it is not SSE-S3 Effect should be Deny The Principal be * , since it is applicable for anyone Service is AWS S3 Action be S3 Put Object ARN should be s3://bucket_name/* We have to deny if it does not match two condition The header x-aws-server-side-encryption can not be null The header x-aws-server-side-encryption is not equal AES-256 Bucket Settings for Block Public Access Use to prevent company data leaks We can block public access to a bucket through (No need to remember the names) new ACL any ACL new Public Bucket Access Point Policies","title":"06 S3 Security And Bucket Policy"},{"location":"Notes/S3/06 S3 Security And Bucket Policy/#security","text":"User Based IAM policy determine, which API calls are allowed from console Resource Based Allow Cross Account Object Access Control List (Object ACL) is finer grain Bucket Access Control List (Bucket ACL) is less common A IAM principal allow access S3 if Any IAM permission allow Any Resource Policy allow No explicit deny in anywhere Supports VPC Endpoint , do it is possible to interact with S3 without public internet API Calls can be logged by Cloud Trail S3 Access Logs can be stored in another S3 Bucket User can use MFA to prevent unintended deletion Presigned URL can be used for premium content Presigned URL is generated by user credential token It is valid for limited time","title":"Security"},{"location":"Notes/S3/06 S3 Security And Bucket Policy/#bucket-policy","text":"JSON Based Policy Keys in the Policy Effect means is the action Allowed or Denied Principal means who is trying to access/taking-actions It can be User Resource Action means what type of action will be happen Can be READ Object GET Object (single/multiple) UPDATE Object DELETE Object Resource means on which Bucket or Object is targeted Hands On Object can not be uploaded if it is not SSE-S3 Effect should be Deny The Principal be * , since it is applicable for anyone Service is AWS S3 Action be S3 Put Object ARN should be s3://bucket_name/* We have to deny if it does not match two condition The header x-aws-server-side-encryption can not be null The header x-aws-server-side-encryption is not equal AES-256","title":"Bucket Policy"},{"location":"Notes/S3/06 S3 Security And Bucket Policy/#bucket-settings-for-block-public-access","text":"Use to prevent company data leaks We can block public access to a bucket through (No need to remember the names) new ACL any ACL new Public Bucket Access Point Policies","title":"Bucket Settings for Block Public Access"},{"location":"Notes/S3/07 S3 Website/","text":"S3 Website S3 can be used to host static website To use S3 to host static website, we need to allow public read access Site URL can be <bucket-name>.s3-website.<AWS-region>.amazonaws.com <bucket-name>.s3-website-<AWS-region>.amazonaws.com Only difference is after s3-website Could be . Could be - Hands on Enable Static Site Hosting Upload file index.html and error.html Enable public access Add bucket policy to enable GET Object","title":"07 S3 Website"},{"location":"Notes/S3/07 S3 Website/#s3-website","text":"S3 can be used to host static website To use S3 to host static website, we need to allow public read access Site URL can be <bucket-name>.s3-website.<AWS-region>.amazonaws.com <bucket-name>.s3-website-<AWS-region>.amazonaws.com Only difference is after s3-website Could be . Could be - Hands on Enable Static Site Hosting Upload file index.html and error.html Enable public access Add bucket policy to enable GET Object","title":"S3 Website"},{"location":"Notes/S3/08 S3 CORS/","text":"S3 CORS Consider while using S3 for Static Website Hosting To allow cross-region request, we need to enable CORS in S3 We can define specific origin or any origin using * For AllowMethod , we can use the followings GET PUT POST DELETE HEAD To allow sites, http and https , we have to put 2 Allow Origin URL CORS Rules AllowedOrigin : Specify the origins, can make the cross domain request AllowedMethod : Specify the cross-domain request methods, GET , PUT , POST , DELETE , HEAD AllowedHeader : Specify the preflight allowed headers CORS Rules Element MaxAgeSeconds : Amount of time browser cache the response ExposeHeader : :TODO","title":"08 S3 CORS"},{"location":"Notes/S3/08 S3 CORS/#s3-cors","text":"Consider while using S3 for Static Website Hosting To allow cross-region request, we need to enable CORS in S3 We can define specific origin or any origin using * For AllowMethod , we can use the followings GET PUT POST DELETE HEAD To allow sites, http and https , we have to put 2 Allow Origin URL","title":"S3 CORS"},{"location":"Notes/S3/08 S3 CORS/#cors-rules","text":"AllowedOrigin : Specify the origins, can make the cross domain request AllowedMethod : Specify the cross-domain request methods, GET , PUT , POST , DELETE , HEAD AllowedHeader : Specify the preflight allowed headers","title":"CORS Rules"},{"location":"Notes/S3/08 S3 CORS/#cors-rules-element","text":"MaxAgeSeconds : Amount of time browser cache the response ExposeHeader : :TODO","title":"CORS Rules Element"},{"location":"Notes/S3/09 S3 Consistency Model/","text":"Consistency Model S3 is Eventual Consistency Model No way to get Strong Consistency Model in S3 We can read a object after write and after the success response If we read a object after write but before success response, we might not get the object for Eventual Consistency Eventual Consistency also applicable for DELETE and PUT object","title":"09 S3 Consistency Model"},{"location":"Notes/S3/09 S3 Consistency Model/#consistency-model","text":"S3 is Eventual Consistency Model No way to get Strong Consistency Model in S3 We can read a object after write and after the success response If we read a object after write but before success response, we might not get the object for Eventual Consistency Eventual Consistency also applicable for DELETE and PUT object","title":"Consistency Model"},{"location":"Notes/S3/10 S3 MFA Delete/","text":"MFA Delete To enable MFA Delete version in the Bucket must be enabled MFA Delete must be enable/disable from the Root Account MFA Delete must be enable/disable from the AWS CLI Use for Permanently delete an object Suspend versioning","title":"10 S3 MFA Delete"},{"location":"Notes/S3/10 S3 MFA Delete/#mfa-delete","text":"To enable MFA Delete version in the Bucket must be enabled MFA Delete must be enable/disable from the Root Account MFA Delete must be enable/disable from the AWS CLI Use for Permanently delete an object Suspend versioning","title":"MFA Delete"},{"location":"Notes/S3/11 S3 Access Logs/","text":"Access Logs Also known as Server Access Logs Logs may be required for audit purposes Access Logs log any authorized/deny request Logs can be analyzed using Athena Do not store logs in the same bucket (Think::: It will make the bucket grow exponentially) Includes Requester Bucket name Request time Request action Referrer Turnaround time Error code information","title":"11 S3 Access Logs"},{"location":"Notes/S3/11 S3 Access Logs/#access-logs","text":"Also known as Server Access Logs Logs may be required for audit purposes Access Logs log any authorized/deny request Logs can be analyzed using Athena Do not store logs in the same bucket (Think::: It will make the bucket grow exponentially) Includes Requester Bucket name Request time Request action Referrer Turnaround time Error code information","title":"Access Logs"},{"location":"Notes/S3/12 S3 Replication/","text":"S3 Replication S3 allows CRR (Cross-region-replication) Use for compliance Lower latency access Replication across accounts SRR (Same-region-replication) Use for Log aggregation (centralized multiple log buckets data) Replication between production, test and staging accounts When cross-region-replication is enabled, it does not replicate the existing objects, only the newly objects Can be replicated to a Bucket of another account Replication is asynchronous To enable replication Versioning must be enabled in both Source Bucket Destination Bucket Should have an proper IAM Policy Activating Replication does not replicate the existing objects Delete operation does not replicate No Chaining Replication happen Bucket 1 replicate to Bucket 2 Bucket 2 replicate to Bucket 3 This does not imply Bucket 1 replicate to Bucket 3","title":"12 S3 Replication"},{"location":"Notes/S3/12 S3 Replication/#s3-replication","text":"S3 allows CRR (Cross-region-replication) Use for compliance Lower latency access Replication across accounts SRR (Same-region-replication) Use for Log aggregation (centralized multiple log buckets data) Replication between production, test and staging accounts When cross-region-replication is enabled, it does not replicate the existing objects, only the newly objects Can be replicated to a Bucket of another account Replication is asynchronous To enable replication Versioning must be enabled in both Source Bucket Destination Bucket Should have an proper IAM Policy Activating Replication does not replicate the existing objects Delete operation does not replicate No Chaining Replication happen Bucket 1 replicate to Bucket 2 Bucket 2 replicate to Bucket 3 This does not imply Bucket 1 replicate to Bucket 3","title":"S3 Replication"},{"location":"Notes/S3/13 S3 Pre Signed Url/","text":"Pre Signed URL Use the permission of the person who generate the url Can generate url using From CLI, for download Using SDK, for upload Default timeout 1 hour Can update url validation by using --expires-in flag Use for Premium content in S3 Temporary user upload file in precise location Ever changing user list, download files by generating url dynamically","title":"13 S3 Pre Signed Url"},{"location":"Notes/S3/13 S3 Pre Signed Url/#pre-signed-url","text":"Use the permission of the person who generate the url Can generate url using From CLI, for download Using SDK, for upload Default timeout 1 hour Can update url validation by using --expires-in flag Use for Premium content in S3 Temporary user upload file in precise location Ever changing user list, download files by generating url dynamically","title":"Pre Signed URL"},{"location":"Notes/S3/14 S3 Storage Tiers/","text":"Storage Tiers 6 types of storage tiers S3 Standard - General Purpose (Use minimum 3 AZ) S3 Standard - Infrequent Access (IA) Use when data is accessed infrequently Use when rapid access is required S3 One Zone - Infrequent Access (Use Single AZ) S3 Intelligent Tiering Glacier Glacier Archive Also there is a deprecated class S3 Reduced Redundancy Storage S3 allows one bucket but multiple storage tier for different folder","title":"14 S3 Storage Tiers"},{"location":"Notes/S3/14 S3 Storage Tiers/#storage-tiers","text":"6 types of storage tiers S3 Standard - General Purpose (Use minimum 3 AZ) S3 Standard - Infrequent Access (IA) Use when data is accessed infrequently Use when rapid access is required S3 One Zone - Infrequent Access (Use Single AZ) S3 Intelligent Tiering Glacier Glacier Archive Also there is a deprecated class S3 Reduced Redundancy Storage S3 allows one bucket but multiple storage tier for different folder","title":"Storage Tiers"},{"location":"Notes/S3/15 S3 Lifecycle Policy/","text":"Lifecycle Policy We can change object storage class using the Lifecycle Policy Can not change storage class from Glacier or Glacier Archive To abort/delete in-complete multipart files after certain time period, lifecycle policy can be used Actions Transition Action Change the storage class after certain times Example Move object to Standard IA after 30 days of creation Archive object after 6 months Expiration Action Delete object after certain times Example Remove access logs after 365 days Removing in-completed multipart files Rules Rules can be applied to object prefix name Rules can be applied to object tags","title":"15 S3 Lifecycle Policy"},{"location":"Notes/S3/15 S3 Lifecycle Policy/#lifecycle-policy","text":"We can change object storage class using the Lifecycle Policy Can not change storage class from Glacier or Glacier Archive To abort/delete in-complete multipart files after certain time period, lifecycle policy can be used","title":"Lifecycle Policy"},{"location":"Notes/S3/15 S3 Lifecycle Policy/#actions","text":"Transition Action Change the storage class after certain times Example Move object to Standard IA after 30 days of creation Archive object after 6 months Expiration Action Delete object after certain times Example Remove access logs after 365 days Removing in-completed multipart files","title":"Actions"},{"location":"Notes/S3/15 S3 Lifecycle Policy/#rules","text":"Rules can be applied to object prefix name Rules can be applied to object tags","title":"Rules"},{"location":"Notes/S3/16 S3 Performance/","text":"S3 Performance Prefix Randomized prefix of s3 buckets improve the performance Each prefix has 3500 put request per sec 5500 get request per sec Baseline S3 prefix is the text between Bucket Name and File Name If object name is bucketName/folder1/folder2/object.jpeg , then the prefix is folder1/folder2 By default S3 is automatically scales at hight request rates For each prefix it is possible to 3500 PUT/COPY/POST/DELETE request 5500 GET/HEAD request There is no limit of prefix The more prefix we use, the more request we can made S3 KMS Limitation If we use SSE-KMS encryption, we need to consider there is Encrypt request to KMS while uploading a file Decrypt request to KMS while downloading a file There is a quota of Number Of KMS Request per second. Multi-Part Upload Recommended to use when file size size is more than 100MB Must use when file size is more than 5GB In this case, while uploading file Files divided in separate parts Upload these parts in parallel S3 join them after being uploaded S3 Transfer Acceleration Use for upload file First file uploaded to the nearest EDGE Location Then using AWS Private Network , goes to desired bucket Compatible with multi-part upload Byte Range Fetches Use for file fetches / download Parallelize Get by requesting specific byte ranges Can be used for retrieve only partial data Like header Response Code 503 when a new object is trying to update, while the same object has million of versions List Items Parameters --size-only : Used to determine if the local and cloud items are synced --exclude : Pass a pattern to exclude items --summary : Display number of retrieved items, total items etc --page-size : Default is 1000 page. If there are too many items, we can specify number of pages --max-items : Numbers of items to be displayed. If there are too many items, we can specify number items to be printed When there are too many items, we can make use of --page-size and --max-items S3 Inventory When an S3 bucket has versioning enabled and a single object has millions of versions, it can throttling and throw 503 error. To determine these objects, can be used the S3 Inventory .","title":"16 S3 Performance"},{"location":"Notes/S3/16 S3 Performance/#s3-performance","text":"","title":"S3 Performance"},{"location":"Notes/S3/16 S3 Performance/#prefix","text":"Randomized prefix of s3 buckets improve the performance Each prefix has 3500 put request per sec 5500 get request per sec","title":"Prefix"},{"location":"Notes/S3/16 S3 Performance/#baseline","text":"S3 prefix is the text between Bucket Name and File Name If object name is bucketName/folder1/folder2/object.jpeg , then the prefix is folder1/folder2 By default S3 is automatically scales at hight request rates For each prefix it is possible to 3500 PUT/COPY/POST/DELETE request 5500 GET/HEAD request There is no limit of prefix The more prefix we use, the more request we can made","title":"Baseline"},{"location":"Notes/S3/16 S3 Performance/#s3-kms-limitation","text":"If we use SSE-KMS encryption, we need to consider there is Encrypt request to KMS while uploading a file Decrypt request to KMS while downloading a file There is a quota of Number Of KMS Request per second.","title":"S3 KMS Limitation"},{"location":"Notes/S3/16 S3 Performance/#multi-part-upload","text":"Recommended to use when file size size is more than 100MB Must use when file size is more than 5GB In this case, while uploading file Files divided in separate parts Upload these parts in parallel S3 join them after being uploaded","title":"Multi-Part Upload"},{"location":"Notes/S3/16 S3 Performance/#s3-transfer-acceleration","text":"Use for upload file First file uploaded to the nearest EDGE Location Then using AWS Private Network , goes to desired bucket Compatible with multi-part upload","title":"S3 Transfer Acceleration"},{"location":"Notes/S3/16 S3 Performance/#byte-range-fetches","text":"Use for file fetches / download Parallelize Get by requesting specific byte ranges Can be used for retrieve only partial data Like header","title":"Byte Range Fetches"},{"location":"Notes/S3/16 S3 Performance/#response-code","text":"503 when a new object is trying to update, while the same object has million of versions","title":"Response Code"},{"location":"Notes/S3/16 S3 Performance/#list-items-parameters","text":"--size-only : Used to determine if the local and cloud items are synced --exclude : Pass a pattern to exclude items --summary : Display number of retrieved items, total items etc --page-size : Default is 1000 page. If there are too many items, we can specify number of pages --max-items : Numbers of items to be displayed. If there are too many items, we can specify number items to be printed When there are too many items, we can make use of --page-size and --max-items","title":"List Items Parameters"},{"location":"Notes/S3/16 S3 Performance/#s3-inventory","text":"When an S3 bucket has versioning enabled and a single object has millions of versions, it can throttling and throw 503 error. To determine these objects, can be used the S3 Inventory .","title":"S3 Inventory"},{"location":"Notes/S3/17 S3 Select and Glacier Select/","text":"S3 Select Server side filtering Can run simple SQL statements to filter Can filter by rows and columns Do not need to load all data to filter Example There is a CSV file in S3 Using S3 Select we can select certain rows and columns Fos S3 Select we can store files as CSV JSON For CSV and JSON files, supported compression is G-ZIP B-ZIP2 Glacier Select Same as S3 Select but happen in Glacier","title":"17 S3 Select and Glacier Select"},{"location":"Notes/S3/17 S3 Select and Glacier Select/#s3-select","text":"Server side filtering Can run simple SQL statements to filter Can filter by rows and columns Do not need to load all data to filter Example There is a CSV file in S3 Using S3 Select we can select certain rows and columns Fos S3 Select we can store files as CSV JSON For CSV and JSON files, supported compression is G-ZIP B-ZIP2","title":"S3 Select"},{"location":"Notes/S3/17 S3 Select and Glacier Select/#glacier-select","text":"Same as S3 Select but happen in Glacier","title":"Glacier Select"},{"location":"Notes/S3/18 S3 Athena/","text":"Athena Serverless service to run analytics in S3 directly Use SQL language to perform query Has JDBC / ODBC driver Built on Presto query engine To reduce cost of query Good to partition data Separate work groups based on users","title":"18 S3 Athena"},{"location":"Notes/S3/18 S3 Athena/#athena","text":"Serverless service to run analytics in S3 directly Use SQL language to perform query Has JDBC / ODBC driver Built on Presto query engine To reduce cost of query Good to partition data Separate work groups based on users","title":"Athena"},{"location":"Notes/S3/19 S3 Athena Vs Select/","text":"Athena Vs Select Both can run SQL. But S3 Select run very simple SQL Statements whereas Athena can run complex SQL Query Athena Athena used for data analytics Analytics is happening in directly S3 Serverless service S3 Select S3 Select is for Data Filtering Not-serverless, it's server-side filtering","title":"19 S3 Athena Vs Select"},{"location":"Notes/S3/19 S3 Athena Vs Select/#athena-vs-select","text":"Both can run SQL. But S3 Select run very simple SQL Statements whereas Athena can run complex SQL Query","title":"Athena Vs Select"},{"location":"Notes/S3/19 S3 Athena Vs Select/#athena","text":"Athena used for data analytics Analytics is happening in directly S3 Serverless service","title":"Athena"},{"location":"Notes/S3/19 S3 Athena Vs Select/#s3-select","text":"S3 Select is for Data Filtering Not-serverless, it's server-side filtering","title":"S3 Select"},{"location":"Notes/S3/20 S3 Event Notification/","text":"Event Notification Anything happen like GET/PUT/COPY/DELETE can raise an event Event can be passed to SNS SQS Lambda Function Versioning should be enabled to ensure the each events notification","title":"20 S3 Event Notification"},{"location":"Notes/S3/20 S3 Event Notification/#event-notification","text":"Anything happen like GET/PUT/COPY/DELETE can raise an event Event can be passed to SNS SQS Lambda Function Versioning should be enabled to ensure the each events notification","title":"Event Notification"},{"location":"Notes/S3/21 S3 Object Lock And Glacier Lock/","text":"S3 Object Lock WORM i.e. Write Once Read Many model Block a object version, no one can delete or modify Glacier Vault Lock WORM i.e. Write Once Read Many model Block a object, so no one can delete or modify Common Theory Lock can be applied on both Bucket Level Object Level Lock has two modes Governance Mode Users with special permission can override the rules and retention period Compliance Mode No one, even the root user can not override the rules and retention period.","title":"21 S3 Object Lock And Glacier Lock"},{"location":"Notes/S3/21 S3 Object Lock And Glacier Lock/#s3-object-lock","text":"WORM i.e. Write Once Read Many model Block a object version, no one can delete or modify","title":"S3 Object Lock"},{"location":"Notes/S3/21 S3 Object Lock And Glacier Lock/#glacier-vault-lock","text":"WORM i.e. Write Once Read Many model Block a object, so no one can delete or modify","title":"Glacier Vault Lock"},{"location":"Notes/S3/21 S3 Object Lock And Glacier Lock/#common-theory","text":"Lock can be applied on both Bucket Level Object Level Lock has two modes Governance Mode Users with special permission can override the rules and retention period Compliance Mode No one, even the root user can not override the rules and retention period.","title":"Common Theory"},{"location":"Notes/S3/22 S3 Protect Object Deletion/","text":"Protect Object Deletion Use MFA delete Use S3 Versioning Use S3 lock or Glacier Lock Use retention period When versioning is enabled and can be enabled different retention period for different version","title":"22 S3 Protect Object Deletion"},{"location":"Notes/S3/22 S3 Protect Object Deletion/#protect-object-deletion","text":"Use MFA delete Use S3 Versioning Use S3 lock or Glacier Lock Use retention period When versioning is enabled and can be enabled different retention period for different version","title":"Protect Object Deletion"},{"location":"Notes/S3/23 S3 Object ACL/","text":"Object ACL To get full access of a object of a bucket by owner needs ACL Policy of bucket-owner-full-control ACL policy can be used to manage buckets object level access","title":"23 S3 Object ACL"},{"location":"Notes/S3/23 S3 Object ACL/#object-acl","text":"To get full access of a object of a bucket by owner needs ACL Policy of bucket-owner-full-control ACL policy can be used to manage buckets object level access","title":"Object ACL"},{"location":"Notes/S3/24 S3 VPC Gateway/","text":"S3 VPC Gateway Need an Endpoint Policy Use this Endpoint Policy for Trusted S3 Bucket","title":"24 S3 VPC Gateway"},{"location":"Notes/S3/24 S3 VPC Gateway/#s3-vpc-gateway","text":"Need an Endpoint Policy Use this Endpoint Policy for Trusted S3 Bucket","title":"S3 VPC Gateway"},{"location":"Notes/S3/25 S3 Event Processing/","text":"Event Processing SQS + lambda If lambda fail to process, send it to DLQ FIFO SQS + lambda If lambda fail to process, send it to DLQ Fan Out Pattern Event first goes to the SNS SNS pass the event to multiple SQS or consumer Let's say, we have 3 SQS service, get same message from application If application crashed after send 2 SQS , the 3rd SQS does not get the message In this case, Fan Out came up Here application only send the message to SNS SNS pass the messages to the respective SQS S3 event can be passed to SQS SNS lambda","title":"25 S3 Event Processing"},{"location":"Notes/S3/25 S3 Event Processing/#event-processing","text":"SQS + lambda If lambda fail to process, send it to DLQ FIFO SQS + lambda If lambda fail to process, send it to DLQ Fan Out Pattern Event first goes to the SNS SNS pass the event to multiple SQS or consumer Let's say, we have 3 SQS service, get same message from application If application crashed after send 2 SQS , the 3rd SQS does not get the message In this case, Fan Out came up Here application only send the message to SNS SNS pass the messages to the respective SQS S3 event can be passed to SQS SNS lambda","title":"Event Processing"},{"location":"Notes/S3/26 S3 Permissions/","text":"S3 Permissions For object level permission use ACL","title":"26 S3 Permissions"},{"location":"Notes/S3/26 S3 Permissions/#s3-permissions","text":"For object level permission use ACL","title":"S3 Permissions"},{"location":"Notes/Serverless/01 Lambda_deprecated/","text":"Lambda Virtual functions No needs to manage servers Run on-demand Automatic scaling Ram uses can be up to 3MB Increasing ram, improves the performance of CPU and Network Almost all programming language are supported by lambda Using Custom Runtime API it can support other languages Docker is not supported . Docker can run in the followings ECS Fargate Cloudwatch Event EventBridge can be used to run CRON job function that is in the Lambda Cloudwatch can be used to debug the code Lambda Limits Execution Memory allocation: 128MB to 3008MB (64MB increments) Max execution time: 15 minutes Env variable size: 4 KB Disk Capacity (/temp): 512MB Concurrent execution: 1000 (can be increased) Deployment Compressed deployment size: 50MB Uncompressed deployment size: 250MB Lambda@Edge Required when Deployed a CDN using Cloudfront Want to run Lambda Function alongside Make 4 types Requests Between User and Cloudfront Viewer Request Viewer Response Between Cloudfront and Origin Origin Request Origin Response Use-cases Web additional security Dynamic app at the Edge SEO Intelligent routing across Origin and Data Center Bot Mitigation at EDGE A/B Testing User Authentication and Authorization User Prioritization User tracking and analytics","title":"01 Lambda deprecated"},{"location":"Notes/Serverless/01 Lambda_deprecated/#lambda","text":"Virtual functions No needs to manage servers Run on-demand Automatic scaling Ram uses can be up to 3MB Increasing ram, improves the performance of CPU and Network Almost all programming language are supported by lambda Using Custom Runtime API it can support other languages Docker is not supported . Docker can run in the followings ECS Fargate Cloudwatch Event EventBridge can be used to run CRON job function that is in the Lambda Cloudwatch can be used to debug the code","title":"Lambda"},{"location":"Notes/Serverless/01 Lambda_deprecated/#lambda-limits","text":"Execution Memory allocation: 128MB to 3008MB (64MB increments) Max execution time: 15 minutes Env variable size: 4 KB Disk Capacity (/temp): 512MB Concurrent execution: 1000 (can be increased) Deployment Compressed deployment size: 50MB Uncompressed deployment size: 250MB","title":"Lambda Limits"},{"location":"Notes/Serverless/01 Lambda_deprecated/#lambdaedge","text":"Required when Deployed a CDN using Cloudfront Want to run Lambda Function alongside Make 4 types Requests Between User and Cloudfront Viewer Request Viewer Response Between Cloudfront and Origin Origin Request Origin Response Use-cases Web additional security Dynamic app at the Edge SEO Intelligent routing across Origin and Data Center Bot Mitigation at EDGE A/B Testing User Authentication and Authorization User Prioritization User tracking and analytics","title":"Lambda@Edge"},{"location":"Notes/Serverless/02 DynamoDB_deprecated/","text":"DynamoDB Managed database, replicated across 3 AZ NoSQL database Enable event driven programming using DynamoDB Streams In DynamoDB database is already created, only needs to create Table Each Table should have a primary key (Must be decided while creating the table) Can have infinite number of rows Max size of item can be 400KB Data types Scalar String Number Binary Boolean Null Document List Map Set String Set Number Set Binary Set To improve performance we can use DAX (mili seconds to micro seconds) Use partition keys of high cardinality, so large number of distinct values for each item DAX Stands for DynamoDB Accelerator Caching mechanism for DynamoDB DynamoDB Stream Raise event on Create , Update , Delete Can be used to trigger events on DB Changes Transaction All or nothing of operations Global Table Replicated in multiple region Needs to enable DynamoDB Stream Active Active Replication Changes in any region, impact all other regions Security Available in VPC Endpoints Fully controlled by IAM Encryption At rest by KMS In flight by SSL / TLS Provisioned Throughput 2 types of Provision Throughput RCU Read Capacity Unit Each RCU can handle one of the followings 1 Strongly Consistent read 4KB/s 2 Eventually Consistent read 4KB/s WCU Write Capacity Unit 1 write 1KB/s Can be used on-demand throughput (price is 2.5X more) Throughput can be exceeded using Burst Credit If Burst Credit is empty, it throws ProvisionThroughputException In case of ProvisionThroughputException , it is recommended to retry Can be used DynamoDB Auto Scaling No need to provision throughput Comparatively expensive","title":"02 DynamoDB deprecated"},{"location":"Notes/Serverless/02 DynamoDB_deprecated/#dynamodb","text":"Managed database, replicated across 3 AZ NoSQL database Enable event driven programming using DynamoDB Streams In DynamoDB database is already created, only needs to create Table Each Table should have a primary key (Must be decided while creating the table) Can have infinite number of rows Max size of item can be 400KB Data types Scalar String Number Binary Boolean Null Document List Map Set String Set Number Set Binary Set To improve performance we can use DAX (mili seconds to micro seconds) Use partition keys of high cardinality, so large number of distinct values for each item DAX Stands for DynamoDB Accelerator Caching mechanism for DynamoDB DynamoDB Stream Raise event on Create , Update , Delete Can be used to trigger events on DB Changes Transaction All or nothing of operations Global Table Replicated in multiple region Needs to enable DynamoDB Stream Active Active Replication Changes in any region, impact all other regions Security Available in VPC Endpoints Fully controlled by IAM Encryption At rest by KMS In flight by SSL / TLS","title":"DynamoDB"},{"location":"Notes/Serverless/02 DynamoDB_deprecated/#provisioned-throughput","text":"2 types of Provision Throughput RCU Read Capacity Unit Each RCU can handle one of the followings 1 Strongly Consistent read 4KB/s 2 Eventually Consistent read 4KB/s WCU Write Capacity Unit 1 write 1KB/s Can be used on-demand throughput (price is 2.5X more) Throughput can be exceeded using Burst Credit If Burst Credit is empty, it throws ProvisionThroughputException In case of ProvisionThroughputException , it is recommended to retry Can be used DynamoDB Auto Scaling No need to provision throughput Comparatively expensive","title":"Provisioned Throughput"},{"location":"Notes/Serverless/03 API Gateway_deprecated/","text":"API Gateway Support web-socket protocol Handle API versioning Multiple Environment Security (Authentication, Authorization) Using API keys, handle request throattling Swagger / Open API to import Definition Transform and validate the Request and Response Generate SDK and API Specification Cache API response Integration Lambda Invoke Lambda function Expose REST API backed by Lambda HTTP Endpoint AWS Service Expose any AWS Service as API Gateway Endpoint Types 3 types of API Gateway Endpoints Edge Optimized This is default behavior API is only one region But to improve latency, request is routed through Cloudfront Edge Locations Regional API is in the one region With combination of Cloudfront We can get Edge Optimized behavior In this case, we have more control over Caching Strategies Distribution Private Use inside the VPC as VPC Endpoint Resource policy is used to define access Security IAM When users/roles is within AWS Account Handle Authentication and Authorization Leverage Sig v4 It's the IAM credentials in the HTTP Header Custom Authorizer or Lambda Authorizer When users are from 3rd party Lambda Authorized can be cached CUP or Cognito User Pool When user pools are manages by Facebook, Google login No need to write custom code Only provide Authentication Authorization must be provided from the backend code Access of developer and users can be separated using IAM Permission Developer can manage and deploy API User can call API SSL/TLS though AWS Certificate Manager is free for API Gateway","title":"03 API Gateway deprecated"},{"location":"Notes/Serverless/03 API Gateway_deprecated/#api-gateway","text":"Support web-socket protocol Handle API versioning Multiple Environment Security (Authentication, Authorization) Using API keys, handle request throattling Swagger / Open API to import Definition Transform and validate the Request and Response Generate SDK and API Specification Cache API response","title":"API Gateway"},{"location":"Notes/Serverless/03 API Gateway_deprecated/#integration","text":"Lambda Invoke Lambda function Expose REST API backed by Lambda HTTP Endpoint AWS Service Expose any AWS Service as API Gateway","title":"Integration"},{"location":"Notes/Serverless/03 API Gateway_deprecated/#endpoint-types","text":"3 types of API Gateway Endpoints Edge Optimized This is default behavior API is only one region But to improve latency, request is routed through Cloudfront Edge Locations Regional API is in the one region With combination of Cloudfront We can get Edge Optimized behavior In this case, we have more control over Caching Strategies Distribution Private Use inside the VPC as VPC Endpoint Resource policy is used to define access","title":"Endpoint Types"},{"location":"Notes/Serverless/03 API Gateway_deprecated/#security","text":"IAM When users/roles is within AWS Account Handle Authentication and Authorization Leverage Sig v4 It's the IAM credentials in the HTTP Header Custom Authorizer or Lambda Authorizer When users are from 3rd party Lambda Authorized can be cached CUP or Cognito User Pool When user pools are manages by Facebook, Google login No need to write custom code Only provide Authentication Authorization must be provided from the backend code Access of developer and users can be separated using IAM Permission Developer can manage and deploy API User can call API SSL/TLS though AWS Certificate Manager is free for API Gateway","title":"Security"},{"location":"Notes/Serverless/04 AWS Cognito/","text":"AWS Cognito 3 tools to discuss Cognito User Pools Users are stored as username and password in cognito Sign In functionality Integrate with api gateway Cognito Identity Pools Users included 3rd party provides, like SAML, facebook, google, etc Provide AWS Credentials to access AWS Resource directly Integrate with CUP as Identity Provider Cognito Sync Sync data to multiple device Deprecated, using AppSync instead User pools are list of user with credentials Identity pools are users from 3rd party Cognito User Pool and Identity Pool are independent with each other Has Guest User facility, so users can access limited resource without authentication Cognito Supports OIDC (Open ID Connect) SAML based identity providers Social identity providers To get data insight of the cognito, used the Cognito Streams To put data in redshift, we can make use of both the new kinesis stream or the cognito stream CUP Serverless database of users for mobile Verification through email/phone/MFA Can enable Federated Identity (Google, Facebook, SAML) -> Becomes Identity Pools from User Pools Verify user credentials and pass JWT Can be integrated with API Gateway for authentication Federated Identity Pools Provide direct access to AWS Resource from client side Steps Identity Provider generate token for valid user(could be CUP ) Federated Identity Verify the token Using STS generate temporary credentials for the APP App can use these credentials and access AWS Resource Developer Authenticated Identities AWS Cognito provides Developer Authenticated Identity It works along with 3rd party identity provider like facebook, google etc With Developer Authenticated Identity AWS can sync the user resource of a particular users resource Can be used to sync between end user device and backend Steps To Use CUP Create CUP In API Gateway, create authorized for cognito user pool id Send token in the header to authorize the request UI Cognito provide a built in login and sign up page We can customize the built in pages We can change the logo by going through the cognito app settings Compromised Credentials Amazon Cognito can determine whether the password has been compromised. We can set the settings block use from the advanced section. Also we can determine the actions like sign in , sign up and password change .","title":"04 AWS Cognito"},{"location":"Notes/Serverless/04 AWS Cognito/#aws-cognito","text":"3 tools to discuss Cognito User Pools Users are stored as username and password in cognito Sign In functionality Integrate with api gateway Cognito Identity Pools Users included 3rd party provides, like SAML, facebook, google, etc Provide AWS Credentials to access AWS Resource directly Integrate with CUP as Identity Provider Cognito Sync Sync data to multiple device Deprecated, using AppSync instead User pools are list of user with credentials Identity pools are users from 3rd party Cognito User Pool and Identity Pool are independent with each other Has Guest User facility, so users can access limited resource without authentication Cognito Supports OIDC (Open ID Connect) SAML based identity providers Social identity providers To get data insight of the cognito, used the Cognito Streams To put data in redshift, we can make use of both the new kinesis stream or the cognito stream","title":"AWS Cognito"},{"location":"Notes/Serverless/04 AWS Cognito/#cup","text":"Serverless database of users for mobile Verification through email/phone/MFA Can enable Federated Identity (Google, Facebook, SAML) -> Becomes Identity Pools from User Pools Verify user credentials and pass JWT Can be integrated with API Gateway for authentication","title":"CUP"},{"location":"Notes/Serverless/04 AWS Cognito/#federated-identity-pools","text":"Provide direct access to AWS Resource from client side Steps Identity Provider generate token for valid user(could be CUP ) Federated Identity Verify the token Using STS generate temporary credentials for the APP App can use these credentials and access AWS Resource","title":"Federated Identity Pools"},{"location":"Notes/Serverless/04 AWS Cognito/#developer-authenticated-identities","text":"AWS Cognito provides Developer Authenticated Identity It works along with 3rd party identity provider like facebook, google etc With Developer Authenticated Identity AWS can sync the user resource of a particular users resource Can be used to sync between end user device and backend","title":"Developer Authenticated Identities"},{"location":"Notes/Serverless/04 AWS Cognito/#steps-to-use-cup","text":"Create CUP In API Gateway, create authorized for cognito user pool id Send token in the header to authorize the request","title":"Steps To Use CUP"},{"location":"Notes/Serverless/04 AWS Cognito/#ui","text":"Cognito provide a built in login and sign up page We can customize the built in pages We can change the logo by going through the cognito app settings","title":"UI"},{"location":"Notes/Serverless/04 AWS Cognito/#compromised-credentials","text":"Amazon Cognito can determine whether the password has been compromised. We can set the settings block use from the advanced section. Also we can determine the actions like sign in , sign up and password change .","title":"Compromised Credentials"},{"location":"Notes/Serverless/05 SAM_legacy/","text":"SAM Serverless Application Model Framework for developing and deploying serverless applications All config is in the YAML formal Lambda function DynamoDB table API Gateway Cognito User Pool SAM help to run followings locally API Gateway DynamoDB Table SAM help to integrate Code Deploy , so Lambda Functions can deploy easily Allow test the Lambda function locally Can invoke function and events locally SAM Templates can be used to test the app through before deploy Using SAM Built In Code Deploy , application can be deployed to the cloud","title":"05 SAM legacy"},{"location":"Notes/Serverless/05 SAM_legacy/#sam","text":"Serverless Application Model Framework for developing and deploying serverless applications All config is in the YAML formal Lambda function DynamoDB table API Gateway Cognito User Pool SAM help to run followings locally API Gateway DynamoDB Table SAM help to integrate Code Deploy , so Lambda Functions can deploy easily Allow test the Lambda function locally Can invoke function and events locally SAM Templates can be used to test the app through before deploy","title":"SAM"},{"location":"Notes/Serverless/05 SAM_legacy/#using-sam-built-in-code-deploy-application-can-be-deployed-to-the-cloud","text":"","title":"Using SAM Built In Code Deploy, application can be deployed to the cloud"},{"location":"Notes/Serverless/01 Lambda/01 Lambda Overview/","text":"Lambda Virtual functions No needs to manage servers Run on-demand Automatic scaling Ram uses can be up to 3GB Increasing ram, improves the performance of CPU and Network Almost all programming language are supported by lambda Using Custom Runtime API it can support other languages Docker is not supported . Docker can run in the followings ECS Fargate Cloudwatch Event EventBridge can be used to run CRON job function that is in the Lambda Cloudwatch can be used to debug the code Runtime Lambda has native support of the following runtimes, Node.js Python Ruby Java Go .NET We can provide our own custom runtime by Include runtime in function deployment package named bootstrap These runtime should be resided in new lambda layer Lambda Limits Execution Memory allocation: 128MB to 3008MB (64MB increments) Max execution time: 15 minutes Env variable size: 4 KB Disk Capacity (/temp): 512MB Concurrent execution: 1000 (can be increased) When we reserve we have to consider 100 for there functions, so usable is 900 Deployment Compressed deployment size: 50MB Uncompressed deployment size: 250MB Lambda@Edge Required when Deployed a CDN using Cloudfront Want to run Lambda Function alongside Make 4 types Requests Between User and Cloudfront Viewer Request Viewer Response Between Cloudfront and Origin Origin Request Origin Response Use-cases Web additional security Dynamic app at the Edge SEO Intelligent routing across Origin and Data Center Bot Mitigation at EDGE A/B Testing User Authentication and Authorization User Prioritization User tracking and analytics Error Types During Deployment InvalidParameterValueException : Invalid request parameters. can be permission error. CodeStorageExceededException : Exceed the total code size (compressed 50MB, un-compressed 250MB) ResourceConflictException : Already a function exists ServiceException : Internal server error Limits Concurrency : Calculated by, concurrency = ( number of invocation per second * number of seconds per invocation took ). BY default lambda has 500 to 3000 concurrency vary from region. With burst capacity, we can exceed it another 500 concurrency. For more concurrency, need to make a request to increase the concurrency to aws. Concurrency limit is calculated by whole account. If the account has limit of 1000, aws will reserve 100 and other 900 can be used. We can distribute all these 900 Gotcha Environment Variables : Regular application environment variables Stage Variables : Related to API Gateway, can be dev , prod , v1 , v2 etc. Also these stage variables acn be mapped with the alias of lambda function Layers : A zip archive, contains runtime or library. Aliases : Pointer to specific lambda version","title":"01 Lambda Overview"},{"location":"Notes/Serverless/01 Lambda/01 Lambda Overview/#lambda","text":"Virtual functions No needs to manage servers Run on-demand Automatic scaling Ram uses can be up to 3GB Increasing ram, improves the performance of CPU and Network Almost all programming language are supported by lambda Using Custom Runtime API it can support other languages Docker is not supported . Docker can run in the followings ECS Fargate Cloudwatch Event EventBridge can be used to run CRON job function that is in the Lambda Cloudwatch can be used to debug the code","title":"Lambda"},{"location":"Notes/Serverless/01 Lambda/01 Lambda Overview/#runtime","text":"Lambda has native support of the following runtimes, Node.js Python Ruby Java Go .NET We can provide our own custom runtime by Include runtime in function deployment package named bootstrap These runtime should be resided in new lambda layer","title":"Runtime"},{"location":"Notes/Serverless/01 Lambda/01 Lambda Overview/#lambda-limits","text":"Execution Memory allocation: 128MB to 3008MB (64MB increments) Max execution time: 15 minutes Env variable size: 4 KB Disk Capacity (/temp): 512MB Concurrent execution: 1000 (can be increased) When we reserve we have to consider 100 for there functions, so usable is 900 Deployment Compressed deployment size: 50MB Uncompressed deployment size: 250MB","title":"Lambda Limits"},{"location":"Notes/Serverless/01 Lambda/01 Lambda Overview/#lambdaedge","text":"Required when Deployed a CDN using Cloudfront Want to run Lambda Function alongside Make 4 types Requests Between User and Cloudfront Viewer Request Viewer Response Between Cloudfront and Origin Origin Request Origin Response Use-cases Web additional security Dynamic app at the Edge SEO Intelligent routing across Origin and Data Center Bot Mitigation at EDGE A/B Testing User Authentication and Authorization User Prioritization User tracking and analytics","title":"Lambda@Edge"},{"location":"Notes/Serverless/01 Lambda/01 Lambda Overview/#error-types-during-deployment","text":"InvalidParameterValueException : Invalid request parameters. can be permission error. CodeStorageExceededException : Exceed the total code size (compressed 50MB, un-compressed 250MB) ResourceConflictException : Already a function exists ServiceException : Internal server error","title":"Error Types During Deployment"},{"location":"Notes/Serverless/01 Lambda/01 Lambda Overview/#limits","text":"Concurrency : Calculated by, concurrency = ( number of invocation per second * number of seconds per invocation took ). BY default lambda has 500 to 3000 concurrency vary from region. With burst capacity, we can exceed it another 500 concurrency. For more concurrency, need to make a request to increase the concurrency to aws. Concurrency limit is calculated by whole account. If the account has limit of 1000, aws will reserve 100 and other 900 can be used. We can distribute all these 900","title":"Limits"},{"location":"Notes/Serverless/01 Lambda/01 Lambda Overview/#gotcha","text":"Environment Variables : Regular application environment variables Stage Variables : Related to API Gateway, can be dev , prod , v1 , v2 etc. Also these stage variables acn be mapped with the alias of lambda function Layers : A zip archive, contains runtime or library. Aliases : Pointer to specific lambda version","title":"Gotcha"},{"location":"Notes/Serverless/01 Lambda/02 Lambda Attach With VPC/","text":"Attach Wit VPC By default, the lambda function can not access the VPC resources. It is deployed outside of the VPC. Even we attach it along with the VPC, it will get the access to the VPC resources, but by default can not access the internet. Attach ENI with VPC Attach NAT with VPC Enable all outbound SG Access Resources To get access to our VPC resources it uses ENI and access the VPC resources through it. Lambda function should attach to An ENI in the VPC subnets, with with our function will be attached An Create ENI To VPC execution role We will need a A role will be Access Internet When it EC2 Instance , we can deploy it in a public subnet and it will get a public IP along with the internet access. But when it comes to Lambda function, even though we connect it with the public subnet ENI, it will not get a public IP and also not get the internet access. To make sure a lambda function can access internet from the VPC we have to Linked it with private/public (private is the best practice) subnet ENI Attach NAT Gateway , so this will link with IGW and ensure the internet connection Make sure lambda associated SG allow the internet connection After getting public internet access, we can access other AWS resource using public internet or using the AWS Private net (VPC Endpoints)","title":"02 Lambda Attach With VPC"},{"location":"Notes/Serverless/01 Lambda/02 Lambda Attach With VPC/#attach-wit-vpc","text":"By default, the lambda function can not access the VPC resources. It is deployed outside of the VPC. Even we attach it along with the VPC, it will get the access to the VPC resources, but by default can not access the internet. Attach ENI with VPC Attach NAT with VPC Enable all outbound SG Access Resources To get access to our VPC resources it uses ENI and access the VPC resources through it. Lambda function should attach to An ENI in the VPC subnets, with with our function will be attached An Create ENI To VPC execution role We will need a A role will be Access Internet When it EC2 Instance , we can deploy it in a public subnet and it will get a public IP along with the internet access. But when it comes to Lambda function, even though we connect it with the public subnet ENI, it will not get a public IP and also not get the internet access. To make sure a lambda function can access internet from the VPC we have to Linked it with private/public (private is the best practice) subnet ENI Attach NAT Gateway , so this will link with IGW and ensure the internet connection Make sure lambda associated SG allow the internet connection After getting public internet access, we can access other AWS resource using public internet or using the AWS Private net (VPC Endpoints)","title":"Attach Wit VPC"},{"location":"Notes/Serverless/01 Lambda/03 Lambda Versioning and Alias/","text":"Versioning Everytime we make a update to the code/configuration of a lambda function, a new version of the lambda function created. When we invoke the function, it usually invoke the latest version of the function, Although we can invoke any previous/specific version of the lambda function. Each version of the lambda function will have their own AWS ARN. Alias When we want to point a specific version of a lambda fuction, we can make use of the Alias. Alias can be dev, test, prod, rc etc. By these alias, we can also implement the blue green deployment. Like an alias can send traffic to both the test and prod with specific percentage or weight. Gotcha Alias can only point to different version/versions of lambda function. It can not point to another alias. $latest version is mutable, it always point to the updated version of the lambda function. Other versions are immutable. If we update any code/configuration, the lambda function versions will be updated.","title":"03 Lambda Versioning and Alias"},{"location":"Notes/Serverless/01 Lambda/03 Lambda Versioning and Alias/#versioning","text":"Everytime we make a update to the code/configuration of a lambda function, a new version of the lambda function created. When we invoke the function, it usually invoke the latest version of the function, Although we can invoke any previous/specific version of the lambda function. Each version of the lambda function will have their own AWS ARN.","title":"Versioning"},{"location":"Notes/Serverless/01 Lambda/03 Lambda Versioning and Alias/#alias","text":"When we want to point a specific version of a lambda fuction, we can make use of the Alias. Alias can be dev, test, prod, rc etc. By these alias, we can also implement the blue green deployment. Like an alias can send traffic to both the test and prod with specific percentage or weight.","title":"Alias"},{"location":"Notes/Serverless/01 Lambda/03 Lambda Versioning and Alias/#gotcha","text":"Alias can only point to different version/versions of lambda function. It can not point to another alias. $latest version is mutable, it always point to the updated version of the lambda function. Other versions are immutable. If we update any code/configuration, the lambda function versions will be updated.","title":"Gotcha"},{"location":"Notes/Serverless/01 Lambda/04 Lambda Performance/","text":"Performance Timeout Default time out for a lambda function is 5 minutes Can be extend the time to 15 minutes Memory Lambda memory start from 128MB and then 64MB of increments. CPU The more memory we add to the lambda function, the more CPU we will get. At 1792MB ram, we will get a full CPU. After memory of 1792, we will have more than 1 CPU, and to utilize the CPU, we should make use of the multi-threading. For CPU bound application, we need to increase RAM. Execution Context Execution context is the application run time used to maintain for a small amount of time. If we define some task outside of the function, like db connection, sdk client initialization etc, these will be persisted and can be used in other functions. We can store some files in /tmp directory and these will be available in the execution context. Maximum size of the /tmp directory will be 512 MB. Lambda Layers A zip archive, used to store additional code, packages, runtime other than the original function. Can have maximum 5 layers. Along with the functions, total size of functions is limited (50MB compressed or 250MB un-compressed). We can use layers to reduce the each time deployment time.","title":"04 Lambda Performance"},{"location":"Notes/Serverless/01 Lambda/04 Lambda Performance/#performance","text":"Timeout Default time out for a lambda function is 5 minutes Can be extend the time to 15 minutes Memory Lambda memory start from 128MB and then 64MB of increments. CPU The more memory we add to the lambda function, the more CPU we will get. At 1792MB ram, we will get a full CPU. After memory of 1792, we will have more than 1 CPU, and to utilize the CPU, we should make use of the multi-threading. For CPU bound application, we need to increase RAM. Execution Context Execution context is the application run time used to maintain for a small amount of time. If we define some task outside of the function, like db connection, sdk client initialization etc, these will be persisted and can be used in other functions. We can store some files in /tmp directory and these will be available in the execution context. Maximum size of the /tmp directory will be 512 MB.","title":"Performance"},{"location":"Notes/Serverless/01 Lambda/04 Lambda Performance/#lambda-layers","text":"A zip archive, used to store additional code, packages, runtime other than the original function. Can have maximum 5 layers. Along with the functions, total size of functions is limited (50MB compressed or 250MB un-compressed). We can use layers to reduce the each time deployment time.","title":"Lambda Layers"},{"location":"Notes/Serverless/01 Lambda/05 Lambda Integrations/","text":"Lambda Integrations Lambda support 3 types invocation: RequestResponse (Default Type) : Invoke the function synchronously. Keeps the connection open until the function return response or times out. The API response contains response, data and status Event : Invoke the function asynchronously. Returns a response code not the response data. We can configure a DLQ (Dead letter queue) to investigate the failure DryRun : Validate parameter and verify roles and permissions We can integrate lambda with API gateway by proxy and non-proxy, Proxy : Redirect all the params, payloads, headers etc directly to the Lambda from api gateway. non-proxy : We need to map all the params, payloads etc.","title":"05 Lambda Integrations"},{"location":"Notes/Serverless/01 Lambda/05 Lambda Integrations/#lambda-integrations","text":"Lambda support 3 types invocation: RequestResponse (Default Type) : Invoke the function synchronously. Keeps the connection open until the function return response or times out. The API response contains response, data and status Event : Invoke the function asynchronously. Returns a response code not the response data. We can configure a DLQ (Dead letter queue) to investigate the failure DryRun : Validate parameter and verify roles and permissions We can integrate lambda with API gateway by proxy and non-proxy, Proxy : Redirect all the params, payloads, headers etc directly to the Lambda from api gateway. non-proxy : We need to map all the params, payloads etc.","title":"Lambda Integrations"},{"location":"Notes/Serverless/01 Lambda/06 Lambda Best Practices/","text":"Lambda Best Practices Separate the lambda handler from the core logic (Handler should be used as the entry point) Use execution context to improve performance Use env variables to pass operational parameters to lambda function Avoid using the recursive code Control the dependencies in functions deployment packages (Can be put them in the lambda layer) Minimize deployment packages Example is, selectively include the libraries that are required For error like ServiceException use retry with ErrorEquals string to match the error","title":"06 Lambda Best Practices"},{"location":"Notes/Serverless/01 Lambda/06 Lambda Best Practices/#lambda-best-practices","text":"Separate the lambda handler from the core logic (Handler should be used as the entry point) Use execution context to improve performance Use env variables to pass operational parameters to lambda function Avoid using the recursive code Control the dependencies in functions deployment packages (Can be put them in the lambda layer) Minimize deployment packages Example is, selectively include the libraries that are required For error like ServiceException use retry with ErrorEquals string to match the error","title":"Lambda Best Practices"},{"location":"Notes/Serverless/01 Lambda/07 Lambda Deployments/","text":"Lambda Deployments With Cloudformation For inline lambda code, in cloudformation, we can add these codes under the ZipFile property of AWS::Lambda::Function .","title":"07 Lambda Deployments"},{"location":"Notes/Serverless/01 Lambda/07 Lambda Deployments/#lambda-deployments","text":"With Cloudformation For inline lambda code, in cloudformation, we can add these codes under the ZipFile property of AWS::Lambda::Function .","title":"Lambda Deployments"},{"location":"Notes/Serverless/02 DynamoDB/01 DynamoDB Overview/","text":"DynamoDB Managed database, replicated across 3 AZ NoSQL database Enable event driven programming using DynamoDB Streams In DynamoDB database is already created, only needs to create Table Each Table should have a primary key (Must be decided while creating the table) Can have infinite number of rows Max size of item can be 400KB Data types Scalar String Number Binary Boolean Null Document List Map Set String Set Number Set Binary Set To improve performance we can use DAX (milliseconds to micro seconds) Use partition keys of high cardinality, so large number of distinct values for each item DAX Stands for DynamoDB Accelerator Caching mechanism for DynamoDB DynamoDB Stream Raise event on Create , Update , Delete Can be used to trigger events on DB Changes Data persisted in the stream of 24 hours Transaction All or nothing of operations Global Table Replicated in multiple region Needs to enable DynamoDB Stream Active Active Replication Changes in any region, impact all other regions Security Available in VPC Endpoints Fully controlled by IAM Encryption Server side encryption is enabled by default in all DynamoDB table At rest by KMS AWS Managed Key for DynamoDB Customer Managed Key In flight by SSL / TLS Provisioned Throughput 2 types of Provision Throughput RCU Read Capacity Unit Each RCU can handle one of the followings 1 Strongly Consistent read 4KB/s 2 Eventually Consistent read 4KB/s WCU Write Capacity Unit 1 write 1KB/s Can be used on-demand throughput (price is 2.5X more) Throughput can be exceeded using Burst Credit If Burst Credit is empty, it throws ProvisionThroughputException In case of ProvisionThroughputException , it is recommended to retry Can be used DynamoDB Auto Scaling No need to provision throughput Comparatively expensive Authorizer Lambda support two types of authorizer Token Based : A bearer token is passed as the caller identity Request Parameter Based : Caller identity is passed to context as combinations of headers, query string parameters etc Access","title":"01 DynamoDB Overview"},{"location":"Notes/Serverless/02 DynamoDB/01 DynamoDB Overview/#dynamodb","text":"Managed database, replicated across 3 AZ NoSQL database Enable event driven programming using DynamoDB Streams In DynamoDB database is already created, only needs to create Table Each Table should have a primary key (Must be decided while creating the table) Can have infinite number of rows Max size of item can be 400KB Data types Scalar String Number Binary Boolean Null Document List Map Set String Set Number Set Binary Set To improve performance we can use DAX (milliseconds to micro seconds) Use partition keys of high cardinality, so large number of distinct values for each item DAX Stands for DynamoDB Accelerator Caching mechanism for DynamoDB DynamoDB Stream Raise event on Create , Update , Delete Can be used to trigger events on DB Changes Data persisted in the stream of 24 hours Transaction All or nothing of operations Global Table Replicated in multiple region Needs to enable DynamoDB Stream Active Active Replication Changes in any region, impact all other regions Security Available in VPC Endpoints Fully controlled by IAM Encryption Server side encryption is enabled by default in all DynamoDB table At rest by KMS AWS Managed Key for DynamoDB Customer Managed Key In flight by SSL / TLS","title":"DynamoDB"},{"location":"Notes/Serverless/02 DynamoDB/01 DynamoDB Overview/#provisioned-throughput","text":"2 types of Provision Throughput RCU Read Capacity Unit Each RCU can handle one of the followings 1 Strongly Consistent read 4KB/s 2 Eventually Consistent read 4KB/s WCU Write Capacity Unit 1 write 1KB/s Can be used on-demand throughput (price is 2.5X more) Throughput can be exceeded using Burst Credit If Burst Credit is empty, it throws ProvisionThroughputException In case of ProvisionThroughputException , it is recommended to retry Can be used DynamoDB Auto Scaling No need to provision throughput Comparatively expensive","title":"Provisioned Throughput"},{"location":"Notes/Serverless/02 DynamoDB/01 DynamoDB Overview/#authorizer","text":"Lambda support two types of authorizer Token Based : A bearer token is passed as the caller identity Request Parameter Based : Caller identity is passed to context as combinations of headers, query string parameters etc","title":"Authorizer"},{"location":"Notes/Serverless/02 DynamoDB/01 DynamoDB Overview/#access","text":"","title":"Access"},{"location":"Notes/Serverless/02 DynamoDB/02 DynamoDB Caching/","text":"Caching Caching can be used in the dynamodb by DAX When dax are used with strongly consistent query, result is not cached. DAX Caching Write Through : While write data to dynamoDB, data will be persisted in DAX Write Around : While write data to dynamoDB, data will not be persisted in DAX","title":"02 DynamoDB Caching"},{"location":"Notes/Serverless/02 DynamoDB/02 DynamoDB Caching/#caching","text":"Caching can be used in the dynamodb by DAX When dax are used with strongly consistent query, result is not cached. DAX Caching Write Through : While write data to dynamoDB, data will be persisted in DAX Write Around : While write data to dynamoDB, data will not be persisted in DAX","title":"Caching"},{"location":"Notes/Serverless/02 DynamoDB/03 DynamoDB TTL/","text":"TTL use to expire and delete items from the table. The time of ttl should be defined in each row. The deletion usually takes place the time we defined. But some cases it might take 2 days to delete. When the deletion happened, the GSI/LSI related tot he item will also be deleted. This ttl delete operation does not require any RCU or WCU. With the DynamoDB streams, we can control/recover the deleted items.","title":"03 DynamoDB TTL"},{"location":"Notes/Serverless/02 DynamoDB/03 DynamoDB TTL/#ttl","text":"use to expire and delete items from the table. The time of ttl should be defined in each row. The deletion usually takes place the time we defined. But some cases it might take 2 days to delete. When the deletion happened, the GSI/LSI related tot he item will also be deleted. This ttl delete operation does not require any RCU or WCU. With the DynamoDB streams, we can control/recover the deleted items.","title":"TTL"},{"location":"Notes/Serverless/02 DynamoDB/04 DynamoDB Streams/","text":"DynamoDB Streams We can track the changes of dynamodb using the dynamoDB streams. On create, delete or updated, when the dynamodb streams is being enabled we can run a lambda function and do necessary tasks. When we use global dynamodb table and data is being replicated between multiple region, the dynamodb streams must be enabled. StreamViewType determines what information are writtne to the stream to this table. StreamViewTypes are KEYS_ONLY : Only pass the modified key items NEW_IMAGE : Pass the new value OLD_IMAGE : Pass the existing value NEW_AND_OLD_IMAGES : Pass both the new and old values As stream source, if we make use of lambda, the lambda needs permission to read these stream. In this case, we can make use of a managed policy, AWSLambdaDynamoDBExecutionRole . dynamoDB Streams can only be integrated with lambda functions, can not integrated with sns or sqs etc.","title":"04 DynamoDB Streams"},{"location":"Notes/Serverless/02 DynamoDB/04 DynamoDB Streams/#dynamodb-streams","text":"We can track the changes of dynamodb using the dynamoDB streams. On create, delete or updated, when the dynamodb streams is being enabled we can run a lambda function and do necessary tasks. When we use global dynamodb table and data is being replicated between multiple region, the dynamodb streams must be enabled. StreamViewType determines what information are writtne to the stream to this table. StreamViewTypes are KEYS_ONLY : Only pass the modified key items NEW_IMAGE : Pass the new value OLD_IMAGE : Pass the existing value NEW_AND_OLD_IMAGES : Pass both the new and old values As stream source, if we make use of lambda, the lambda needs permission to read these stream. In this case, we can make use of a managed policy, AWSLambdaDynamoDBExecutionRole . dynamoDB Streams can only be integrated with lambda functions, can not integrated with sns or sqs etc.","title":"DynamoDB Streams"},{"location":"Notes/Serverless/02 DynamoDB/05 DynamoDB Performance/","text":"DynamoDB Performance Caching using DAX Avoid SCAN and try to use query instead Reduce the page size in both cases SCAN and query Supports BatchGetItem and BatchWriteItem to make use of bulk read and write operations Atomic counter Update a numeric attribute with UpdateItem operation. No matter what UpdateItem operation does, like update any properties, the atomic counter update the value If UpdateItem operation fails and try again, the atomic counter increase twice DB Locking Optimistic Locking : Before update/delete, make sure the item is same as the client Example : Let's say, someone is updating the price of a product. First get the product { id: 1, price: 10 } . Now the updated price should be 15 . While do the update, the dynamoDB client find out, in the db, the price is already updated 20 by someone. In this case the price will not be updated. This is optimistic locking. Pessimistic Locking : Pessimistic Locking lock the document in the DB so no one can modify while it's being operated by an user. Useful for prevent overwriting but interrupt the other users operations. Overly Optimistic Locking : Used in a system where there i only one user/one operation at a time. Conditional Writing : DynamoDB allows conditional writing, where write operation is being happened if certain defined condition matched. Rate limit parallel scan can reduce the cost Getting Write Consumed Capacity When there is an operation of Put , Update , Delete in the DynamoDB, it consumes some of the write capacity. We can get these consumed capacity as the response of the operation. To get the consumed write capacity, after an operation is being completed, we can make use of the following properties, TOTAL : Returns total number of consumed capacity used by the operation INDEXES : Returns total consumed capacity along with the sub total of affected secondary indexes. NONE : This is the default behavior and no consumed write capacity is returns Avoid Hot Partitioning A hot partition occurs when an individual partition is being accessed more frequently from application downstream. To reduce the hot partitioning, we can, Increase Read and Write Capacity of Table Implement Error Retries and Exponential Back-off : SDK already has this feature installed Distribute Read And Write Operations Evenly Across the Table Implement Caching by DAX Caching with DAX is expensive and Distribute Read And Write Operations Evenly Across the Table needs to refactor the whole application. In this case, the easiest and quickest solution is Increase Read and Write Capacity of Table and Implement Error Retries and Exponential Back-off . Transactions DynamoDB provide all-or-nothing types operation with TransactWriteItems and TransactGetItems . With TransactWriteItems , we can do a batch of 25 items within same account, region and multiple tables. We can perform the followings, Put Update Delete ConditionCheck Best Practices Keep number of indexes minimum Avoid indexing for heavy write activity","title":"05 DynamoDB Performance"},{"location":"Notes/Serverless/02 DynamoDB/05 DynamoDB Performance/#dynamodb-performance","text":"Caching using DAX Avoid SCAN and try to use query instead Reduce the page size in both cases SCAN and query Supports BatchGetItem and BatchWriteItem to make use of bulk read and write operations Atomic counter Update a numeric attribute with UpdateItem operation. No matter what UpdateItem operation does, like update any properties, the atomic counter update the value If UpdateItem operation fails and try again, the atomic counter increase twice DB Locking Optimistic Locking : Before update/delete, make sure the item is same as the client Example : Let's say, someone is updating the price of a product. First get the product { id: 1, price: 10 } . Now the updated price should be 15 . While do the update, the dynamoDB client find out, in the db, the price is already updated 20 by someone. In this case the price will not be updated. This is optimistic locking. Pessimistic Locking : Pessimistic Locking lock the document in the DB so no one can modify while it's being operated by an user. Useful for prevent overwriting but interrupt the other users operations. Overly Optimistic Locking : Used in a system where there i only one user/one operation at a time. Conditional Writing : DynamoDB allows conditional writing, where write operation is being happened if certain defined condition matched. Rate limit parallel scan can reduce the cost","title":"DynamoDB Performance"},{"location":"Notes/Serverless/02 DynamoDB/05 DynamoDB Performance/#getting-write-consumed-capacity","text":"When there is an operation of Put , Update , Delete in the DynamoDB, it consumes some of the write capacity. We can get these consumed capacity as the response of the operation. To get the consumed write capacity, after an operation is being completed, we can make use of the following properties, TOTAL : Returns total number of consumed capacity used by the operation INDEXES : Returns total consumed capacity along with the sub total of affected secondary indexes. NONE : This is the default behavior and no consumed write capacity is returns","title":"Getting Write Consumed Capacity"},{"location":"Notes/Serverless/02 DynamoDB/05 DynamoDB Performance/#avoid-hot-partitioning","text":"A hot partition occurs when an individual partition is being accessed more frequently from application downstream. To reduce the hot partitioning, we can, Increase Read and Write Capacity of Table Implement Error Retries and Exponential Back-off : SDK already has this feature installed Distribute Read And Write Operations Evenly Across the Table Implement Caching by DAX Caching with DAX is expensive and Distribute Read And Write Operations Evenly Across the Table needs to refactor the whole application. In this case, the easiest and quickest solution is Increase Read and Write Capacity of Table and Implement Error Retries and Exponential Back-off .","title":"Avoid Hot Partitioning"},{"location":"Notes/Serverless/02 DynamoDB/05 DynamoDB Performance/#transactions","text":"DynamoDB provide all-or-nothing types operation with TransactWriteItems and TransactGetItems . With TransactWriteItems , we can do a batch of 25 items within same account, region and multiple tables. We can perform the followings, Put Update Delete ConditionCheck","title":"Transactions"},{"location":"Notes/Serverless/02 DynamoDB/05 DynamoDB Performance/#best-practices","text":"Keep number of indexes minimum Avoid indexing for heavy write activity","title":"Best Practices"},{"location":"Notes/Serverless/02 DynamoDB/06 DynamoDB Indexing/","text":"DynamoDB Indexing A DynamoDB table can have two types of primary key, Simple primary key: Consist of a HASH key or partition key Composite primary key: Consist of two keys, HASH or partition key and RANGE or sort key Partition keys used to determine, in which partition the item should be placed. LSI & GSI DynamoDB requires us to specify the primary key for all operations and hence provide faster performance. We can scan on tables without primary key and it is not efficient and should avoid as much as possible. LSI : Stands for local secondary index. We must create the LSI whenever we create table. After a table being created, we can not update the LSI. We can create up to 5 local secondary indexes. LSI supports both the eventual consistency and strongly consistency. GSI : Stands for global secondary index. A global secondary index can be created with entirely new partition key and sort key. Global secondary index took a new partition and hence it needs a completely new throughput capacity for the RCU and WCU. GSI can be created anytime, not bound to the table creation time like the LSI. GSI can only perform eventual consistency Sparse index is useful for querying over a small subsection of a table.","title":"06 DynamoDB Indexing"},{"location":"Notes/Serverless/02 DynamoDB/06 DynamoDB Indexing/#dynamodb-indexing","text":"A DynamoDB table can have two types of primary key, Simple primary key: Consist of a HASH key or partition key Composite primary key: Consist of two keys, HASH or partition key and RANGE or sort key Partition keys used to determine, in which partition the item should be placed.","title":"DynamoDB Indexing"},{"location":"Notes/Serverless/02 DynamoDB/06 DynamoDB Indexing/#lsi-gsi","text":"DynamoDB requires us to specify the primary key for all operations and hence provide faster performance. We can scan on tables without primary key and it is not efficient and should avoid as much as possible. LSI : Stands for local secondary index. We must create the LSI whenever we create table. After a table being created, we can not update the LSI. We can create up to 5 local secondary indexes. LSI supports both the eventual consistency and strongly consistency. GSI : Stands for global secondary index. A global secondary index can be created with entirely new partition key and sort key. Global secondary index took a new partition and hence it needs a completely new throughput capacity for the RCU and WCU. GSI can be created anytime, not bound to the table creation time like the LSI. GSI can only perform eventual consistency Sparse index is useful for querying over a small subsection of a table.","title":"LSI &amp; GSI"},{"location":"Notes/Serverless/02 DynamoDB/07 DynamoDB Permissions/","text":"DynamoDB Permissions With dynamoDB We can grant permission of the table, but only for certain items. These permissioned rows should be included in the generated IAM policy For example, user will only get access of data of that user id We can also permissioned for certain rows For example, user will get location information only closer to him Implementation Condition must be applied with condition in IAM policy Track key should be the primary key Set the primary key using the dynamodb:LeadingKeys","title":"07 DynamoDB Permissions"},{"location":"Notes/Serverless/02 DynamoDB/07 DynamoDB Permissions/#dynamodb-permissions","text":"With dynamoDB We can grant permission of the table, but only for certain items. These permissioned rows should be included in the generated IAM policy For example, user will only get access of data of that user id We can also permissioned for certain rows For example, user will get location information only closer to him Implementation Condition must be applied with condition in IAM policy Track key should be the primary key Set the primary key using the dynamodb:LeadingKeys","title":"DynamoDB Permissions"},{"location":"Notes/Serverless/02 DynamoDB/10. DynamoDB Misc/","text":"Misc Invalidate API caching by sending Cache-Control: max-age=0 header To ensure, only authorized client can invalidate the cache, we can make use of Require Authentication Using dynamoDB TTL, items can be deleted automatically","title":"10. DynamoDB Misc"},{"location":"Notes/Serverless/02 DynamoDB/10. DynamoDB Misc/#misc","text":"Invalidate API caching by sending Cache-Control: max-age=0 header To ensure, only authorized client can invalidate the cache, we can make use of Require Authentication Using dynamoDB TTL, items can be deleted automatically","title":"Misc"},{"location":"Notes/Serverless/03 API Gateway/01 API Gateway Overview/","text":"Overview Support web-socket protocol Handle API versioning Multiple Environment Security (Authentication, Authorization) Using API keys, handle request throttling Swagger / Open API to import Definition Transform and validate the Request and Response Generate SDK and API Specification Cache API response Can be orchestrate multiple web app and micro services Can set different usage plan for different users for different level of access Can set different quota and throttling to different level of access To pass a stage variable, use $stagevariables.<variableName> concept Integration Lambda Invoke Lambda function Expose REST API backed by Lambda HTTP Endpoint AWS Service Expose any AWS Service as API Gateway Endpoint Types 3 types of API Gateway Endpoints Edge Optimized This is default behavior API is only one region But to improve latency, request is routed through Cloudfront Edge Locations Regional API is in the one region With combination of Cloudfront We can get Edge Optimized behavior In this case, we have more control over Caching Strategies Distribution Private Use inside the VPC as VPC Endpoint Resource policy is used to define access Security IAM When users/roles is within AWS Account Handle Authentication and Authorization Leverage Sig v4 It's the IAM credentials in the HTTP Header Custom Authorizer or Lambda Authorizer When users are from 3rd party Lambda Authorized can be cached CUP or Cognito User Pool When user pools are manages by Facebook, Google login No need to write custom code Only provide Authentication Authorization must be provided from the backend code Access of developer and users can be separated using IAM Permission Developer can manage and deploy API User can call API SSL/TLS though AWS Certificate Manager is free for API Gateway Invoke_Async is deprecated invocation type. Only the invoke is being used.","title":"01 API Gateway Overview"},{"location":"Notes/Serverless/03 API Gateway/01 API Gateway Overview/#overview","text":"Support web-socket protocol Handle API versioning Multiple Environment Security (Authentication, Authorization) Using API keys, handle request throttling Swagger / Open API to import Definition Transform and validate the Request and Response Generate SDK and API Specification Cache API response Can be orchestrate multiple web app and micro services Can set different usage plan for different users for different level of access Can set different quota and throttling to different level of access To pass a stage variable, use $stagevariables.<variableName> concept","title":"Overview"},{"location":"Notes/Serverless/03 API Gateway/01 API Gateway Overview/#integration","text":"Lambda Invoke Lambda function Expose REST API backed by Lambda HTTP Endpoint AWS Service Expose any AWS Service as API Gateway","title":"Integration"},{"location":"Notes/Serverless/03 API Gateway/01 API Gateway Overview/#endpoint-types","text":"3 types of API Gateway Endpoints Edge Optimized This is default behavior API is only one region But to improve latency, request is routed through Cloudfront Edge Locations Regional API is in the one region With combination of Cloudfront We can get Edge Optimized behavior In this case, we have more control over Caching Strategies Distribution Private Use inside the VPC as VPC Endpoint Resource policy is used to define access","title":"Endpoint Types"},{"location":"Notes/Serverless/03 API Gateway/01 API Gateway Overview/#security","text":"IAM When users/roles is within AWS Account Handle Authentication and Authorization Leverage Sig v4 It's the IAM credentials in the HTTP Header Custom Authorizer or Lambda Authorizer When users are from 3rd party Lambda Authorized can be cached CUP or Cognito User Pool When user pools are manages by Facebook, Google login No need to write custom code Only provide Authentication Authorization must be provided from the backend code Access of developer and users can be separated using IAM Permission Developer can manage and deploy API User can call API SSL/TLS though AWS Certificate Manager is free for API Gateway Invoke_Async is deprecated invocation type. Only the invoke is being used.","title":"Security"},{"location":"Notes/Serverless/03 API Gateway/02 API Gateway Response Code/","text":"Response Code For 4xx , client errors 400 Bad Request 403 Access Denied and WAF filtered 429 Quota Exceeded, throttle For 5xx , server errors 502 Gateway exception Heavy load Out of order invocations 503 Service unavailable Incompatible output from the lambda 504 Integration Failure Integration Timeout (Request time after 29 seconds)","title":"02 API Gateway Response Code"},{"location":"Notes/Serverless/03 API Gateway/02 API Gateway Response Code/#response-code","text":"For 4xx , client errors 400 Bad Request 403 Access Denied and WAF filtered 429 Quota Exceeded, throttle For 5xx , server errors 502 Gateway exception Heavy load Out of order invocations 503 Service unavailable Incompatible output from the lambda 504 Integration Failure Integration Timeout (Request time after 29 seconds)","title":"Response Code"},{"location":"Notes/Serverless/03 API Gateway/03 API Gateway Deployment/","text":"Deployment To make differenent types of deployment for the API gateway, we can make use of stage variable. For example we need dev, prod, rc, releases. We first define the stage variables and attach them to fthe HTTP endpoints. To make the API's live, we have to deploy them. Deployment is going under stages. we can put name and configurations on these stages. After each deployments, the previous deployment history is being persisted and we can role back if something goes wrong with latest deployment. We can define the stages by stage varibales. Stage variables can be used for different types of configuations like, lambda alias, http endpoints, parameter mapping templates etc. In lambda variable, we can access the stage variables using the context parameters. Integration Between Stage Variables and Lambda Alias In api gateway we can create stage variable with the same name of the lambda alias, we can map them. Staging For each stage, we can define separate settings for caching, logging, x-ray-tracing etc. Canary Deployment This is a blue/green deployment of API Gateway + Lambda . With canary deployment, initially we send small amount of traffic to the new function and do separate monitoring and logging for that function. If everything goes fine, we then fully move to the new function,","title":"03 API Gateway Deployment"},{"location":"Notes/Serverless/03 API Gateway/03 API Gateway Deployment/#deployment","text":"To make differenent types of deployment for the API gateway, we can make use of stage variable. For example we need dev, prod, rc, releases. We first define the stage variables and attach them to fthe HTTP endpoints. To make the API's live, we have to deploy them. Deployment is going under stages. we can put name and configurations on these stages. After each deployments, the previous deployment history is being persisted and we can role back if something goes wrong with latest deployment. We can define the stages by stage varibales. Stage variables can be used for different types of configuations like, lambda alias, http endpoints, parameter mapping templates etc. In lambda variable, we can access the stage variables using the context parameters.","title":"Deployment"},{"location":"Notes/Serverless/03 API Gateway/03 API Gateway Deployment/#integration-between-stage-variables-and-lambda-alias","text":"In api gateway we can create stage variable with the same name of the lambda alias, we can map them.","title":"Integration Between Stage Variables and Lambda Alias"},{"location":"Notes/Serverless/03 API Gateway/03 API Gateway Deployment/#staging","text":"For each stage, we can define separate settings for caching, logging, x-ray-tracing etc.","title":"Staging"},{"location":"Notes/Serverless/03 API Gateway/03 API Gateway Deployment/#canary-deployment","text":"This is a blue/green deployment of API Gateway + Lambda . With canary deployment, initially we send small amount of traffic to the new function and do separate monitoring and logging for that function. If everything goes fine, we then fully move to the new function,","title":"Canary Deployment"},{"location":"Notes/Serverless/03 API Gateway/04 API Gateway Caching/","text":"Caching Caching can be used to reduce the number of api calls to the backend. With caching enabled, we response will provided from the cahce if available. We can enable cache on the stage level as well as method level. To invalidate a cache, we need to make use of http header Cache-Control: max-age=0 . The client should have IAM permission to invalidate the cache. In case, we do not require any IAM permission for cache invalidation, then every client can invalide the cache, which is not expected.","title":"04 API Gateway Caching"},{"location":"Notes/Serverless/03 API Gateway/04 API Gateway Caching/#caching","text":"Caching can be used to reduce the number of api calls to the backend. With caching enabled, we response will provided from the cahce if available. We can enable cache on the stage level as well as method level. To invalidate a cache, we need to make use of http header Cache-Control: max-age=0 . The client should have IAM permission to invalidate the cache. In case, we do not require any IAM permission for cache invalidation, then every client can invalide the cache, which is not expected.","title":"Caching"},{"location":"Notes/Serverless/03 API Gateway/05 API Gateway Monitoring/","text":"Monitoring We can integrate the cloudwatch logs at the stage levels and ApI basis. With X-Ray, the full picture of the api-gateway and lambda can be displayed Cloudwatch Metrics: CacheHitCount the more the better caching mechanism, (Number of request served from the cache) CacheMissCount the less the better caching mechanism (Number of requests served from the backend) Count Total number of API request at a certain time frame IntegrationLatency Time frame of sending request to lambda and getting response from lambda Latency Time to receive request in API Gateway and sending response from API Gateway","title":"05 API Gateway Monitoring"},{"location":"Notes/Serverless/03 API Gateway/05 API Gateway Monitoring/#monitoring","text":"We can integrate the cloudwatch logs at the stage levels and ApI basis. With X-Ray, the full picture of the api-gateway and lambda can be displayed Cloudwatch Metrics: CacheHitCount the more the better caching mechanism, (Number of request served from the cache) CacheMissCount the less the better caching mechanism (Number of requests served from the backend) Count Total number of API request at a certain time frame IntegrationLatency Time frame of sending request to lambda and getting response from lambda Latency Time to receive request in API Gateway and sending response from API Gateway","title":"Monitoring"},{"location":"Notes/Serverless/03 API Gateway/07 API Gateway Integration Types/","text":"Integration Types There are 4 types of integration type, can be used to integrate other services with API Gateway. HTTP or HTTP Custom Integration A custom http endpoint, need to set up mapping HTTP_PROXY Use for http endpoint, no needs for any mapping AWS (For Serverless/Lambda) AWS_PROXY (For Serverless/Lambda) With proxy integration, there is no need for any mapping and it is easier, it just redirect all the requests, parameters and payloads to the defined endpoint.","title":"07 API Gateway Integration Types"},{"location":"Notes/Serverless/03 API Gateway/07 API Gateway Integration Types/#integration-types","text":"There are 4 types of integration type, can be used to integrate other services with API Gateway. HTTP or HTTP Custom Integration A custom http endpoint, need to set up mapping HTTP_PROXY Use for http endpoint, no needs for any mapping AWS (For Serverless/Lambda) AWS_PROXY (For Serverless/Lambda) With proxy integration, there is no need for any mapping and it is easier, it just redirect all the requests, parameters and payloads to the defined endpoint.","title":"Integration Types"},{"location":"Notes/Serverless/04 SAM/01 SAM Overview/","text":"OVERVIEW Serverless Application Model Framework for developing and deploying serverless applications All config is in the YAML format Lambda function DynamoDB table API Gateway Cognito User Pool SAM help to run followings locally API Gateway DynamoDB Table SAM help to integrate Code Deploy , so Lambda Functions can deploy easily Allow test the Lambda function locally Can invoke function and events locally SAM Templates can be used to test the app through before deploy Using SAM Built In Code Deploy , application can be deployed to the cloud","title":"01 SAM Overview"},{"location":"Notes/Serverless/04 SAM/01 SAM Overview/#overview","text":"Serverless Application Model Framework for developing and deploying serverless applications All config is in the YAML format Lambda function DynamoDB table API Gateway Cognito User Pool SAM help to run followings locally API Gateway DynamoDB Table SAM help to integrate Code Deploy , so Lambda Functions can deploy easily Allow test the Lambda function locally Can invoke function and events locally SAM Templates can be used to test the app through before deploy Using SAM Built In Code Deploy , application can be deployed to the cloud","title":"OVERVIEW"},{"location":"Notes/Serverless/04 SAM/02 SAM Properties/","text":"SAM Properties Transform : Use to specify the SAM version Mappings : A literal for mapping keys and associated values, can be later used in parameters, tables lookup or condition Parameters : Used to reger values, can be passed during the runtime of the template Format Version : The cloudformation template version, to which the SAM template will be transformed SAM Resources Types AWS::Serverless::Application : Use to define nested application AWS::Serverless::Function : Use to define Lambda function AWS::Serverless::LayerVersion : Use to define Lambda layer version (Runtime/library) AWS::Serverless::API : Use to define Api Gateway","title":"02 SAM Properties"},{"location":"Notes/Serverless/04 SAM/02 SAM Properties/#sam-properties","text":"Transform : Use to specify the SAM version Mappings : A literal for mapping keys and associated values, can be later used in parameters, tables lookup or condition Parameters : Used to reger values, can be passed during the runtime of the template Format Version : The cloudformation template version, to which the SAM template will be transformed","title":"SAM Properties"},{"location":"Notes/Serverless/04 SAM/02 SAM Properties/#sam-resources-types","text":"AWS::Serverless::Application : Use to define nested application AWS::Serverless::Function : Use to define Lambda function AWS::Serverless::LayerVersion : Use to define Lambda layer version (Runtime/library) AWS::Serverless::API : Use to define Api Gateway","title":"SAM Resources Types"},{"location":"Notes/Serverless/04 SAM/03 SAM Deployments/","text":"SAM Deployments SAM usage Cloudformation as the underlying deployment mechanism. After develop and test locally, we can deploy code using sam package (Includes in the sam package ) sam deploy Deployment commands, sam init : Initialize app using sam template sam build : Created a deployment ready build directory sam package : create zip, upload s3, create packaged template sam deploy : deploy sam publish : take packaged template and publish SAM has the built in codeDeploy for helping the safe lambda deployments. Canary : A certain amount of defined traffic will go to new function for a defined time. If everything goes fine, all traffic will shift to the new function Linear : Traffic is shifted in equal increments with an equal number of minutes between each increment. All-at-once : All traffic will be shifted to the new lambda functions. Canary Example Canary10Percent10Minutes : Move 10 percent if the traffic immediately to the new version. After 10 minutes, all traffic is shifted to the new version. CodeDeployDefault.LambdaCanary10Percent5Minutes : Move 10 percent traffic to new lambda function and then all the traffic will be shifted to the new lambda function. Linear Example CodeDeployDefault.LambdaLinear10PercentEvery1Minute Will redirect 10 percent of traffics each minutes and in 10 minutes all traffics will be shifted. CodeDeployDefault.LambdaLinear10PercentEvery2Minutes Will redirect 10 percent of traffics each 2 minutes and in 20 minutes all traffics will be shifted.","title":"03 SAM Deployments"},{"location":"Notes/Serverless/04 SAM/03 SAM Deployments/#sam-deployments","text":"SAM usage Cloudformation as the underlying deployment mechanism. After develop and test locally, we can deploy code using sam package (Includes in the sam package ) sam deploy Deployment commands, sam init : Initialize app using sam template sam build : Created a deployment ready build directory sam package : create zip, upload s3, create packaged template sam deploy : deploy sam publish : take packaged template and publish SAM has the built in codeDeploy for helping the safe lambda deployments. Canary : A certain amount of defined traffic will go to new function for a defined time. If everything goes fine, all traffic will shift to the new function Linear : Traffic is shifted in equal increments with an equal number of minutes between each increment. All-at-once : All traffic will be shifted to the new lambda functions. Canary Example Canary10Percent10Minutes : Move 10 percent if the traffic immediately to the new version. After 10 minutes, all traffic is shifted to the new version. CodeDeployDefault.LambdaCanary10Percent5Minutes : Move 10 percent traffic to new lambda function and then all the traffic will be shifted to the new lambda function. Linear Example CodeDeployDefault.LambdaLinear10PercentEvery1Minute Will redirect 10 percent of traffics each minutes and in 10 minutes all traffics will be shifted. CodeDeployDefault.LambdaLinear10PercentEvery2Minutes Will redirect 10 percent of traffics each 2 minutes and in 20 minutes all traffics will be shifted.","title":"SAM Deployments"},{"location":"Notes/Serverless/04 SAM/04 SAM vs Serverless Application Framework/","text":"SAM vs Serverless Application Framework Both are open source, popular framework for building serverless solution. SAM is native solution for AWS whether serverless is developed and maintained by 3rd party provider SAM is only applicable for AWS, whereas serverless can be used to AZURE or GCP also","title":"04 SAM vs Serverless Application Framework"},{"location":"Notes/Serverless/04 SAM/04 SAM vs Serverless Application Framework/#sam-vs-serverless-application-framework","text":"Both are open source, popular framework for building serverless solution. SAM is native solution for AWS whether serverless is developed and maintained by 3rd party provider SAM is only applicable for AWS, whereas serverless can be used to AZURE or GCP also","title":"SAM vs Serverless Application Framework"},{"location":"Notes/Solution Architect Misc/01 Caching Strategies/","text":"Caching Strategies Cloudfront Edge level caching Closest to the user API Gateway Region level caching DB Caching ( Application Level) Redis ( Elastic Cache ) Memcached ( Elastic Cache ) DAX","title":"01 Caching Strategies"},{"location":"Notes/Solution Architect Misc/01 Caching Strategies/#caching-strategies","text":"Cloudfront Edge level caching Closest to the user API Gateway Region level caching DB Caching ( Application Level) Redis ( Elastic Cache ) Memcached ( Elastic Cache ) DAX","title":"Caching Strategies"},{"location":"Notes/Solution Architect Misc/02 Blocking IP Address/","text":"Blocking IP Address For EC2 instance NACL in Subnet Level Security Group in Instance Level Run Firewall Software in EC2 instance This includes CPU cost When using a ALB NACL in Subnet Level Security Group in ALB Security Group does not work in the instance level, it only shows the ALB IP WAF in ALB Can be used for IP Filtering When using a NLB NACL in Subnet Level Security Group in Instance Level Run Firewall Software in EC2 instance This includes CPU cost When using a Cloudfront NACL and Security Group does not work here NACL and Security Group only sees the Cloudfront IP and ALB IP respectively Cloudfront Geo Restriction WAF in Cloudfront Can be used for IP Filtering","title":"02 Blocking IP Address"},{"location":"Notes/Solution Architect Misc/02 Blocking IP Address/#blocking-ip-address","text":"For EC2 instance NACL in Subnet Level Security Group in Instance Level Run Firewall Software in EC2 instance This includes CPU cost When using a ALB NACL in Subnet Level Security Group in ALB Security Group does not work in the instance level, it only shows the ALB IP WAF in ALB Can be used for IP Filtering When using a NLB NACL in Subnet Level Security Group in Instance Level Run Firewall Software in EC2 instance This includes CPU cost When using a Cloudfront NACL and Security Group does not work here NACL and Security Group only sees the Cloudfront IP and ALB IP respectively Cloudfront Geo Restriction WAF in Cloudfront Can be used for IP Filtering","title":"Blocking IP Address"},{"location":"Notes/Solution Architect Misc/03 HPC/","text":"HPC HPC stands for High Performance Computing Data Transfer and Management AWS Direct Connect (Move data GB/s) Snowball / Snow mobile (Move PB Scale data) AWS Data Sync (Move Data from on premise to s3, EFS, FSx) Compute And Networking EC2 instance, that are CPU Optimized and GPU Optimized Spot Instance for huge EC2 Fleet with very low cost EC2 Placement Group for good network Performance Using ENA , EFA Storage EBS Instance Store S3 EFS FSx for Luster","title":"03 HPC"},{"location":"Notes/Solution Architect Misc/03 HPC/#hpc","text":"HPC stands for High Performance Computing Data Transfer and Management AWS Direct Connect (Move data GB/s) Snowball / Snow mobile (Move PB Scale data) AWS Data Sync (Move Data from on premise to s3, EFS, FSx) Compute And Networking EC2 instance, that are CPU Optimized and GPU Optimized Spot Instance for huge EC2 Fleet with very low cost EC2 Placement Group for good network Performance Using ENA , EFA Storage EBS Instance Store S3 EFS FSx for Luster","title":"HPC"},{"location":"Notes/Solution Architect Misc/04 ENA & EFA/","text":"ENA And EFA ENA stands for Elastic Network Adapter EFA stands for Elastic Fabric Adapter EFA is an improved version of ENA and only works with linux Used for Enhanced Networking i.e. SRIOV Higher Bandwidth Higher PPS (Packet Per Second) High performance inter instance performance EFA special use case Leverage MPI , i.e. Message Passing Interface This bypass underlying Linus OS for low latency","title":"04 ENA & EFA"},{"location":"Notes/Solution Architect Misc/04 ENA & EFA/#ena-and-efa","text":"ENA stands for Elastic Network Adapter EFA stands for Elastic Fabric Adapter EFA is an improved version of ENA and only works with linux Used for Enhanced Networking i.e. SRIOV Higher Bandwidth Higher PPS (Packet Per Second) High performance inter instance performance EFA special use case Leverage MPI , i.e. Message Passing Interface This bypass underlying Linus OS for low latency","title":"ENA And EFA"},{"location":"Notes/Solution Architect Misc/05 Automation & Orchestration/","text":"Automation & Orchestration AWS Batch Schedule Jobs AWS Parallel Cluster Open source cluster management tools","title":"05 Automation & Orchestration"},{"location":"Notes/Solution Architect Misc/05 Automation & Orchestration/#automation-orchestration","text":"AWS Batch Schedule Jobs AWS Parallel Cluster Open source cluster management tools","title":"Automation &amp; Orchestration"},{"location":"Notes/Solution Architect Misc/06 Highly Available EC2 Instance/","text":"Highly Available EC2 Instance // TODO","title":"06 Highly Available EC2 Instance"},{"location":"Notes/Solution Architect Misc/06 Highly Available EC2 Instance/#highly-available-ec2-instance","text":"// TODO","title":"Highly Available EC2 Instance"},{"location":"Notes/Solution Architect Misc/07 Bastion Host HA/","text":"Bastion Host HA // TODO","title":"07 Bastion Host HA"},{"location":"Notes/Solution Architect Misc/07 Bastion Host HA/#bastion-host-ha","text":"// TODO","title":"Bastion Host HA"},{"location":"Notes/Study Point/01 README/","text":"ALB is not a regional service NLB does not support custom security policy consists of Protocols and ciphers Terminate TLS connection in NLB Require one certificate for each TLS connection to encrypt traffic between client and NLB AWS Certificate manager can be used, since it it automatically renew on expiry CLB (Classic load balancer) Supports the ASG AWS well architect framework includes Monitoring and alerts using Cloudtrail and Cloudwatch Spread EC2 Instances across multiple AZ When web distribution falls under PCI distribution Enable Cloudfront Logs Capture request, sent to the Cloudfront API AWS Public Dataset like satellite imagery, geospatial, genomic is free, need no charge RDP aka Remote Desktop Protocol use port 3389","title":"01 README"},{"location":"Notes/VPC/01 CIDR/","text":"CIDR Classless Inter-Domain Routing Used in Security groups AWS networking Help define IP Range Has two components Base IP (XX.XX.XX.XX) IP that contains in the range Subnet Mask (/XX) Determine how many bits can be change in the IP Subnet Mask Calculate IP Ranges Formula (2^(32-subnetMaskNumber)) Some Examples /32 allows 1 IP = 2^0 /31 allows 2 IP = 2^1 /30 allows 4 IP = 2^2 /29 allows 8 IP = 2^3 /28 allows 16 IP = 2^4 /27 allows 32 IP = 2^5 /26 allows 64 IP = 2^6 /25 allows 128 IP = 2^7 /24 allows 256 IP = 2^8 /16 allows 65,536 IP = 2^16 /0 allows ALL IP = 2^32 Quick Memo /32 - no IP number can change /24 - last IP number can change /16 - last IP two numbers can change /8 - last IP three numbers can change /0 - All IP numbers can change Exercise Find range 192.168.0.0/24 Last IP number can change Range be 192.168.0.0 to 192.168.0.255 ( 256 IP) Alternate 2^(32-24) = 256 IP Find range 192.168.0.0/16 Last 2 IP number can change Range be 192.168.0.0 to 192.168.255.255 ( 65,536 IP) Alternate 2^(32-16) = 65,536 IP Find range 134.56.78.123/32 No number can change Range be 134.56.78.123 to 134.56.78.123 ( 1 IP) Alternate 2^(32-32) = 1 IP Find range 0.0.0.0/32 All number can change Range be 0.0.0.0 to 255.255.255.255 ( ALL IP) Alternate 2^(32-0) = 2^32 IP = ALL IP Private IP Big Network Range 10.0.0.0 - 10.255.255.255 and CIDR ( 10.0.0.0/8 ) AWS Default Range 172.16.0.0 - 172.31.255.255 and CIDR ( 172.16.0.0/12 ) Home Network Range 192.168.0.0 - 192.168.255.255 and CIDR ( 192.168.0.0/16 ) Public IP Any IP but Private IP","title":"01 CIDR"},{"location":"Notes/VPC/01 CIDR/#cidr","text":"Classless Inter-Domain Routing Used in Security groups AWS networking Help define IP Range Has two components Base IP (XX.XX.XX.XX) IP that contains in the range Subnet Mask (/XX) Determine how many bits can be change in the IP","title":"CIDR"},{"location":"Notes/VPC/01 CIDR/#subnet-mask","text":"Calculate IP Ranges Formula (2^(32-subnetMaskNumber)) Some Examples /32 allows 1 IP = 2^0 /31 allows 2 IP = 2^1 /30 allows 4 IP = 2^2 /29 allows 8 IP = 2^3 /28 allows 16 IP = 2^4 /27 allows 32 IP = 2^5 /26 allows 64 IP = 2^6 /25 allows 128 IP = 2^7 /24 allows 256 IP = 2^8 /16 allows 65,536 IP = 2^16 /0 allows ALL IP = 2^32 Quick Memo /32 - no IP number can change /24 - last IP number can change /16 - last IP two numbers can change /8 - last IP three numbers can change /0 - All IP numbers can change Exercise Find range 192.168.0.0/24 Last IP number can change Range be 192.168.0.0 to 192.168.0.255 ( 256 IP) Alternate 2^(32-24) = 256 IP Find range 192.168.0.0/16 Last 2 IP number can change Range be 192.168.0.0 to 192.168.255.255 ( 65,536 IP) Alternate 2^(32-16) = 65,536 IP Find range 134.56.78.123/32 No number can change Range be 134.56.78.123 to 134.56.78.123 ( 1 IP) Alternate 2^(32-32) = 1 IP Find range 0.0.0.0/32 All number can change Range be 0.0.0.0 to 255.255.255.255 ( ALL IP) Alternate 2^(32-0) = 2^32 IP = ALL IP","title":"Subnet Mask"},{"location":"Notes/VPC/01 CIDR/#private-ip","text":"Big Network Range 10.0.0.0 - 10.255.255.255 and CIDR ( 10.0.0.0/8 ) AWS Default Range 172.16.0.0 - 172.31.255.255 and CIDR ( 172.16.0.0/12 ) Home Network Range 192.168.0.0 - 192.168.255.255 and CIDR ( 192.168.0.0/16 )","title":"Private IP"},{"location":"Notes/VPC/01 CIDR/#public-ip","text":"Any IP but Private IP","title":"Public IP"},{"location":"Notes/VPC/02 Default VPC/","text":"Default VPC Properties All new account have default VPC If not specified, new instances are launched in Default VPC Has internet connectivity Each new instance is configured as Have public IP Have Public and Private DNS name Default VPC has following components Subnets Route Tables Internet Gateway Network ACL","title":"02 Default VPC"},{"location":"Notes/VPC/02 Default VPC/#default-vpc","text":"Properties All new account have default VPC If not specified, new instances are launched in Default VPC Has internet connectivity Each new instance is configured as Have public IP Have Public and Private DNS name Default VPC has following components Subnets Route Tables Internet Gateway Network ACL","title":"Default VPC"},{"location":"Notes/VPC/04 VPC Overview/","text":"VPC Virtual Private Cloud Max 5 VPC in per region (Soft Limit) Max 5 CIDR Each CIDR Min Size /28 = 16 IP address Maz Size /16 = 65536 IP Address VPC is Private , so Private IP Ranges are Big Network Range 10.0.0.0 - 10.255.255.255 and CIDR ( 10.0.0.0/8 ) AWS Default Range 172.16.0.0 - 172.31.255.255 and CIDR ( 172.16.0.0/12 ) Home Network Range 192.168.0.0 - 192.168.255.255 and CIDR ( 192.168.0.0/16 ) CIDR should not overlap other networks AZ name varies from Account to Account us-east-1a can be different AZ to another account AWS randomize the AZ name When IP Address is not available in defined CIDR range, can be added Secondary CIDR","title":"04 VPC Overview"},{"location":"Notes/VPC/04 VPC Overview/#vpc","text":"Virtual Private Cloud Max 5 VPC in per region (Soft Limit) Max 5 CIDR Each CIDR Min Size /28 = 16 IP address Maz Size /16 = 65536 IP Address VPC is Private , so Private IP Ranges are Big Network Range 10.0.0.0 - 10.255.255.255 and CIDR ( 10.0.0.0/8 ) AWS Default Range 172.16.0.0 - 172.31.255.255 and CIDR ( 172.16.0.0/12 ) Home Network Range 192.168.0.0 - 192.168.255.255 and CIDR ( 192.168.0.0/16 ) CIDR should not overlap other networks AZ name varies from Account to Account us-east-1a can be different AZ to another account AWS randomize the AZ name When IP Address is not available in defined CIDR range, can be added Secondary CIDR","title":"VPC"},{"location":"Notes/VPC/05 Subnet Overview/","text":"Subnet Subnets are tied to specific AZ Multiple Subnets can be provisioned in one AZ One Subnet can not be provisioned in multi-AZ Two types of Subnets Public Subnet use to put Load balancer Private Subnet use to put Applications and DB Servers Every time create a Subnet , loose 5 IP address. First 4 and the last one First one for Network Address Second one for VPC Router , reserved by AWS Third one for AWS Provided DNS , reserved by AWS Fourth one for future use, reserved by AWS Last one for Broadcast Address , although AWS does not support Broadcast Exercise Can /27 handle 29 IP Address ? It has 2^(32-27) = 32 IP Address Since 5 IP address is not usable, we can use 32-5 = 27 IP Address So /27 can not handle more than 27 IP Address To handle 29 IP Address we might need at least /26 i.e. 2^(32-26) = 64 IP Address In this case we can use 64-5 = 59 IP Address , that matches the requirements We can enable Auto Assign Public IPv4 Address features, so any instance being created within the subnet, will have a public IP. In Custom VPC , the feature Auto Assign Public IPv4 Address is disabled by default, whereas in Default VPC it is enabled by default. To ensure the Public Subnet can access the Private Subnet , need to ensure, the SG have the required rules defined to allow traffic Since internet and intranet is corresponding to separate AZ, need separate AZ for them","title":"05 Subnet Overview"},{"location":"Notes/VPC/05 Subnet Overview/#subnet","text":"Subnets are tied to specific AZ Multiple Subnets can be provisioned in one AZ One Subnet can not be provisioned in multi-AZ Two types of Subnets Public Subnet use to put Load balancer Private Subnet use to put Applications and DB Servers Every time create a Subnet , loose 5 IP address. First 4 and the last one First one for Network Address Second one for VPC Router , reserved by AWS Third one for AWS Provided DNS , reserved by AWS Fourth one for future use, reserved by AWS Last one for Broadcast Address , although AWS does not support Broadcast Exercise Can /27 handle 29 IP Address ? It has 2^(32-27) = 32 IP Address Since 5 IP address is not usable, we can use 32-5 = 27 IP Address So /27 can not handle more than 27 IP Address To handle 29 IP Address we might need at least /26 i.e. 2^(32-26) = 64 IP Address In this case we can use 64-5 = 59 IP Address , that matches the requirements We can enable Auto Assign Public IPv4 Address features, so any instance being created within the subnet, will have a public IP. In Custom VPC , the feature Auto Assign Public IPv4 Address is disabled by default, whereas in Default VPC it is enabled by default. To ensure the Public Subnet can access the Private Subnet , need to ensure, the SG have the required rules defined to allow traffic Since internet and intranet is corresponding to separate AZ, need separate AZ for them","title":"Subnet"},{"location":"Notes/VPC/06 Internet Gateway/","text":"Internet Gateway (IG) Use to provide Internet Connection to the VPC It is scalable, highly available and redundant One VPC can have one IG One IG can have one VPC IG work like NAT for instance, that has public IP Default VPC have an attached IG For Custom VPC need to create an IG and attach to the VPC","title":"06 Internet Gateway"},{"location":"Notes/VPC/06 Internet Gateway/#internet-gateway-ig","text":"Use to provide Internet Connection to the VPC It is scalable, highly available and redundant One VPC can have one IG One IG can have one VPC IG work like NAT for instance, that has public IP Default VPC have an attached IG For Custom VPC need to create an IG and attach to the VPC","title":"Internet Gateway (IG)"},{"location":"Notes/VPC/07 Route Table/","text":"Route Table By default there is a Main Route Table when VPC is created Every subnet is associated to the Main Route Table if it is not explicitly set up. By default all the VPC CIDR is routed to the local network The Public Subnet needs to add a route to the IG , if it is not communicating with VPC CIDR Route Table Target VPC Peering start with pcx- VPN Connection start with vgw- Direct Connect start with vgw- Secondary CIDR is local","title":"07 Route Table"},{"location":"Notes/VPC/07 Route Table/#route-table","text":"By default there is a Main Route Table when VPC is created Every subnet is associated to the Main Route Table if it is not explicitly set up. By default all the VPC CIDR is routed to the local network The Public Subnet needs to add a route to the IG , if it is not communicating with VPC CIDR Route Table Target VPC Peering start with pcx- VPN Connection start with vgw- Direct Connect start with vgw- Secondary CIDR is local","title":"Route Table"},{"location":"Notes/VPC/08 Internet Connection To Public Subnet/","text":"Internet Connection To Public Subnet Instance Make sure IG is attached to the VPC Make sure Public Subnet has a route towards IG The instance should have a public IP or elastic IP","title":"08 Internet Connection To Public Subnet"},{"location":"Notes/VPC/08 Internet Connection To Public Subnet/#internet-connection-to-public-subnet-instance","text":"Make sure IG is attached to the VPC Make sure Public Subnet has a route towards IG The instance should have a public IP or elastic IP","title":"Internet Connection To Public Subnet Instance"},{"location":"Notes/VPC/09 NAT Instance/","text":"NAT Instance Network Address Translation Allow Instances in the Private Subnet to access interner Must be launched in the Public Subnet Must be disabled Source and Destination check Need and Elastic IP (ENI) attached to the Nat Instance From the Private Route Table this IP be the target NAT Instance security rules: Allow HTTP from VPC CIDR Allow HTTPS from VPC CIDR Allow All ICMP - IPv4 from VPC CIDR for ping Private Route Table security rules Any connection outgoing to the internet (0.0.0.0/0) , be target to NAT Instance Cons of NAT Instance Not HA Not easy setup Elastic IP to make stable route Internet Traffic depends on EC2 performance ( Network Throughput )","title":"09 NAT Instance"},{"location":"Notes/VPC/09 NAT Instance/#nat-instance","text":"Network Address Translation Allow Instances in the Private Subnet to access interner Must be launched in the Public Subnet Must be disabled Source and Destination check Need and Elastic IP (ENI) attached to the Nat Instance From the Private Route Table this IP be the target NAT Instance security rules: Allow HTTP from VPC CIDR Allow HTTPS from VPC CIDR Allow All ICMP - IPv4 from VPC CIDR for ping Private Route Table security rules Any connection outgoing to the internet (0.0.0.0/0) , be target to NAT Instance Cons of NAT Instance Not HA Not easy setup Elastic IP to make stable route Internet Traffic depends on EC2 performance ( Network Throughput )","title":"NAT Instance"},{"location":"Notes/VPC/10 NAT Gateway/","text":"NAT Gateway Managed By AWS Pricing is based on hour and bandwidth Can not used by the instance of same Subnet (Only from other Subnets ) Require IG Things are Private Subnet -> NAT Gateway -> IG Scale from 5Gbps to 45Gbps No Security Group is required NAT Gateway is resilient withing a Single AZ . For fault-tolerance , require multiple NAT Gateway in Multiple AZ Private Route Table security rules Any connection outgoing to the internet (0.0.0.0/0) , be target to NAT Gateway In VPC Peering , Nat Gateway can not be shared. Need to use separate Nat Gateway","title":"10 NAT Gateway"},{"location":"Notes/VPC/10 NAT Gateway/#nat-gateway","text":"Managed By AWS Pricing is based on hour and bandwidth Can not used by the instance of same Subnet (Only from other Subnets ) Require IG Things are Private Subnet -> NAT Gateway -> IG Scale from 5Gbps to 45Gbps No Security Group is required NAT Gateway is resilient withing a Single AZ . For fault-tolerance , require multiple NAT Gateway in Multiple AZ Private Route Table security rules Any connection outgoing to the internet (0.0.0.0/0) , be target to NAT Gateway In VPC Peering , Nat Gateway can not be shared. Need to use separate Nat Gateway","title":"NAT Gateway"},{"location":"Notes/VPC/11 DNS Resolution/","text":"DNS Resolution in VPC Two settings enableDnsSupport It is DNS Resolution Setting Default is true Helps if DNS Resolution is supported for VPC If true , queries the AWS DNS Server at 169.254.169.253 enableDnsHostname It is DNS Hostname Setting Default value false in Custom VPC true in Default VPC If true , a public hostname assigned to the Instance If false , only IP assigned to the Instance","title":"11 DNS Resolution"},{"location":"Notes/VPC/11 DNS Resolution/#dns-resolution-in-vpc","text":"Two settings enableDnsSupport It is DNS Resolution Setting Default is true Helps if DNS Resolution is supported for VPC If true , queries the AWS DNS Server at 169.254.169.253 enableDnsHostname It is DNS Hostname Setting Default value false in Custom VPC true in Default VPC If true , a public hostname assigned to the Instance If false , only IP assigned to the Instance","title":"DNS Resolution in VPC"},{"location":"Notes/VPC/12 Route 53 Private Zone/","text":"Route 53 Private Zone Make sure following 2 VPC DNS Resolution is enabled enableDnsHostname is true enableDnsSupport is true Since it is Private Zone and accessible from VPC . we can create records that is not owned","title":"12 Route 53 Private Zone"},{"location":"Notes/VPC/12 Route 53 Private Zone/#route-53-private-zone","text":"Make sure following 2 VPC DNS Resolution is enabled enableDnsHostname is true enableDnsSupport is true Since it is Private Zone and accessible from VPC . we can create records that is not owned","title":"Route 53 Private Zone"},{"location":"Notes/VPC/13 NACL & SG/","text":"NACL & SG NACL stands for Network Access Control List SG stands for Security Group SG associated with Instance NACL associated with Subnet SG is stateful If Inbound Rule allowed IP / IP Ranges , Outbound Rule is automatically allowed If Outbound Rule allowed IP / IP Ranges , Inbound Rule is automatically a llowed NACL stateless i.e. both Inbound Rule and Outbound Rule is separately evaluated NACL is evaluated Lowest number has high preference If no rules found, it goes to * numbered rule Default NACL allow every traffic for both Inbound and Outbound Custom NACL block every traffic for both Inbound and Outbound Each Subnet goes under Default NACL if not explicitly associated Ephemeral PORT should be open for highly restricted NACL To block an IP Address use NACL","title":"13 NACL & SG"},{"location":"Notes/VPC/13 NACL & SG/#nacl-sg","text":"NACL stands for Network Access Control List SG stands for Security Group SG associated with Instance NACL associated with Subnet SG is stateful If Inbound Rule allowed IP / IP Ranges , Outbound Rule is automatically allowed If Outbound Rule allowed IP / IP Ranges , Inbound Rule is automatically a llowed NACL stateless i.e. both Inbound Rule and Outbound Rule is separately evaluated NACL is evaluated Lowest number has high preference If no rules found, it goes to * numbered rule Default NACL allow every traffic for both Inbound and Outbound Custom NACL block every traffic for both Inbound and Outbound Each Subnet goes under Default NACL if not explicitly associated Ephemeral PORT should be open for highly restricted NACL To block an IP Address use NACL","title":"NACL &amp; SG"},{"location":"Notes/VPC/14 VPC Peering/","text":"VPC Peering Connect two VPC using AWS network After VPC Peering communication between two VPC use AWS Network instead of Public Internet Two VPC can not be peered, if Have overlapping CIDR Any transitive peering with on-premise server Edge to Edge routing via a gateway Transitive peering A Peered Connection is to be created between two VPC Peered Connection can be established with an VPC in another Region and another Account ( inter-region , cross-account ) Can use Peered VPC SG reference Connection is not Transitive VPC A is peered to VPC B VPC B is peered to VPC C Does not imply VPC A is peered with VPC C Still we need to peer VPC A with VPC C explicitly Each Subnet Route Table of each peered VPC should be updated. Target of the Peered VPC CIDR should be the Peered Connection To establish VPC Peering Create VPC Peer Connection with own VPC and another VPC Accept the Peer Connection request Update Subnet Route Table for both VPC NAT Gateway can not be shared over VPC Peering","title":"14 VPC Peering"},{"location":"Notes/VPC/14 VPC Peering/#vpc-peering","text":"Connect two VPC using AWS network After VPC Peering communication between two VPC use AWS Network instead of Public Internet Two VPC can not be peered, if Have overlapping CIDR Any transitive peering with on-premise server Edge to Edge routing via a gateway Transitive peering A Peered Connection is to be created between two VPC Peered Connection can be established with an VPC in another Region and another Account ( inter-region , cross-account ) Can use Peered VPC SG reference Connection is not Transitive VPC A is peered to VPC B VPC B is peered to VPC C Does not imply VPC A is peered with VPC C Still we need to peer VPC A with VPC C explicitly Each Subnet Route Table of each peered VPC should be updated. Target of the Peered VPC CIDR should be the Peered Connection To establish VPC Peering Create VPC Peer Connection with own VPC and another VPC Accept the Peer Connection request Update Subnet Route Table for both VPC NAT Gateway can not be shared over VPC Peering","title":"VPC Peering"},{"location":"Notes/VPC/15 VPC Endpoints/","text":"VPC Endpoints Allow using AWS Service using Private Network from VPC No need of IG and NAT Gateway Two types of VPC Endpoints Gateway S3 and DynamoDB uses Gateway Provision ENI (i.e. Private IP Address ) as entry point Need SG Interface Provision Target and use Route Table Service, other than S3 and DynamoDB , uses Gateway To establish a VPC Endpoint Check DNS Resolution Check Route Table When there is VPC Endpoint and a Public Internet Connection , VPC Endpoint got priority. VPC Endpoint does not support cross region request From a single Route Table , can not have multiple VPC Endpoints of the same service. FYI, A service can have multiple endpoints To allow S3 to as VPC Endpoint Gateway , needs Endpoint Policy for trusted VPC","title":"15 VPC Endpoints"},{"location":"Notes/VPC/15 VPC Endpoints/#vpc-endpoints","text":"Allow using AWS Service using Private Network from VPC No need of IG and NAT Gateway Two types of VPC Endpoints Gateway S3 and DynamoDB uses Gateway Provision ENI (i.e. Private IP Address ) as entry point Need SG Interface Provision Target and use Route Table Service, other than S3 and DynamoDB , uses Gateway To establish a VPC Endpoint Check DNS Resolution Check Route Table When there is VPC Endpoint and a Public Internet Connection , VPC Endpoint got priority. VPC Endpoint does not support cross region request From a single Route Table , can not have multiple VPC Endpoints of the same service. FYI, A service can have multiple endpoints To allow S3 to as VPC Endpoint Gateway , needs Endpoint Policy for trusted VPC","title":"VPC Endpoints"},{"location":"Notes/VPC/16 Flow Logs/","text":"Flow Logs Capture IP Traffic and Network information Help monitor and troubleshoot connectivity issues Can be used for VPC level Subnet Level Elastic Network Interface level Store logs in S3 / Cloudwatch Logs Can query the logs using Athena in S3 logs Cloudwatch Log Insights in Cloudwatch Logs Can not enable Flow Logs of VPC that is Peered and belongs to Another Account After creating a flow log, configuration can not be changed For example, can not change IAM Roles Following IP Traffic does not monitor AWS DNS server Traffic of Windows Instance activating licence Traffic of Instance Metadata (169.254.169.254) DHCP traffic Reserved IP Address of Default VPC Router","title":"16 Flow Logs"},{"location":"Notes/VPC/16 Flow Logs/#flow-logs","text":"Capture IP Traffic and Network information Help monitor and troubleshoot connectivity issues Can be used for VPC level Subnet Level Elastic Network Interface level Store logs in S3 / Cloudwatch Logs Can query the logs using Athena in S3 logs Cloudwatch Log Insights in Cloudwatch Logs Can not enable Flow Logs of VPC that is Peered and belongs to Another Account After creating a flow log, configuration can not be changed For example, can not change IAM Roles Following IP Traffic does not monitor AWS DNS server Traffic of Windows Instance activating licence Traffic of Instance Metadata (169.254.169.254) DHCP traffic Reserved IP Address of Default VPC Router","title":"Flow Logs"},{"location":"Notes/VPC/17 Bastion Host/","text":"Bastion Host Use to SSH to the Private Instance Launched in the Public Subnet Security rules should be strict Only PORT 22 be opened for SSH Only My IP should allow Supports TCP","title":"17 Bastion Host"},{"location":"Notes/VPC/17 Bastion Host/#bastion-host","text":"Use to SSH to the Private Instance Launched in the Public Subnet Security rules should be strict Only PORT 22 be opened for SSH Only My IP should allow Supports TCP","title":"Bastion Host"},{"location":"Notes/VPC/18 Site To Site VPN/","text":"Site To Site VPN Connect Corporate Data Center with AWS Cloud Seems they are both part of same network Traffic between Corporate Data Center with AWS Cloud goes over Public Internet To set up Site To Site VPN Set a Customer Gateway in the Corporate Data Center Set a Virtual Private Network Gateway (i.e. VPG , i.e. VPN Gateway ) in AWS VPC In between Customer Gateway and VPN Gateway , provision a Site To Site VPN Connection Customer Gateway Set in Corporate DC (i.e. Corporate Data Center ) IP Address can be one of followings Static IP If behind NAT , use NAT public address Virtual Private Gateway i.e. VPN Gateway / VPG VPN Concentrator in the AWS side of the VPN Connection VGW is created and attached to VPC Possible to customize Autonomous System Number i.e. ( ASN ) To improve performance need to use ECMP protocol This protocol be enabled in VGW Need to implement this for each VPN Tunnel Result faster data transfers","title":"18 Site To Site VPN"},{"location":"Notes/VPC/18 Site To Site VPN/#site-to-site-vpn","text":"Connect Corporate Data Center with AWS Cloud Seems they are both part of same network Traffic between Corporate Data Center with AWS Cloud goes over Public Internet To set up Site To Site VPN Set a Customer Gateway in the Corporate Data Center Set a Virtual Private Network Gateway (i.e. VPG , i.e. VPN Gateway ) in AWS VPC In between Customer Gateway and VPN Gateway , provision a Site To Site VPN Connection Customer Gateway Set in Corporate DC (i.e. Corporate Data Center ) IP Address can be one of followings Static IP If behind NAT , use NAT public address Virtual Private Gateway i.e. VPN Gateway / VPG VPN Concentrator in the AWS side of the VPN Connection VGW is created and attached to VPC Possible to customize Autonomous System Number i.e. ( ASN ) To improve performance need to use ECMP protocol This protocol be enabled in VGW Need to implement this for each VPN Tunnel Result faster data transfers","title":"Site To Site VPN"},{"location":"Notes/VPC/19 Direct Connect/","text":"Direct Connect Provide Dedicated Private Connection from Remote Network to VPC Need to establish physical connection between Corporate Data Center and AWS Direct Connect Location Corporate Data Center => AWS Direct Connect Location => VPC In VPC a Virtual Private Gateway is required DC allow both Public and Private resources Supports IPv4 and IPv6 Two types of DC available Dedicated Connection Bandwidth be 1Gbps to 10Gbps A dedicated physical ethernet port will be available in AWS for the customer To establish Dedicated Connection , first request AWS and complete by AWS Direct Connect Partners Hosted Connection To establish Hosted Connection , don't need to request AWS , can be completed by AWS Direct Connect Partners Hosted Connection Direct Connect is a time consuming process (approximately 1 month) Encryption Data In Transit is not encrypted Data In Transit is private between Corporate Data Center , Direct Connect Partner and AWS Cloud Encryption can be established by IPsec-encrypted private connection IPsec-encryption is combination of Direct Connect and VPN This VPN is between Corporate Data Center and AWS Direct Connect Partner Used for Increase bandwidth throughput Working with large data set lower bandwidth cost Consistent network to avoid data drops connection shut down real time data feeds Hybrid environments ( On Premise Data Center and Cloud Data Center ) LAG can be used to aggregate maximum throughput","title":"19 Direct Connect"},{"location":"Notes/VPC/19 Direct Connect/#direct-connect","text":"Provide Dedicated Private Connection from Remote Network to VPC Need to establish physical connection between Corporate Data Center and AWS Direct Connect Location Corporate Data Center => AWS Direct Connect Location => VPC In VPC a Virtual Private Gateway is required DC allow both Public and Private resources Supports IPv4 and IPv6 Two types of DC available Dedicated Connection Bandwidth be 1Gbps to 10Gbps A dedicated physical ethernet port will be available in AWS for the customer To establish Dedicated Connection , first request AWS and complete by AWS Direct Connect Partners Hosted Connection To establish Hosted Connection , don't need to request AWS , can be completed by AWS Direct Connect Partners Hosted Connection Direct Connect is a time consuming process (approximately 1 month) Encryption Data In Transit is not encrypted Data In Transit is private between Corporate Data Center , Direct Connect Partner and AWS Cloud Encryption can be established by IPsec-encrypted private connection IPsec-encryption is combination of Direct Connect and VPN This VPN is between Corporate Data Center and AWS Direct Connect Partner Used for Increase bandwidth throughput Working with large data set lower bandwidth cost Consistent network to avoid data drops connection shut down real time data feeds Hybrid environments ( On Premise Data Center and Cloud Data Center ) LAG can be used to aggregate maximum throughput","title":"Direct Connect"},{"location":"Notes/VPC/20 Direct Connect Gateway/","text":"Direct Connect Gateway To establish DC with multiple VPC use DCG DCW does not establish VPC Peering VPC can be in Different Region but Same Account Connected VPC CIDR can not be Overlapped","title":"20 Direct Connect Gateway"},{"location":"Notes/VPC/20 Direct Connect Gateway/#direct-connect-gateway","text":"To establish DC with multiple VPC use DCG DCW does not establish VPC Peering VPC can be in Different Region but Same Account Connected VPC CIDR can not be Overlapped","title":"Direct Connect Gateway"},{"location":"Notes/VPC/21 Site to Site VPN vs DC vs DCG/","text":"Site To Site VPN vs DC vs DC Gateway Site To Site VPN Corporate Data Center to AWS Cloud Use Public Internet In VPC use VPG In Customer Data Center use Customer Gateway DC Corporate Data Center to AWS Direct Connect Partner to AWS Cloud Use Physical Connection from Corporate Data Center to AWS Direct Connect Partner Use Private Internet from AWS Direct Connect Partner to AWS Cloud In VPC use VPG DC Gateway If Direct Connect is required with multiple VPC , use DC Gateway","title":"21 Site to Site VPN vs DC vs DCG"},{"location":"Notes/VPC/21 Site to Site VPN vs DC vs DCG/#site-to-site-vpn-vs-dc-vs-dc-gateway","text":"Site To Site VPN Corporate Data Center to AWS Cloud Use Public Internet In VPC use VPG In Customer Data Center use Customer Gateway DC Corporate Data Center to AWS Direct Connect Partner to AWS Cloud Use Physical Connection from Corporate Data Center to AWS Direct Connect Partner Use Private Internet from AWS Direct Connect Partner to AWS Cloud In VPC use VPG DC Gateway If Direct Connect is required with multiple VPC , use DC Gateway","title":"Site To Site VPN vs DC vs DC Gateway"},{"location":"Notes/VPC/22 Egress Only Internet Gateway/","text":"Egress Only Internet Gateway Same as NAT Gateway , but for IPv6 Egress means outgoing Egress Only Internet Gateway Allow traffic to go outside Deny any traffic coming inside Very much like NAT Instance / NAT Gateway But only works for IPv6 Uses IPv6 does not have Private Address When we provision EC2 Instance with IPv6 Address in Private Subnet Since IPv6 does not have Private Address , it will get a IPv6 Public Address To restrict coming internet from outside use Egress Only Internet Gateway","title":"22 Egress Only Internet Gateway"},{"location":"Notes/VPC/22 Egress Only Internet Gateway/#egress-only-internet-gateway","text":"Same as NAT Gateway , but for IPv6 Egress means outgoing Egress Only Internet Gateway Allow traffic to go outside Deny any traffic coming inside Very much like NAT Instance / NAT Gateway But only works for IPv6 Uses IPv6 does not have Private Address When we provision EC2 Instance with IPv6 Address in Private Subnet Since IPv6 does not have Private Address , it will get a IPv6 Public Address To restrict coming internet from outside use Egress Only Internet Gateway","title":"Egress Only Internet Gateway"},{"location":"Notes/VPC/23 AWS Private Link/","text":"AWS Private Link Also known as VPC Endpoint Service Ideal when, a Service needs to be exposed from a VPC to multiple VPC Problem Scenario In my VPC , I have a Web service Need to expose that service to other VPC Possible solution be Make Web Service Public This is a security hazard Use VPC peering Need to update route table Other services will be accessible as well To establish Private Link Create a Network Load Balancer in Service VPC Create a ENI in the Customer VPC Connect NLB with ENI using the AWS Private Link To make it scalable Launch NLB in multi-AZ Create ENI in multi-AZ Private Link can be used with inter-region VPC Peering","title":"23 AWS Private Link"},{"location":"Notes/VPC/23 AWS Private Link/#aws-private-link","text":"Also known as VPC Endpoint Service Ideal when, a Service needs to be exposed from a VPC to multiple VPC Problem Scenario In my VPC , I have a Web service Need to expose that service to other VPC Possible solution be Make Web Service Public This is a security hazard Use VPC peering Need to update route table Other services will be accessible as well To establish Private Link Create a Network Load Balancer in Service VPC Create a ENI in the Customer VPC Connect NLB with ENI using the AWS Private Link To make it scalable Launch NLB in multi-AZ Create ENI in multi-AZ Private Link can be used with inter-region VPC Peering","title":"AWS Private Link"},{"location":"Notes/VPC/24 Classic Link/","text":"AWS Classic Link (Deprecated) It is distractor option in AWS Exam Try to avoid this option in MCQ Allow linking with EC2-Classic instance from VPC EC2-Classic instances are in a single network, shared with customer (Used before VPC )","title":"24 Classic Link"},{"location":"Notes/VPC/24 Classic Link/#aws-classic-link-deprecated","text":"It is distractor option in AWS Exam Try to avoid this option in MCQ Allow linking with EC2-Classic instance from VPC EC2-Classic instances are in a single network, shared with customer (Used before VPC )","title":"AWS Classic Link (Deprecated)"},{"location":"Notes/VPC/25 AWS Cloud Hub/","text":"AWS Cloud Hub Allow secure connection (through VPN ) among multiple Corporate Data Centers and AWS Cloud To establish this connection Create a Cloud Hub in AWS Connect Corporate Data Centers with this Cloud Hub Since this is VPN connection Use Public Internet Can use Encryption This is a low-cost Hub and Spoke Model","title":"25 AWS Cloud Hub"},{"location":"Notes/VPC/25 AWS Cloud Hub/#aws-cloud-hub","text":"Allow secure connection (through VPN ) among multiple Corporate Data Centers and AWS Cloud To establish this connection Create a Cloud Hub in AWS Connect Corporate Data Centers with this Cloud Hub Since this is VPN connection Use Public Internet Can use Encryption This is a low-cost Hub and Spoke Model","title":"AWS Cloud Hub"},{"location":"Notes/VPC/26 Transit Gateway/","text":"Transit Gateway Simplify network topology Star Gateway (Hub and Spoke Model) for VPC and On Premise Network Using Route Table we can define which VPC can talk to which VPC Can setup Direct Connect VPN Connection Support IP Multi-cast Only service in AWS that supports Ip Multi-cast","title":"26 Transit Gateway"},{"location":"Notes/VPC/26 Transit Gateway/#transit-gateway","text":"Simplify network topology Star Gateway (Hub and Spoke Model) for VPC and On Premise Network Using Route Table we can define which VPC can talk to which VPC Can setup Direct Connect VPN Connection Support IP Multi-cast Only service in AWS that supports Ip Multi-cast","title":"Transit Gateway"},{"location":"Notes/VPC/27 Cloud Hub vs Transit Gateway/","text":"AWS Cloud Hub vs Transit Gateway Cloud Hub One VPC connect with multiple On Premise Network Use VPN to establish connection Transit Gateway Connect multiple VPC with multiple On Premise Network Use VPN and Direct Connect to establish connection Supports IP Multi-cast","title":"27 Cloud Hub vs Transit Gateway"},{"location":"Notes/VPC/27 Cloud Hub vs Transit Gateway/#aws-cloud-hub-vs-transit-gateway","text":"Cloud Hub One VPC connect with multiple On Premise Network Use VPN to establish connection Transit Gateway Connect multiple VPC with multiple On Premise Network Use VPN and Direct Connect to establish connection Supports IP Multi-cast","title":"AWS Cloud Hub vs Transit Gateway"},{"location":"Notes/VPC/28 Networking Cost/","text":"Networking Cost Traffic from Public Internet to Instance is free Traffic from one Instance to another Instance in same AZ is free Traffic from one Instance to another Instance in different AZ 0.02 for Public Network / Elastic IP 0.01 for Private IP Traffic from one Region to another Region is 0.02 Best Practice Uses Private IP instead of Public IP Low cost, better performance Use same AZ as more as possible Risk on HA","title":"28 Networking Cost"},{"location":"Notes/VPC/28 Networking Cost/#networking-cost","text":"Traffic from Public Internet to Instance is free Traffic from one Instance to another Instance in same AZ is free Traffic from one Instance to another Instance in different AZ 0.02 for Public Network / Elastic IP 0.01 for Private IP Traffic from one Region to another Region is 0.02 Best Practice Uses Private IP instead of Public IP Low cost, better performance Use same AZ as more as possible Risk on HA","title":"Networking Cost"},{"location":"Notes/VPC/29 Global Accelerator/","text":"Global Accelerator Improve Availability and Performance of the applications Global Accelerator provides 2 IP address, we can also bring ours How Global Accelerator works Users connect to the Edge Location Edge Location pass traffic to Global Accelerator Global Accelerator pass the traffic to the Endpoint Group Without Global Accelerator user have to go through a lot of ISP Provider to reach the AWS Region Global Accelerator provides the following components Static IP Address Provide 2 static IP Address We can also bring our own Accelerator Direct traffic from the Edge Location to Optimal Endpoint through the AWS Global Network DNS Name DNS Name is provided to the IP Address Network Zone Similar to AZ Each Network Zone provide the Static IP If one Network Zone is blocked/not-available, it use the healthy one Listener Traffic can be distributed using Traffic Dial Endpoint Group Act as a fixed entry point of the using multiple endpoints of multiple region Endpoint","title":"29 Global Accelerator"},{"location":"Notes/VPC/29 Global Accelerator/#global-accelerator","text":"Improve Availability and Performance of the applications Global Accelerator provides 2 IP address, we can also bring ours How Global Accelerator works Users connect to the Edge Location Edge Location pass traffic to Global Accelerator Global Accelerator pass the traffic to the Endpoint Group Without Global Accelerator user have to go through a lot of ISP Provider to reach the AWS Region Global Accelerator provides the following components Static IP Address Provide 2 static IP Address We can also bring our own Accelerator Direct traffic from the Edge Location to Optimal Endpoint through the AWS Global Network DNS Name DNS Name is provided to the IP Address Network Zone Similar to AZ Each Network Zone provide the Static IP If one Network Zone is blocked/not-available, it use the healthy one Listener Traffic can be distributed using Traffic Dial Endpoint Group Act as a fixed entry point of the using multiple endpoints of multiple region Endpoint","title":"Global Accelerator"},{"location":"Notes/VPC/30 VPN Connection/","text":"VPN Connection Connection between On premise data center AWS Resource Connection is secure and private using IPsec","title":"30 VPN Connection"},{"location":"Notes/VPC/30 VPN Connection/#vpn-connection","text":"Connection between On premise data center AWS Resource Connection is secure and private using IPsec","title":"VPN Connection"},{"location":"Notes/security/01 Security Threat/","text":"NACL Use to block certain IP / IP Range HOST Based Firewall Use to block certain IP / IP Range HOST Based Firewalls are firewall iptables ufw Windows Firewall HOST Based Firewalls do not work when ALB is being used. Here NACL be used in ALB . ( Think Why? ) WAF Web Application Firewall Used for Common Security Threats Check Origin IP address SQL Injection Cross Site Scripting Check Headers When Cloudfront is used, set WAF in front of Cloudfront Rate Based ACL Rules can be used to avoid potential threat AWS Shield Use to prevent the DDoS attack AWS GuardDuty Thread detection service Used to monitor malicious activity and protect from unauthorized activities AWS Firewall Manager Used to manage the AWS WAF and AWS Shield Encryption On Flight (SSL) When a data is sending to server over internet, it is encrypted and only the server knows how to decrypt Use to prevent MITH (Man In The Middle) attack Encryption At Rest Before data is persist in the server, the data is encrypted and before retrival the data will be decrypted Key for encryption and decryption is being managed by another service like KMS The server should have permission to access KMS for the encryption and decryption operation With Encryption at Rest , even the server become vulnarable, the will still be safe Client Side Encryption Clients are responsible for encryption and decryption Server can not / should not decrypt the data This method is utilized by the Envelope Encryption","title":"01 Security Threat"},{"location":"Notes/security/01 Security Threat/#nacl","text":"Use to block certain IP / IP Range","title":"NACL"},{"location":"Notes/security/01 Security Threat/#host-based-firewall","text":"Use to block certain IP / IP Range HOST Based Firewalls are firewall iptables ufw Windows Firewall HOST Based Firewalls do not work when ALB is being used. Here NACL be used in ALB . ( Think Why? )","title":"HOST Based Firewall"},{"location":"Notes/security/01 Security Threat/#waf","text":"Web Application Firewall Used for Common Security Threats Check Origin IP address SQL Injection Cross Site Scripting Check Headers When Cloudfront is used, set WAF in front of Cloudfront Rate Based ACL Rules can be used to avoid potential threat","title":"WAF"},{"location":"Notes/security/01 Security Threat/#aws-shield","text":"Use to prevent the DDoS attack","title":"AWS Shield"},{"location":"Notes/security/01 Security Threat/#aws-guardduty","text":"Thread detection service Used to monitor malicious activity and protect from unauthorized activities","title":"AWS GuardDuty"},{"location":"Notes/security/01 Security Threat/#aws-firewall-manager","text":"Used to manage the AWS WAF and AWS Shield","title":"AWS Firewall Manager"},{"location":"Notes/security/01 Security Threat/#encryption-on-flight-ssl","text":"When a data is sending to server over internet, it is encrypted and only the server knows how to decrypt Use to prevent MITH (Man In The Middle) attack","title":"Encryption On Flight (SSL)"},{"location":"Notes/security/01 Security Threat/#encryption-at-rest","text":"Before data is persist in the server, the data is encrypted and before retrival the data will be decrypted Key for encryption and decryption is being managed by another service like KMS The server should have permission to access KMS for the encryption and decryption operation With Encryption at Rest , even the server become vulnarable, the will still be safe","title":"Encryption At Rest"},{"location":"Notes/security/01 Security Threat/#client-side-encryption","text":"Clients are responsible for encryption and decryption Server can not / should not decrypt the data This method is utilized by the Envelope Encryption","title":"Client Side Encryption"},{"location":"Notes/security/02 Cloud HSM/","text":"Cloud HSM Use key , not password Stands for Hardware Security Module Provide temper resistance environment for managing keys Its a dedicated hardware security module Manage your own keys, hence no access to AWS managed services Once key is lost, no way to retrieve it CloudHSM is Level 3 standard ( FIPS 140-2 Level 3 ) KMS is Level 2 standard AWS managed service Runs within VPC Single tenant, dedicated hardware, multi AZ cluster Use industry standard API Required when Need complete control over keys, including the underlying hardware and manage the lifecycle of keys Strict regulatory compliance is needed Level 3 compliance is needed PKCS#11 Java Cryptography Extensions Microsoft CryptoNG To keep Cloud HSM backup in secure and durable way Use EBK i.e. Ephemeral Backup Key to encrypt data Use PBK i.e. Persistent Backup Key to encrypt EBK Save the data to S3","title":"02 Cloud HSM"},{"location":"Notes/security/02 Cloud HSM/#cloud-hsm","text":"Use key , not password Stands for Hardware Security Module Provide temper resistance environment for managing keys Its a dedicated hardware security module Manage your own keys, hence no access to AWS managed services Once key is lost, no way to retrieve it CloudHSM is Level 3 standard ( FIPS 140-2 Level 3 ) KMS is Level 2 standard AWS managed service Runs within VPC Single tenant, dedicated hardware, multi AZ cluster Use industry standard API Required when Need complete control over keys, including the underlying hardware and manage the lifecycle of keys Strict regulatory compliance is needed Level 3 compliance is needed PKCS#11 Java Cryptography Extensions Microsoft CryptoNG To keep Cloud HSM backup in secure and durable way Use EBK i.e. Ephemeral Backup Key to encrypt data Use PBK i.e. Persistent Backup Key to encrypt EBK Save the data to S3","title":"Cloud HSM"},{"location":"Notes/security/04 Parameter Store/","text":"System Manager Parameter Store (SSM) Parameter Store is a component of AWS System Manager Manage secrets and configurations securely Parameter store is centralized tool to caching and distributing parameters across AWS services Helpful to separate configs and secrets from source control It is Serverless Scalable High Performance Used to store data and secrets Application configuration DB String Password API key Host Name Access Keys lambda functions env variable (when encrypted variables is shared to multiple lambda function) Values can be stored Encrypted by KMS Plaintext Can store parameters in hierarchies (Max 15 levels), like dev/app1/config prod/app1/config Can track version and roll back Can use TTL to expire values like passwords Can use to login to EC2 Instance using Run Command without using RDP or SSH Patch Manager : Used to pactch the managed instances to overcome security vulnarebilities","title":"04 Parameter Store"},{"location":"Notes/security/04 Parameter Store/#system-manager-parameter-store-ssm","text":"Parameter Store is a component of AWS System Manager Manage secrets and configurations securely Parameter store is centralized tool to caching and distributing parameters across AWS services Helpful to separate configs and secrets from source control It is Serverless Scalable High Performance Used to store data and secrets Application configuration DB String Password API key Host Name Access Keys lambda functions env variable (when encrypted variables is shared to multiple lambda function) Values can be stored Encrypted by KMS Plaintext Can store parameters in hierarchies (Max 15 levels), like dev/app1/config prod/app1/config Can track version and roll back Can use TTL to expire values like passwords Can use to login to EC2 Instance using Run Command without using RDP or SSH Patch Manager : Used to pactch the managed instances to overcome security vulnarebilities","title":"System Manager Parameter Store (SSM)"},{"location":"Notes/security/05 KMS/","text":"KMS When key is managed by in house security team' For Encryption Generate data key using Customer managed Cmk Encrypt data with data key Delete data key Store encrypted data key and data in S3 For decryption Use CMK to decrypt data key Decrypt data using Decrypted data key KMS Master Key is region specific By default KMS can encrypt mx 4kb of data. If we need to encrypt more data, we need to make use of Envelope Encryption KMS keys are region bounded Moving KMS encrypted resources between regions Create a snapshot of the resources While move it between region define new region KMS key Types of CMK Symmetric (AES-256) : Use single key for encryption and decryption Asymmetric (RSA) : Use key pairs, public key and private key. Public key for encryption and private key for decryption operation. Encryption is being happened from outside of the AWS. Envelope Encryption CMK is used to generate , encrypt and decrypt the data keys Data Keys are used to encrypt and decrypt the data, from outside the AWS Envelope Encryption Local Encryption Usage In local environment, when a data is encrypted using a key, the data is protected. But we also need to encrypt the encryption key . We can encrypt the encryption key using another master key, called Master Key or CMK (Customer Master Key) . This CMK is stored in the KMS and never leave the KMS. To use this CMK we must call the KMS api. To encrypt the local data, First get the data encryption key using GenerateDataKey api This data encryption key can be used to encrypt the data Delete the data encryption key To decrypt local data First decrypt the encrypted data key and get plaintext data key Decrypt local data using the plaintext data key Delete the plaintext data key","title":"05 KMS"},{"location":"Notes/security/05 KMS/#kms","text":"When key is managed by in house security team' For Encryption Generate data key using Customer managed Cmk Encrypt data with data key Delete data key Store encrypted data key and data in S3 For decryption Use CMK to decrypt data key Decrypt data using Decrypted data key KMS Master Key is region specific By default KMS can encrypt mx 4kb of data. If we need to encrypt more data, we need to make use of Envelope Encryption KMS keys are region bounded","title":"KMS"},{"location":"Notes/security/05 KMS/#moving-kms-encrypted-resources-between-regions","text":"Create a snapshot of the resources While move it between region define new region KMS key","title":"Moving KMS encrypted resources between regions"},{"location":"Notes/security/05 KMS/#types-of-cmk","text":"Symmetric (AES-256) : Use single key for encryption and decryption Asymmetric (RSA) : Use key pairs, public key and private key. Public key for encryption and private key for decryption operation. Encryption is being happened from outside of the AWS.","title":"Types of CMK"},{"location":"Notes/security/05 KMS/#envelope-encryption","text":"CMK is used to generate , encrypt and decrypt the data keys Data Keys are used to encrypt and decrypt the data, from outside the AWS Envelope Encryption Local Encryption Usage In local environment, when a data is encrypted using a key, the data is protected. But we also need to encrypt the encryption key . We can encrypt the encryption key using another master key, called Master Key or CMK (Customer Master Key) . This CMK is stored in the KMS and never leave the KMS. To use this CMK we must call the KMS api. To encrypt the local data, First get the data encryption key using GenerateDataKey api This data encryption key can be used to encrypt the data Delete the data encryption key To decrypt local data First decrypt the encrypted data key and get plaintext data key Decrypt local data using the plaintext data key Delete the plaintext data key","title":"Envelope Encryption"}]}